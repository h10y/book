[["index.html", "Hosting Shiny Applications for R and Python Everything you wanted to know about hosting Shiny apps Welcome License", " Hosting Shiny Applications for R and Python Everything you wanted to know about hosting Shiny apps Péter Sólymos and Kalvin Eng July 30, 2024 Welcome This is the online version of Hosting Shiny Applications for R and Python, a book currently under development and intended for a 2025 release by CRC Press. Visit the GitHub repository for this site. License This book is licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License. The code samples in this book are licensed under the MIT License. "],["foreword.html", "Foreword", " Foreword This is an awesome book! I can recommend it. John Doe, Affiliation "],["preface.html", "Preface Motivation Who Is This Book For? What Will You Learn? What Will You Not Learn? Conventions About the Authors Acknowledgements", " Preface Motivation Shiny is a reactive web application framework used by R and Python users to communicate their results to others. Whether you are just starting out with Shiny, or you are a seasoned programmer, at some point you’ll need to share your app with your colleagues and users. You might decide to send them scripts or deploy the app and share the URL that takes people to your app. The documentation and books about the Shiny ecosystem are phenomenal resources. However, these resources focus on the development of the applications and decidedly cover very little about what to do once the app is deployed. It might surprise you, but there are at least /FIXME: CHECK THIS NUMBER/ 20 hosting options for Shiny. Navigating these options and finding up-to-date advice on choosing one and getting started is not easy or trivial. Information about Shiny hosting is scattered around the Internet and is often incomplete. Recommendations are usually narrowly focused and never really ask important questions such as: Why do you want this app to be deployed? Who is this app for? What is your budget? The goal of the book is to help you learn about the hosting options and what it takes to host your apps securely. Current available books focus on the R aspect of Shiny, which is not surprising given that Shiny for Python is a relatively recent addition to the Shiny ecosystem. Luckily, Shiny for R and Python have very similar deployment and hosting patterns. Therefore, we can cover both in the same book. We think this is important, because with the increasing popularity of Shiny available for R and Python, learning more about the hosting side of the application life-cycle will be a critical need for programming enthusiasts and data science professionals alike. Who Is This Book For? If you are asking any of the following questions, this book is definitely for you: What Shiny hosting solutions are out there? How do I decide which of these options suits my needs, my budget, and my skill level the best? How can I get started, and where can I get help if I get stuck? If you are not (yet) asking these questions, but want to learn about servers, cloud instances, and containers from the perspective of Shiny apps, this book is still for you. You might be a researchers in academia, government, and at a nonprofit organization who wants to self-host Shiny apps so that you can show your results to your colleagues, your stakeholders, or to the public. You might work at a small agency or a startup and want to securely host apps for your clients and users. Or you might be a data scientist who wants to effectively collaborate with people in the IT and engineering department on Shiny app deployment and hosting. What Will You Learn? The book is structured into four main parts: Part I. Getting started – This will give you the fundamentals for the rest of the book, including some background, concepts, and setup instructions. Part II. Shiny Apps – This part deals with everything that happens on your local machine, including developing and organizing Shiny apps, and ways of running the apps locally. Part III. Hosting Shiny Apps – This part describes all the ways of how Shiny apps can be hosted on remote machines, like cloud servers. This part is the longest, because there are quite a few hosting options. Part IV. What is next – You’ll find a summary of all the things you might want to do next with your Shiny app, things that might either be too advanced or are harder to describe in a physical book format. The book does not end on the last page. You can find online supplements, including the HTML version of the book and all the code that we present in the book and more on the book’s website at FIXME: add final website. What Will You Not Learn? This book is focused on the many different ways of hosting Shiny apps. This topic is at the intersection of R &amp; Python programming, Shiny app development, and DevOps. We assume you are already familiar with either R or Python. If this is not the case, we suggest you start with accessible introductions to R or Python before going further. You don’t need to know both languages, but knowing at least one is definitely needed. We will use example Shiny apps to test and demonstrate the different hosting options, but we are not going to teach you how to master Shiny, how to engineer production-grade apps, and how to make outstanding user interfaces with Shiny. You can find all about those topics in these excellent books: Mastering Shiny (Wickham 2021) Engineering Production-Grade Shiny Apps (Fay et al. 2021) Outstanding User Interfaces with Shiny (Granjon 2022) There is also quite a bit that can be said about servers and cloud infrastructure that we won’t cover in this book. DevOps for Data Science (Gold 2024) gives an accessible overview of DevOps conventions, tools, and practices, and provides pointers for further broadening your knowledge in these areas. Conventions Throughout the book, we will use title case and all caps for software and frameworks, e.g. Shiny, R, Python SSH, HTML, CSS, etc. We will use monospace code font for inline mentions of libraries and extension packages (e.g. shiny, pandas), also for functions and methods (e.g. summary(), pandas.read_csv()), and file names (image.png). Italicized text will be preserved for publication titles (e.g. Mastering Shiny) and for cases when we imitate self talk. For example: I feel a little bit lost, what do I do now? We will use bold when highlighting or emphasizing words. URLs when pointing to Internet resources, like websites, will be in normal font (e.g. https://mastering-shiny.org), URLs when mentioning with regards to examples will be in code font (e.g. http://127.0.0.1). We will use angle brackets for variable values that you should replace in your code, e.g. https://&lt;your-domain&gt;. Dot-dot-dot within brackets, [...], will indicate that we trimmed the output to save space by not showing too verbose or repetitive text in the book. We will use backslash (\\) to break up long commands to help readability and to fit on the printed page. We use the h10y as a shorthand for Hosting Shiny, and it means that between the first letter h and the last letter y there are 10 other letters. You will see the domain h10y.com used to shorten links and to prevent link rot after the print version of the book is out. We will also use h10y in Docker image namespaces to make image tags shorter. About the Authors Péter Sólymos is a senior data scientist with 20 years of experience in wildlife, environmental, and utilities sectors. He holds a PhD in biology, he has authored 70 peer-reviewed publications and several statistical software packages. He is passionate about using statistics and data science to bridge the gap between data and decision making. His focus is on enabling this by helping organizations adopting cloud-native practices into their operations. FIXME: revise. https://peter.solymos.org Kalvin Eng is a PhD candidate in Computing Science at the University of Alberta with over 15 years of software development experience. His research involves understanding and improving the processes of software engineering. He is passionate about helping organizations develop efficient processes to build software more effectively. FIXME: revise. https://kalvineng.com Acknowledgements We are grateful for Kahlid Lemzouji for continuously finding new reasons to up our game with Shiny hosting. Thanks to Kalob Taulien for helping with our early Shiny hosting endeavors. Some of the content of this book was developed as part of workshops held by the authors at the Edmonton R User Group (YEGRUG). We are thankful for the Statistical Society of Canada for inviting us to teach the course Delivering applied statistics from concept to production. FIXME: add here more names as needed References Fay, Colin, Sébastien Rochette, Vincent Guyader, and Cervan Girard. 2021. Engineering Production-Grade Shiny Apps. Chapman &amp; Hall. https://engineering-shiny.org/. Gold, Alex. 2024. DevOps for Data Science. Chapman &amp; Hall. https://do4ds.com/. Granjon, David. 2022. Outstanding User Interfaces with Shiny. Chapman &amp; Hall. https://unleash-shiny.rinterface.com/. Wickham, Hadley. 2021. Mastering Shiny. O’Reilly Media, Inc. https://mastering-shiny.org/. "],["1-part1-background.html", "Chapter 1 Background 1.1 Data-intensive Apps 1.2 Enter Shiny 1.3 Application Development 1.4 Application Hosting 1.5 The DevOps Cycle 1.6 The Hosting Cycle 1.7 Summary", " Chapter 1 Background As a data professional working with R or Python, you might have a workflow that loads and transforms some data. You might have to explore and visualize this data, run some analyses and summarize your findings in reports. You can send the report, or even the data and reproducible scripts to others. In which case, they will have to be able to load and run you code. Wouldn’t it be easier to deliver your findings as interactive documents or web applications? In this chapter, you’ll see what Shiny can offer when it comes to data-intensive web applications, and you will be familiar with the workflow of developing and operating such web applications. 1.1 Data-intensive Apps Data-intensive applications, or data apps for short, are modern web applications that are centered around operations on data. This is different from traditional websites that focus on providing information for a user. The primary goal of data apps is to make data-intensive operations approachable. For non-technical users, this might mean simplifying a sophisticated/complex task like processing a CSV data file to a PDF report. For technical users, it can offer an intuitive understanding of complex relationships through interactive visualization, or it can help in institutionalizing the workflows that are incorporated in the apps. The steps involved in simplifying data-intensive operations resemble more traditional workflows used by analysts through the command line: Data import (files, databases) Data transformation (extract–transform–load; ETL) Summarizing the data (descriptive statistics) Visualization and exploratory data analysis (EDA) Repeating these steps for different subsets or slices of the data (interactive and reactive) Saving, storing, or exporting these results (files, databases) Because these apps allow users to interact with the data, these apps share some similarities with desktop software applications, like Excel. But unlike desktop applications, data apps are accessed through the browser. This is advantageous as your application is able to run on any device that has a web browser like a desktop, tablet, or phone. There is also a distinction between our definition of data apps and general-purpose analytics tools, like Microsoft PowerBI or Tableau. Such tools are primarily used for exploratory purposes, whereas data apps are custom built and capable of answering questions in a lot more depth. This is possible because of all the analytics capabilities of R and Python can be called upon in real time. In other words, data apps can be exploratory and explanatory at the same time. 1.2 Enter Shiny If your analytics workflow is built using R or Python, Shiny is the easiest way to create data science related web applications. Shiny can also power dynamic documents. As advertized by the shiny R package description (Chang et al. 2024): [Shiny] Makes it incredibly easy to build interactive web applications with R. Automatic “reactive” binding between inputs and outputs and extensive prebuilt widgets make it possible to build beautiful, responsive, and powerful applications with minimal effort. In other words, you can turn your data and scripts into a web application with limited web development skills. Shiny lets you to create a web app without knowing HTML, JavaScript and CSS. But more importantly, it will also let you include reactivity, so your users can dynamically explore the results. Reactivity is the most important and distinctive feature of Shiny, compared to similar web application frameworks, like Streamlit, know mostly in Python. Reactivity is responsible for re-executing parts of the code when things change as the result of users interacting with the app. This is possible through reactive bindings between inputs and outputs. The apps are also interactive, which is different from being reactive in the sense that during interactive behavior, the state of the application stays the same on the backend, changes are only rendered on the client side (i.e. in your browser). Think of popups, hover labels, transitions, and animations. These behaviors are usually powered by the front-end JavaScript code. The third feature mentioned in the Shiny R package description is responsiveness. It refers to the styling of the rendered web application. Again, client-side JavaScript and CSS code used by the Shiny user interface is responsible for the app responding to various device sizes (desktop, tablet, phone), or viewports (resizing the window). Size responsive systems allow the web page to respond to changes according to well thought-out rules without the interface elements falling apart. 1.3 Application Development Application development begins with an idea of what you would like your app to do. In a first iteration, you will have a proof-of-concept version of the app that will likely have bugs and need more refinement. It might take several iterations of fixing bugs and incorporating feedback from the users until the data app reaches its final form. Even after that, you might have to update the data behind the app, incorporate latest technologies, and provide security updates for the users. You might not make fundamental changes to the app any more, but you are still releasing new versions from time to time. This also means that you might want to test your changes before releasing the new version to your users. App development is usually based on accumulating small changes, continuously testing these changes, and releasing these changes to the users at a pre-determined cadence, depending on the criticality of the changes made to the app. You might not wait too long with a security update. However, new features might get released monthly or yearly. If you are using Shiny, you are participating in application development. Shiny is a web application framework that can help with the application development process. As a framework, it abstracts most of the complicated code needed for user interfaces, leaving you more time to focus on developing data features for your data app. Shiny is integral in simplifying the application development process for data-intensive apps. 1.4 Application Hosting Once you’ve developed your Shiny application, you might be wondering how you can share the application to your colleagues or a wider audience. This is where the concept of application hosting comes in. Hosting an app includes operations required for successfully and securely running the app online, so that the users of the app get access to it without interruption. It starts with deploying the application. What deployment means, is that a person or an automated process moves some files from one computer to another and refreshes the hosting server to display the new version of the app. You might want to monitor the usage of the app, i.e. how many users are active, when and for how long they are using the app. You should also monitor resource usage on the hosting server, for example CPU, memory, and disk space. This kind of visibility into the system and the ability to retrieve system and application level logs will be critical in case something goes wrong. All these require some up-front investment in learning about and setting up the systems that you will host you app with. After the resources spent on app development and the initial setup, operating costs will be determined by the kind of setup and hosting that is required for delivering the value to your users. The user experience also largely depends on the performance of the servers that the apps are running on, because with Shiny most of the computations happen on the server-side – i.e. on a remote computer or in the cloud. Other requirements that are less important include the client’s machine specifications. Data apps are expected to run on desktops, laptops, tablets, and phones via a web browser. This is good news for app developers because data apps can be written in different programming languages and still can result in a comparable user experience. One might prefer Python or R, others might use JavaScript. The end result will be very similar: a data app running in the browser. This book will help you gain a better understanding of how to host your Shiny applications walking you through different solutions that can best fit your requirements. 1.5 The DevOps Cycle DevOps is an important part of application development and hosting. DevOps is a set of practices and tools that integrates and in most cases automates the work of software development (Dev) and information technology (IT) operations (Ops). The Dev side is concerned with building a better app, while the Ops side is concerned with providing the best infrastructure to run the app on. DevOps therefore refers to the collaboration culture between the people on both sides to make this process as smooth as possible through the use of monitoring and logging to identify and quickly resolve issues. Figure 1.1 shows the of DevOps cycle as it is often visualized, capturing the development and operations that we described in the previous subsections. Figure 1.1: The DevOps cycle. DevOps is also often associated with Continuous Integration and Continuous Deployment (CI/CD). CI/CD practices ensure the reliable delivery of frequent code changes. It emphasizes the use of automation to reduce human error. You can view DevOps and CI/CD as two sides of the same coin. CI/CD is pro-active, and tries to eliminate sources of errors through automation. DevOps is a broader framework that codifies reactive practices when we notice deviations from the plan. Figure 1.1 might suggest that DevOps is a never ending cycle, and that changes can only originate from within the cycle. All of those steps are under the control of the development and operations teams. But in reality, we can imagine many factors that can perturb the ideal system from the outside. These would be factors that we cannot or do not want to control. This is when things do not go according to plan. For example, the system being hacked is never a pleasant experience. Paying ransom, or highjacked servers sending spam emails or mining Bitcoin is nobody’s idea of having fun. Even the biggest cloud providers can experience outages. These are all events that we can prepare for, but cannot control. Other events throwing the DevOps cycle off the plan might be more benign. Having too many users is often considered a good kind of problem. But unexpected surges in usage might lead to unresponsive servers. Adequate monitoring and alerting can prevent these unfortunate events from happening. But at some point, the team might be faced with a decision. How can we support more users? How can we improve user experience by reducing loading time of the apps? Adapting to changing needs and circumstances often leads to changes in the way how the application is hosted. Changes can be incremental, like you need more CPU or memory for the server. Or it can be more drastic, for example moving from a hosting platform to self hosting on your own cloud server. The DevOps cycle illustrates how the development and deployment of an application is iterative. Our book presents the hosting of Shiny applications in a similar fashion that we coin as the hosting cycle described in the next section. 1.6 The Hosting Cycle The hosting cycle is inspired by the DevOps cycle. It is meant to illustrate how the hosting of your Shiny application might take many iterations. Let’s give an example. You have a Shiny app that you’d like to deploy. You have read the documentation on the Shiny website and now you are thinking: Posit Connect is pricy, Shiny Server has no licensing fee, and both require self hosting on my own server or in the cloud. I don’t know how to do that. So I will go with Shinyapps. It is free to start with, I can deploy my app with a click of a button from my RStudio Desktop. Shinyapps definitely seems like the best option when starting out. It is free for the first five apps (with limited hours of app usage). You host your app in a cloud platform, i.e. on someone else’s server, that is fully managed. This means less headache for you. You sign up to Shinyapps, you click the deploy button in RStudio Desktop, and a few of minutes later your app opens up in the browser. Let’s take a look at Figure 1.2 that illustrates the cycle of evaluating the Shiny hosting options from time to time. In our example above, we assumed that you are new to Shiny hosting. You did some research, you made your choice based on some criteria, e.g. pricing (free) and operational complexity (managed hosting with push button publishing). Now you are at a stage where the option that you picked is working just fine. This could be the end of the story for hosting your Shiny apps. But more often than not, questions will pop into your head as time goes by. I have many active users and I am running out of free app hours on Shinyapps. I want to host more than &gt;5 apps. I want my custom domain with HTTPS. I need better performance and more memory. I need specialized software libraries installed. I need fine grained access control to my apps. Should you upgrade your Shinyapps subscription? Should you set up your own server and try Shiny Server? Should you convince your boss to foot the bill and buy Posit Connect for your team? There could also be other options out there. This is where the contents of this book will help you make a decision. Your answer might depend on many factors, or your needs might evolve over time. Having an understanding of what it takes to go with certain hosting options will provide the foundation for that. Figure 1.2: Shiny hosting cycle. Reading this book and trying some of the examples yourself will help you to build confidence when it comes to evaluating you Shiny hosting needs and options the next time you arrive to a crossroad. 1.7 Summary We presented a very generic outline for how the need for a data-intensive application might arise, and the reasons why Shiny might be the right tool for that. You might have your own reasons for picking Shiny and you might also have your unique circumstances and needs. But the DevOps principles will likely apply to you too when developing your app. And the hosting related considerations and options listed in the “hosting cycle” section will also apply to you. That is why we think that by reading this book, you will be able to make an informed decision regarding your next steps around hosting your Shiny apps. In the next chapter, we will review important concepts that you might already be familiar with or at least heard about those concepts. It is very important to have a working knowledge of these concepts because as you browse the web or read more advanced materials, you will run into these words without much explanation. Shiny simplifies web technology for data professionals, but those web technologies sit under the hood of Shiny’s abstraction layer. Hosting directly interfaces with these fundamentals, so understanding those concepts will help you a lot. References Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2024. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny. "],["2-part1-hosting-concepts.html", "Chapter 2 Hosting Concepts 2.1 Domains and Networking 2.2 Website Technologies 2.3 Servers 2.4 Hosting Environments 2.5 Summary", " Chapter 2 Hosting Concepts To better understand this book, you will need to first gain a general understanding about how web applications are hosted. This includes: domains and networking, website technologies, servers, and hosting environments. Imagine accessing a Shiny application on the Internet. At a high level, you visit the application by clicking a link or typing a URL in the browser. By visiting the application, the browser performs operations related to domains and networking to serve you the application. The application itself is run in the browser with website technologies. The actual data of the application is hosted on a server that has a hosting environment catered to the Shiny application. Users accessing a Shiny application primarily remember only the website link, abstracting them from understanding the concepts needed for the application to run on the Internet. This chapter delves into how Shiny applications are hosted, using accessing a Shiny application as a motivating example. By the end of this chapter, you should be able to grasp why an application might not be loading, or why it is taking longer than you have anticipated for the application to load. 2.1 Domains and Networking To access a Shiny application, it begins with a URL, or universal resource locator, like: https://h10y.github.io/faithful/. There is a lot to unpack with this URL. First, there is the protocol which is the https part. The protocol specifies how data will be transferred to you. If you use https, which is shorthand for “Hypertext Transfer Protocol Secure”, it means that you will transfer data securely with encryption. If you use http (Hypertext Transfer Protocol), it means that data will be transferred without any encryption. Next is the domain name which is the h10y.github.io where h10y is the subdomain of github.io and github is the subdomain of the top level domain (TLD) io. The domain name specifies where data will be transferred from. It is a reference to an Internet Protocol (IP) address that identifies a computer (server) that is available on the Internet. The domain lets your computer know which computer to request data from. Finally, there is the path part which is /faithful/. The path lets you specify what resource you want from the server. Sometimes, you might see a URL like: https://h10y.github.io:443/faithful/ which is different from the previous URL we introduced (note the “:443”). This new part specifies a port. The port specifies the connection point of a server. There are common ports for different protocols. For example, for “https” it is usually 443. For “http”, it is usually 80. The URL just specifies how a client connects to a server. The actual connection, requires networking between the client and server using TCP/IP (Transmission Control Protocol/Internet Protocol). TCP/IP is a suite of communication protocols to help computers connect with each other with forms a network of computers. In the case of the Shiny application, a client computer (i.e. your laptop) would request data from a server computer and the server sends a response with the requested data. This data contains all the necessary information to serve your application. These request/response operations happen everyday on the Internet to provide access to applications like your Bank portal or social media app. In this book, you’ll see the version 4 of the Internet Protocol addresses (IPv4) being used. IPv4 addresses can be recognized by a unique combination of numbers and periods, e.g. 192.177.17.4. Version (IPv6) addresses can be recognized by having sets of decimal and hexadecimal numbers separated by colons. There are also a few special-purpose IP addresses that you’ll see mentioned in the book. The address 127.0.0.1, or the localhost, is a self-reference to the current device. The 0.0.0.0 address is used as an unspecified address and the server will listen to any address on any interface. 2.2 Website Technologies The data that comes from a server is provided in a raw format that needs to be interpreted by a client (Fig. 2.1, top). In the case of a Shiny application, the data provided is meant to be interpreted by a web browser that serves a website. A website uses many technologies to interpret which commonly includes: HTML (HyperText Markup Language), CSS (Cascading Style Sheets), and JavaScript. Shiny applications use HTML, CSS, and JavaScript to render a web application. The HTML contains textual information about a website. While the CSS provides styling for a website. The JavaScript enables interactivity for a website by communicating with the Shiny application backend. The Shiny application backend runs on either R or Python and creates the data needed to render the application on a web browser. A feature of Shiny is the use of websockets that enable the client computer to establish a channel with the server for two-way communication. This feature is critical in providing the reactivity that make Shiny so well suited for data related applications (Fig. 2.1, middle). Computations on the client side are made possible by R or Python libraries compiled into WebAssembly (Wasm) (Fig. 2.1, bottom). This is the technology used by Shinylive applications that require only static hosting of HTML files and related “assets”, which is how we usually refer to the JavaScript, CSS, and other files like images etc. Figure 2.1: Web technologies from simple HTTP request/response (top), to Websocket connetions (middle), and WebAssempbly (Wasm) based applications (bottom). 2.3 Servers A server is a computer that runs indefinitely, and is available to serve content to anybody that requests it. Servers can be costly to run and are often shared with other users. To alleviate any concerns of security and resource sharing a server can be shared through a virtual layer. By using a virtual layer, a virtual private server can be created. This is where a server’s resources can be allocated to be used exclusively by a user. Servers can be offered as either as a IaaS (infrastructure as a service) or PaaS (platform as a service). IaaS provides access to a server without any support. It requires configuration and some server administration knowledge. While PaaS usually abstracts direct access to the server by offering tools to help host and manage your application on a platform. Serving static files is much easier that serving dynamic applications. Static files, as the name implies, will not change. Any modifications to the data are only made on the client side in memory. In contrast with this, dynamic applications require constant communication between the client and the server. As a result, hosting static files vs. dynamic apps will require different setups on the server side and will have different resources to effectively serve the users. 2.4 Hosting Environments On servers that are meant to serve web applications, environments are specifically setup to help run web applications. A lot of consideration of security, compatibility, and optimization goes into hosting environments to ensure that a web application runs as intended. In terms of security, you must consider firewalls, how data is transmitted, and who can access your app. These considerations may not be known to the end user of your Shiny application, but it is important for you to be aware of when hosting a Shiny application. In terms of compatibility, virtualization and containers can help make your application run on any platform. Otherwise, you might spend a lot of time installing the right software to run your Shiny application. This also helps make your application scalable by being able to easily deploy on multiple instances of a platform. In terms of optimization, you must consider how to route your traffic to ensure that your app runs smoothly. Many times, there might be high demand for your application. This is where multiple instances of your Shiny application might come in handy, where you are able to distribute the requests for your application. You will see a considerable portion of this book devoted to containers and the description of container-based hosting options for Shiny apps. Containers bundle their own software, libraries and configuration files and are isolated from one another as shown in Figure 2.2 that shows how hardware level virtualization (VMs) and containers relate to each other at a high level. Figure 2.2: Virtual machines (VMs) virtualize environments at the hardware level while containers create virtualized environments at the software level. FIXME: We will leave pointers to: other chapters/books/websites 2.5 Summary In this chapter, we have covered the basic concepts needed for understanding the subsequent sections. In short, we have explained at a high level what goes into hosting a Shiny application and how it is served over the Internet. In the next chapters, you will learn more details about how hosting a Shiny application including more advanced concepts such as: Creating a virtual environment for your Shiny application with a container Where to host your Shiny Application in the Cloud Considerations for making your Shiny Application production ready "],["3-part1-local-setup.html", "Chapter 3 Local Setup 3.1 Installing Your Developer Tools 3.2 The Command Line 3.3 Source Code Management with Git 3.4 Working with Servers 3.5 Readability on the Command Line 3.6 Other Useful Tools 3.7 Summary", " Chapter 3 Local Setup To make your Shiny applications accessible for a wider audience, you must have some tools available locally on your laptop or desktop — sorry, a tablet or a phone are not recommended devices for this. You will become familiar with several tools used to manage your code locally or communicate with remote machines. By the end of this chapter, you should have obtained the knowledge and the local setup of tools to connect to a remote server and run a Shiny app from the command line. These are also the prerequisites for you to be able to follow the examples in this book. 3.1 Installing Your Developer Tools 3.1.1 R R is a programming language designed mainly for statistical computation, but it has been extended with packages to support tasks like developing web apps with Shiny. To begin, you must install the appropriate binary for your operating system available at https://cran.r-project.org/ under the “Download and Install R” section. Next, you should install an integrated development environment or IDE of your choice. One of the most popular ways to develop in R is to use the RStudio Desktop developed by Posit (formerly RStudio). You can download the binary from https://posit.co/download/rstudio-desktop/. Another increasingly popular IDE option for R is Microsoft’s VisualStudio Code, or VS Code for short. You can download the installer from https://code.visualstudio.com/download. It is recommended to use the R extension for VS Code that relies on the languageserver R package (Lai 2023). You can find setup instructions in the REditorSupport/vscode-R GitHub repository. Another useful extension is the Shiny VS Code extension from the VS Code Marketplace or the Open VSX Registry. To install Shiny, you can run the command install.packages(\"shiny\") in the R console. You will need a few other packages too: install.packages(c(&quot;bslib&quot;, &quot;shiny&quot;, &quot;shinylive&quot;, &quot;htmltools&quot;, &quot;rmarkdown&quot;)) FIXME: list other R packages. 3.1.2 Python As a scripting language, Python is versatile and can be developed in different environments including text editors and IDEs. The recommended IDE for developing in Shiny with Python is VS Code. To use the IDE for Python, you must ensure that Python can run on your operating system. Operating systems like Mac OS X and Linux should already have come bundled with a version of Python, but it may be an older version. Therefore, we recommend installing the latest Python version for Mac OS X and Windows from https://www.python.org/downloads/. For Linux/Unix based operating systems, we recommend installing Python using the command line with the following commands: FIXME: Python install command Install Shiny for Python with pip install shiny. Some of the examples will use Shinylive, you can install it with pip install shinylive. FIXME: check this, how to install Python and pip etc. The Shiny VS Code extension from the VS Code Marketplace or the Open VSX Registry works with R and Python as well. Python itself also has VS Code extensions that can be used. A rathe new IDE option that R and Python users should keep an eye on is Positron by Posit as it promises to be an extensible, polyglot tool for writing code and exploring data. 3.1.3 Web Browser You likely already have a web browser on your local machine. Most people use Chrome, Safari, Edge, Firefox. Sometimes, as you develop apps and check the results in your browser, you might run into very obscure errors. In such cases, before searching for fixes on the Internet, try clearing the browser cache or do a hard refresh. You might also find it useful to view the apps in incognito mode. This way it will automatically clear the cache and cookies without affecting your regular browser windows. 3.1.4 Quarto Quarto is a open-source scientific and technical publishing system by Posit that can include Shiny apps. You can find install instructions at https://quarto.org/docs/get-started/. 3.1.5 Docker Desktop To download and install Docker for your platform of choice, visit the Docker download page and follow the instructions. Note that commercial use of Docker Desktop in larger enterprises (more than 250 employees or more than $10 million USD in annual revenue) requires a paid subscription. If you are a Mac OS X user on Apple Silicone (M1 chip and above), you might want to enable virtualization to be able to run and build images for AMD64 architecture. Otherwise the images might have poor performance or fail on other platforms. Go to the Settings in Docker Desktop, and under the General tab check the “Use Virtualization framework” and “Use Rosetta for x86_64/amd64 emulation on Apple Silicon” boxes. 3.2 The Command Line The command line is a text-based user interface to control a computer. Instead of using a mouse, you use your keyboard to type commands to control the computer. This is different than the graphical user interfaces (GUIs) used in modern day operating systems like Windows, Mac OS X, and Ubuntu Desktop Linux. The command line is important for managing your Shiny app deployment on a remote server where there might not be a GUI for you to control the remote server. Depending on your operating system, there are different options for the command line. On Windows, we recommend installing Windows Subsytem for Linux in order to use a Linux based command line. This is because some commands that we describe in this book may be incompatible with Windows based command lines like PowerShell (comes preinstalled on Windows) or Git Bash. For Mac OS X, we recommend using the built-in terminal application. You will be able to run the commands that we describe in the book. For Linux-based systems like Ubuntu, you can also use the built-in terminal application. Our book will cover commands using a “*nix based command line”. This means the Windows Subsytem for Linux terminal or the built-in terminals with Mac OS X or Linux. The terminal is also available from inside your IDE (RStudio or VS Code), which means that you won’t have to change windows to access it. Now let’s review some of the most common shell commands that we’ll need in the absence of a graphical user interface (GUI). The shell is a program that interprets and executes the commands you type into your command line terminal. 3.2.1 Navigation Navigating through the command line might seem daunting at first. However, it is quite simple once you memorize 3 key commands involving directories. 3.2.1.1 Current Working Directory To know which directory you are in, you can type the pwd command. This will give you the path of the “working directory” that you are in. 3.2.1.2 Listing Directory The ls command is used to list all files in the current directory. This is important to know when you are trying to find a file on the command line. ls -al will also print hidden files and directories and information about the files and directories with their permissions and last updated time. To list files in another folder, add the relative or absolute path after the command as ls &lt;path&gt; or ls -al &lt;path&gt;. 3.2.1.3 Change Directory The cd command is used to change directories and traverse a computer. For example, you may want to change your working directory to a nested folder of your current working directory. In order to do so, you would type cd in the command line and typing a space after, and then press tab to see the available directories to traverse to. With the correct directory in mind, you can type the directory name after and press enter to change directories. To verify that you are in the new directory, your command line should indicate the current directory that you are in. If you are still unsure, you can use the pwd command to get the path of your current working directory. By running cd .. you can go up one directory. To go up 2 directories, you can run: cd ../... 3.2.2 Editing Files You can navigate through the computer with the command line. The next important task to know about is editing files with a text editor. There are many popular text editors that usually come bundled with your *nix based command line including: nano, vim, and emacs. Vim and Emacs are much more complex and require more technical knowledge to edit and save files. Therefore, if your are just staring out, we recommend using Nano. Nano is a user-friendly text editor that allows you to edit files from the command line. To begin, you can type nano followed by a space and the name of the file you want to edit, e.g. nano app.R. If the name of the file does not exist, nano will create the file for you when you save it. To navigate through a file with Nano, you can use your arrow keys to move the cursor. You can add text to the file by typing on your keyboard. The instructions for nano are at the bottom of the editor. The ^ stands for the CTRL key and the letter beside it stands for the key you press to run the command. Press the CTRL key together with the letter on the keyboard to execute a command. To save the file, you would press CTRL+O and confirm the prompts with enter. To exit Nano, you would press CTRL+X. 3.2.3 File and Directory Operations It is also important to know about how to move, copy, and delete files and directories. Furthermore it is also important to know how to create directories. 3.2.3.1 Making Directories To make a directory you would type mkdir followed by a space and the directory name. To confirm that the directory is created, you can run the ls command and see that the folder has been created. You can even try to navigate to it with the cd command. 3.2.3.2 Moving Files and Directories To move a file or directory, you would simply type mv &lt;path-to-file-to-move&gt; &lt;destination-path&gt;. You can verify that a file or directory has moved with ls &lt;destination path&gt; and you should see that in its new location. 3.2.3.3 Copying Files and Directories Copying files is similar to moving files. To copy a file, you would run cp &lt;path-to-file-to-copy&gt; &lt;destination-path-and-filename&gt;. To copy a directory, you will need to add the -r flag so that the whole directory is copied “recursively”. The command you would run would be: cp -r &lt;path-to-directory-to-copy&gt; &lt;destination-path-and-directory-name&gt;. You can verify that a file or directory has been copied with ls &lt;destination path&gt; and you should see that in its new location. 3.2.3.4 Deleting Files and Directories To delete files run: rm &lt;path-to-file-to-delete&gt;. For deleting directories, you will need to add the -r flag so that the whole directory is deleted “recursively”. The command to delete directories is: rm -r &lt;path-to-directory-to-delete&gt;. You can verify that a file or directory has been removed with ls &lt;path-to-old-file-or-directory&gt;. There should be an error saying that the file and directory do not exist. 3.2.4 Super User Access At times you may find that you do not have the correct permissions to edit a file or even view a directory. Therefore, you must use super user access to gain the correct permissions for your current user. If you are looking for super user permissions for a single command you can run: sudo &lt;command&gt;. Command in this case can be any command line command including the ones we have described earlier in the chapter like rm. For example, you could run sudo rm test.txt where we are using super user access to delete the test.txt file. You may also find that you need super user permissions for multiple commands. In this case you can start a command line as a super user by running sudo -i and following the prompts. To exit out of the super user command line you can press CTRL+D or run the command logout or exit. 3.3 Source Code Management with Git Source code management is important to keep centralized versions of your Shiny application source code. By centralizing your source code, you can have one source truth of your application code that can be duplicated anywhere. Source code management also helps to save different versions of a file without creating multiple copies of a file. Git is one of the most popular version control systems for software. In this section, we will be describing how to get started with Git. Git comes installed by default on most Mac OS X and Linux machines. On Windows you can install Git from https://git-scm.com/download/. Try git version in the terminal to test if Git is installed properly. 3.3.1 Git Services Git services help hosting your source code remotely. Two of the most popular Git services are GitHub and GitLab. GitHub is the dominant platform, whereas GitLab is open source can also be self-hosted for free. Both are good choices and the instructions in this section apply to both services. We recommend using GitHub as later in the book we describe continuous deployment and continuous integration (CI/CD) that relies on GitHub Actions. GitLab’s CI/CD pipelines are also excellent, but we will only focus on the most popular service in this book. 3.3.2 Git Commands To follow along with the commands, you will first need to sign up for your Git service. Following sign up, you should create an empty repository which will host your source code. In this book, we will outline how to use Git from the command line. There are also graphical user interfaces (GUIs) for git. However, when connecting remotely to a server, a GUI to use on the server is unavailable. We recommend that you set your default Git text editor to nano with this command: git config --global core.editor \"nano\". Often the default editor is set to Vim or Emacs which are harder to use. 3.3.2.1 Git Setup Git uses a user name and email to associate commits with an identity. The Git user name might or might not be the same as used for GitHub or GitLab. But GitHub uses your Git email address to associate commits with your account on GitHub. Use the git config command to set the user name and email globally (it will be set for every repository on your computer) as: git config --global user.name &quot;Your Name&quot; git config --global user.email &quot;&lt;your-email&gt;&quot; Verify the settings with: git config --global user.name git config --global user.email 3.3.2.2 Cloning Repositories Once you have created a repository, you can clone it to your computer. To do so, you would run git clone &lt;repository url&gt; and enter the appropriate credentials. For example, you can clone the GitHub repository with one of our Shiny app examples, the Old Faithful example as: git clone https://github.com/h10y/faithful.git If you are cloning a private GitHub repository, you must setup an access token to be used as a password for the git clone command. GitHub does not allow you to use your password that you use to log into the GitHub website for security reasons. For private GitLab repositories, you would use the same password to be used as you would sign in on the website, but you can also create an access token. 3.3.2.3 Creating a Commit When you have cloned the repository, you can begin adding file changes via a commit. To begin, you would add or modify files in the repository directory. After you have made changes, you can create a commit using three commands. First, you would run git add . from the root directory of your repository. This command stages your changes for the commit. The . signifies the current directory and all its files and directories in it. Next, you would run git commit -m \"&lt;commit-message&gt;\" to create a commit with a message. The -m flag indicates that you will have a message for your commit. You must enclose your message within quotation marks. A commit must always have a message. Finally, once you have created your commit, you can push your changes with git push. Note that you can only directly commit your changes to a repository if you have write access to it. So you either need to be the owner of that repository, or you have to ask the owner of the repository to grant you write access to the repository. Granting direct write access, however, is rarely a good idea when it comes to outside collaborators. A better approach is to for the repository as a way of collaboration (read more below). 3.3.2.4 Pulling Changes You might have multiple copies of your repository on different computers. In such cases, to keep the changes of files and directories in sync, you can run the git pull command. It should be noted that if you make changes in your repository without committing them, you must commit your changes before performing a pull. If you have committed changes and the repository has been updated elsewhere, merge conflicts may arise. To detect which files have conflicts, you can run the git diff --check command which shows the files and line numbers that have conflicts. The conflicts in the files will appear as &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD at the top with the current changes you have under it. This is separated with ======= and the changes that are conflicting under it. The conflicting changes are concluded with &gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;branch name&gt;. You must delete the &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD and enclosing &gt;&gt;&gt;&gt;&gt;&gt;&gt; &lt;branch name&gt;, keeping only the change that you want and running git commit to resolve any conflicts for your repository. 3.3.2.5 Status of Repository To determine what files you changed and are not tracking, you can run the git status command. This command will give you the files you have modified and files that are not yet tracked by git. This command will also tell you how many changes you have locally that haven’t been pushed to the Git service yet. 3.3.3 .gitignore Often there are extraneous files that are artifacts of running your Shiny application or contain sensitive information, like passwords or access tokens, and do not need to be committed. To ignore these files, you should create a .gitignore file in the root of your repository. To generate a .gitignore file, you can visit https://gitignore.io which will generate a file with the files and directories to ignore for an operating system, programming language, or tool. We recommend selecting: linux, macOS, windows, R, and Python as a base .gitignore. 3.3.4 Branching Branching is a more advanced concept in Git that is useful when you have multiple people working on the same files. In this subsection, we will briefly touch on the basics of Git branching. A branch is a copy of a repository that you can work on and make changes for a specific feature. Once that feature has been completed you can merge it back to your main branch. This allows multiple people to work on the same codebase. To see the branches that exist in your repository, run the git branch command. To create a branch in your repository, run the git checkout -b &lt;branch name&gt; command. It should be noted that when you first try to push your new branch with git push, you will be unsuccessful since the remote repository has no idea of the branch. Therefore you must run git push --set-upstream origin &lt;branch name&gt;. Do not worry if you cannot remember this command! Git will give you a reminder of the command to run to push a new branch. To switch branches, run the git checkout &lt;branch name&gt; command. To merge the changes of one branch to your current branch, run the git merge &lt;branch name&gt; command. There is an option to review merges on GitHub and GitLab. On GitHub, you would open a pull request (or PR for short). On GitLab, you would open a merge request. Both of these options are not part of Git itself, but offered by Git services so that code development can be more collaborative. A pull request or merge request, allows you to preview the merged changes, comment on specific lines of the changes, and fix any conflicts before merging. Often, you can use Git-based workflows to make changes to your Shiny application codebase. The GitHub workflow is a lightweight workflow that leverages pull requests for collaborative development. Developers create feature branches from the main branch, make changes, and then open pull requests to propose these changes for review. Once reviewed and approved, the changes are merged into the main branch. 3.3.4.1 Collaborating with Git Outside collaborators (i.e. people that you do not know but who are kind enough to send you feedback and improvements for free) will not be able to create branches off of a repository and commit to it. Instead, they can go to the repository website (in this case to https://github.com/h10y/faithful/) and create a “fork” of the repository. A fork is a new repository that shares code and visibility settings with the original “upstream” repository. The forked version will live within the namespace of your personal GitHub account (i.e. github.com//faithful/). You have write access to your fork, and can start making changes through commits. To collaborate with the upstream repository (i.e. the source of your fork), you can submit a pull request on GitHub or a merge request on GitLab as if those were branches of the original repository. Once the original repository’s owner checked and approved your request, it will be merged into the upstream code base. You might find that owners will not accept contributions without a Developer Certificate of Origin (DCO) sign-off. The DCO is a per-commit sign-off made by a contributor stating that they agree to the terms published at https://developercertificate.org/ for that particular contribution. If your local Git client is set up with your name and email, you can easily sign off your commits with the -s flag: git commit -s -m \"&lt;commit-message&gt;\". This will add a line at the end of your commit message stating: Signed-off-by: Full Name &lt;email&gt;. 3.4 Working with Servers Your Shiny app deployment is likely going to live on a remote computer that is always running, aptly named a “server”. In this section, we will be explaining how to connect remotely to servers via secure shell (SSH) from the command line. Servers running remotely often do not contain a graphical user interface, and therefore it is recommended to become familiar with the command line. Once connected, you can run the command line commands explained in the previous subsections of this chapter. The commands in the following subsections require the setup of a server. We outline how to do so in Part III /FIXME: check part here/. However, you can also just read along to familiarize yourself with the basic concepts of using a remote server. 3.4.1 Secure Shell (SSH) The secure shell protocol (SSH) is used to securely connect with a remote computer. The ssh command is part of the command line on *nix systems and also part of the Windows Subsystem for Linux. Type ssh and hit enter. It should give you the available options for the command. 3.4.1.1 Password Authentication There are two common ways to authenticate with SSH: using a password, or using a private/public key pair. It is recommended to use the private/public key pair method as it is much harder to guess a private key compared to a password. To authenticate with a password, you can run this command: ssh &lt;user name&gt;@&lt;server ip address&gt;. When you authenticate with a password, a private/public key pair is automatically generated for you. However, this method is less secure because a password is easier to guess compared to a randomly generated key file. 3.4.1.2 Key-based Authentication The connection via private/public key cryptography means that a public key lives on the server and you have a private key on your local machine. The private key on your local machine matches with the public key on the remote machine to verify your identity. To authenticate with a key file, you can run this command: ssh -i &lt;location of key file&gt; &lt;user name&gt;@&lt;server ip address&gt;. By running this command, the remote computer can verify that the user logging in with the public key that is stored on the remote computer. We outline how to generate a key in Part Part III /FIXME: check part here/. 3.5 Readability on the Command Line Sometimes commands are too long or it would help reading them if we could put the parts on multiple lines without “hitting Enter” (executing) too early. This is where the backslash (\\) comes to our aid. We can use the \\ to write multi-line commands. For example these two dummy examples are identical: ssh -i ~/.ssh/id_rsa root@123.456.78.90 ssh -i ~/.ssh/id_rsa \\ root@123.456.78.90 Writing commands this way helps readability as well and you can track your changes with Git better, i.e. changing only parts of a command instead of the whole line. 3.6 Other Useful Tools There are a few tools that are not absolutely necessary but represent improvements and efficiencies when using the command line. Having these tools available can speed up your command line skills because you don’t have to switch context between the command line and a general purpose language to for example parse text files. Here are a few tools that are worth installing: jq is a lightweight and flexible command-line JSON processor, https://jqlang.github.io/jq/. cat echo curl pipe, redirect FIXME: add here other tools that we use in the book. 3.7 Summary You have all the knowledge and tools installed on your local computer to effectively develop your Shiny app and to interact with remote servers and online services. Before we review the different ways of developing and hosting your Shiny app, we quickly introduce a few example apps. We will use these apps to demonstrate the steps for the hosting options. References Lai, Randy. 2023. Languageserver: Language Server Protocol. https://CRAN.R-project.org/package=languageserver. "],["4-part1-examples.html", "Chapter 4 Examples 4.1 Old Faithful 4.2 Bananas 4.3 Load Balancing Test 4.4 Summary", " Chapter 4 Examples Shiny apps come in many different shapes and forms. We will not be able to represent this vast diversity, but instead we wanted some apps that can be used to showcase common patterns, and that can also fit onto the pages of a printed book reasonably well. We will use 3 Shiny apps as examples, all 3 are implemented in both R and Python: faithful: a “Hello Shiny!” app displaying the Old Faithful geyser waiting times data as a histogram with a slider that allows to adjust the number of bins used in the histogram — this app demonstrates the very basics of of reactivity, and it is very short. bananas: an app that classifies the ripeness of banana fruits based on the color composition (green, yellow, brown) — this app demonstrates a more complex use case with dependencies, and the app also relies on a machine learning model, thus it better reflects real world use cases. lbtest: an app to test load balancing when scaling Shiny apps to multiple instances. Let’s learn about the example apps. 4.1 Old Faithful This is the classic “Hello Shiny!” app that you can see in R by trying shiny::runExample(\"01_hello\"). The app displays the Old Faithful geyser waiting times data as a histogram with a slider that allows to adjust the number of bins used in the histogram (Fig. 4.1). The R version of the app was originally written by the Shiny package authors (Chang et al. 2024). The “Hello Shiny!” in R has no dependencies other than shiny. The Old Faithful app in Python has more requirements besides shiny, because the Python standard library does not have the geyser data readily available, and you need e.g. matplotlib (Hunter 2007) for the histogram. We wrote the Python version as a mirror translation of the R version, so that you can see the similarities and the differences. Figure 4.1: The faithful example Shiny app. In R, the data set datasets::faithful (R Core Team 2024) contains waiting time between eruptions and the duration of the eruption for the Old Faithful geyser in Yellowstone National Park, Wyoming, USA. We got the Python data set from the Seaborn library seaborn.load_dataset(\"geyser\") (Waskom 2021). The source code for the different builds of the Old Faithful Shiny app is at https://github.com/h10y/faithful. You can download the GitHub repository az a zip file from GitHub, or clone the repository with git clone https://github.com/h10y/faithful.git. 4.2 Bananas The bananas app was born out of a “stuck-in-the-house” COVID-19 project when one of the authors bought some green bananas at the store and took daily photographs of each fruit. Later, the data set was used as part of teaching a workshops. The motivation for the app is that it follows a workflow that is fairly common in all kinds of data science projects: Have a question to answer: Is my banana ripe? Collect data: Go to the store, buy bananas, set up a ring light and take pictures every day over 3 weeks. Compile the training data: Classify colour pixels and calculate the relative proportions, score pictures according to ripeness status. Run exploratory data analysis: Let’s explore and visualize the data set. Train a classification model: Estimate the the banana ripeness class and probability given the colour composition. Build a “scoring engine”: Given some colour inputs for a new fruit, tell me the probability for the ripeness classes. Build a user interface: Let a non technical user to do the data exploration and classification as part of a web application. 4.2.1 The Bananas Data Set The data set tracks the ripening colour composition of banana fruits daily over a 3-week period. The full data set can be found in the GitHub repository and R package bananas (install.packages(\"bananas\", repos = \"https://psolymos.r-universe.dev\")). The subset used in the book and the Shiny app constitutes the 6 fruits that were kept at room temperature. The table has the following fields: fruit: the identifier of the fruit, day: number between 0 and 20, the number of days since the first set of photographs, ripeness: the ripeness class of the fruit based in Péter’s personal judgement (Under, Ripe, Very, Over), green, yellow, brown: colour composition, these 3 values add up to 1 (100%). The colour composition was determined based on colour mapping the pixel values of the banana fruits and converting the pixel based 2-dimensional area to proportions. The following summary presents the ripeness and the percentage values of green, yellow, brown colours. Figure 4.2 shows the change in colour composition over the 3 weeks of the experiment. You can see that the proportion of green colour went down, parallel to that the yellow colour proportion peaked around day 5. Yellow started decreasing after that while the proportion of brown started increasing. We can also present the same information according to the ripeness classes (Fig. 4.3). You can see that the under-ripe class is characterized by high green proportion and the absence of brown. The ripe class is characterized by the highest proportion of yellow. Very ripe bananas have higher proportion of brown while yellow colour is still the most common. Over ripe bananas are mostly brown. Figure 4.2: Colour composition of the bananas over time. Figure 4.3: Colour composition of the bananas by ripeness class. 4.2.2 Model Training We chose Support Vector Machines (SVM) to model a multi-level response variable (Under, Ripe, Very, Over) as a function of the green, yellow, and brown colours. We used the e1071 package (Meyer et al. 2023) in R, and the SVM model’s prediction accuracy was 90.8%. We saved the trained model object as an R binary .rds file: library(e1071) # Read the bananas data x &lt;- read.csv(&quot;bananas.csv&quot;) x$ripeness &lt;- factor(x$ripeness, c(&quot;Under&quot;, &quot;Ripe&quot;, &quot;Very&quot;, &quot;Over&quot;)) # Multinomial classification with Support Vector Machines m &lt;- svm(ripeness ~ green + yellow + brown, data = x, probability = TRUE ) # Two-way table to test prediction accuracy table(x$ripeness, predict(m)) sum(diag(table(x$ripeness, predict(m)))) / nrow(x) # Predict ripeness class predict(m, data.frame(green = 1, yellow = 0, brown = 0), probability = TRUE) predict(m, data.frame(green = 0, yellow = 1, brown = 0), probability = TRUE) predict(m, data.frame(green = 0, yellow = 0, brown = 1), probability = TRUE) predict(m, data.frame(green = 0.1, yellow = 0.2, brown = 0.7), probability = TRUE) # Save the model object saveRDS(m, &quot;bananas-svm.rds&quot;) We can fit a similar SVM model in Python using scikit-learn (sklearn) (Pedregosa et al. 2011): import pandas as pd from joblib import dump from sklearn import svm # Global x = pd.read_csv(&#39;bananas.csv&#39;) # Train SVM x.loc[x.ripeness == &#39;Under&#39;, &#39;target&#39;] = 0 x.loc[x.ripeness == &#39;Ripe&#39;, &#39;target&#39;] = 1 x.loc[x.ripeness == &#39;Very&#39;, &#39;target&#39;] = 2 x.loc[x.ripeness == &#39;Over&#39;, &#39;target&#39;] = 3 data_X = x[[&#39;green&#39;, &#39;yellow&#39;, &#39;brown&#39;]].to_numpy() data_y = x.target.values svm_model = svm.SVC(probability = True) svm_model.fit(data_X, data_y) #&#39; Predict ripeness class svm_model.predict_proba([[1, 0, 0]]) svm_model.predict_proba([[0, 1, 0]]) svm_model.predict_proba([[0, 0, 1]]) svm_model.predict_proba([[0.1, 0.2, 0.7]]) # Write model object to file dump(svm_model, &#39;bananas-svm.joblib&#39;) 4.2.3 The Shiny App The Shiny app consists of a ternary plot showing the daily colour composition of each banana fruit, alongside the new point to be classified (in red), as shown in Figure 4.4. The three numeric inputs on the left hand side of the plot control the position of the red dot. The classification results based on these inputs are shown on the right hand side of the ternary plot. You can see probabilities of under-ripe, ripe, very ripe, and over-ripe classes, and the class with highest probability is assigned as a label. Figure 4.4: The bananas example Shiny app. The source code for the different builds of the Bananas Shiny app is at https://github.com/h10y/bananas. You can download the GitHub repository az a zip file from GitHub, or clone the repository with git clone https://github.com/h10y/bananas.git. 4.3 Load Balancing Test Shiny apps can run multiple sessions in the same app instance. A common problem when scaling the number of replicas for Shiny apps is that traffic might not be sent to the same session and thus the app might randomly fail. This app is used to determine if the HTTP requests made by the client are correctly routed back to the same R or Python process for the session. Both the Python and the R version of the app registers a dynamic route for the client to try to connect to. The JavaScript code on the client side will repeatedly hit the dynamic route. The server will send a 200 OK status code only if the client reached the correct Shiny session, where it originally came from (Fig. 4.5). Figure 4.5: The lbtest example Shiny app. The original Python app was written by Joe Cheng and is from the rstudio/py-shiny GitHub repository. We wrote the R version to mirror the Python version. This app will be useful when the deployment includes load balancing between multiple replicas. For such deployments, session affinity (or sticky sessions) needs to be available. This app can be used to test such setups. If the test fails, it will stop before the counter reaches 100 and will say Failure! If the app succeeds 100 times, you’ll see Test complete. The app is not useful for testing a single instance deployment, or with Shinylive, because these setups won’t fail, but you can still try it. The source code for the different builds of the load balancing test Shiny app is at https://github.com/h10y/lbtest. You can download the GitHub repository az a zip file from GitHub, or clone the repository with git clone https://github.com/h10y/lbtest.git. 4.4 Summary This is the end of Part I. We covered all the fundamentals that the rest of the book builds upon. In the next part, we’ll cover all the technical details of Shiny hosting that happens on your local machine. We recommend getting the example repositories mentioned in this chapter available on your computer. This way you will be able to follow all the examples from the following chapters and won’t have to copy paste the text from the book to files. Visit the GitHub organization h10y which stands for hostingshiny (there are 10 letters between the first h and the last y): https://github.com/h10y/. References Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2024. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny. Hunter, J. D. 2007. “Matplotlib: A 2D Graphics Environment.” Computing in Science &amp; Engineering 9 (3): 90–95. https://doi.org/10.1109/MCSE.2007.55. Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2023. E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071. Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Waskom, Michael L. 2021. “Seaborn: Statistical Data Visualization.” Journal of Open Source Software 6 (60): 3021. https://doi.org/10.21105/joss.03021. "],["5-part2-developing-shiny-apps.html", "Chapter 5 Developing Shiny Apps 5.1 Creating a Shiny App 5.2 Organizing Shiny Apps 5.3 Running Shiny Apps Locally 5.4 Sharing the Shiny App Code 5.5 Summary", " Chapter 5 Developing Shiny Apps Shiny apps can be written in two languages, R and Python. These 2 programming languages are commonly used for data analysis. R is an interpreted language for statistical computing and graphics. Likewise, Python is an interpreted general programming language that is often used for data science. Both R and Python run on a wide variety of operating systems including Windows, Mac OS X, and Linux. In this chapter, we will cover getting started with developing in Shiny using R and Python environments. We discuss the tools commonly used for these programming languages and provide instructions on how to run our example Shiny application projects in an integrated development environment (IDE). Then we’ll review some of the easiest ways of sharing the Shiny apps. 5.1 Creating a Shiny App A Shiny app is made up of the user interface (UI) and the server function. The UI and the server can be written in pure R or Python, but it can also incorporate JavaScript, CSS, HTML, or Markdown code. The app is served to the client (app user) through a host (Internet Protocol or IP address) and port number. The server then keeps a websocket connection open to receive requests. The Shiny session behind the app will make sure this request translates into the desired interactivity and sends back the response, usually an updated object, like a plot or a table (Fig. 5.1). Figure 5.1: Simplified Shiny app architecture. The Old Faithful app is a relatively simple example that is concise enough to demonstrate the structure of Shiny apps with the basics of reactivity. It draws a histogram based on the Old Faithful geyser waiting times in the Yellowstone National Park. The number of bins in the histogram can be changed by the user with a slider. The source code for the different builds of the Old Faithful Shiny app is at https://github.com/h10y/faithful/. You can download the GitHub repository az a zip file from GitHub, or clone the repository with git clone https://github.com/h10y/faithful.git. The repository should be inside the faithful folder. We’ll refer to the files inside the folder using relative paths. You can either open these files, or follow the instructions for creating the folders and files fresh. To run this app in R, create a folder called r-shiny with a new file called app.R inside the folder. Put this inside the file: # r-shiny/app.R library(shiny) x &lt;- faithful$waiting app_ui &lt;- fixedPage( title = &quot;Old Faithful&quot;, h2(&quot;Old Faithful&quot;), plotOutput(outputId = &quot;histogram&quot;), sliderInput( inputId = &quot;n&quot;, label = &quot;Number of bins:&quot;, min = 1, max = 50, value = 25, ticks = TRUE ) ) server &lt;- function(input, output, session) { output$histogram &lt;- renderPlot( alt = &quot;Histogram of waiting times&quot;, { hist( x, breaks = seq(min(x), max(x), length.out = input$n + 1 ), freq = TRUE, col = &quot;#BB74DB&quot;, border = &quot;white&quot;, main = &quot;Histogram of waiting times&quot;, xlab = &quot;Waiting time to next eruption [mins]&quot;, ylab = &quot;Frequency&quot; ) box() } ) } shinyApp(ui = app_ui, server = server) For the Python version, create a new file called app.py inside the py-shiny folder and put this inside the file: # py-shiny/app.py import seaborn as sns import matplotlib.pyplot as plt from shiny import App, render, ui faithful = sns.load_dataset(&quot;geyser&quot;) x = faithful.waiting app_ui = ui.page_fixed( ui.panel_title(&quot;Old Faithful&quot;), ui.output_plot(id = &quot;histogram&quot;), ui.input_slider( id=&quot;n&quot;, label=&quot;Number of bins:&quot;, min=1, max=50, value=25, ticks=True ), ) def server(input, output, session): @render.plot(alt=&quot;Histogram of waiting times&quot;) def histogram(): plt.hist( x, bins = input.n(), density=False, color=&quot;#BB74DB&quot;, edgecolor=&quot;white&quot;) plt.title(&quot;Histogram of waiting times&quot;) plt.xlabel(&quot;Waiting time to next eruption [mins]&quot;) plt.ylabel(&quot;Frequency&quot;) app = App(ui = app_ui, server = server) Besides Shiny, you’ll need to have the seaborn library installed to load the dataset and the matplotlib library installed to visualize the geyser data set with a histogram. R has these functions as part of the base distribution, so no additional installation is needed. You can install the Python dependencies from the requirements.txt file with pip install -r py-shiny/requirements.txt. Here are the contents of the requirements.txt file: # py-shiny/requirements.txt shiny&gt;=0.10.2 matplotlib seaborn You have probably noticed the similarities between the R and Python versions. Both begin by loading/importing libraries and defining a globally available variable that contains the Old Faithful geyser waiting times. The files then define the user interface (app_ui) and the server function. At the end, the Shiny app is defined as shinyApp(ui = app_ui, server = server) and App(ui = app_ui, server = server). Now let us explore the user interface and the server function. 5.1.1 The User Interface The user interface (UI) object controls the layout and appearance of the Shiny app. The UI in R is defined as and object called app_ui: app_ui &lt;- fixedPage( title = &quot;Old Faithful&quot;, h2(&quot;Old Faithful&quot;), plotOutput(outputId = &quot;histogram&quot;), sliderInput( inputId = &quot;n&quot;, label = &quot;Number of bins:&quot;, min = 1, max = 50, value = 25, ticks = TRUE ) ) The fixedPage() function renders the main Shiny interface, a plot output is nested inside of it alongside the range slider input. The slider with the ID \"n\" controls the number of bins in the histogram (ranging between 1 and 50, initial value set to 25). The plot with ID \"histogram\" will show the distribution of the waiting times. If we print the app_ui object, we get the following (slightly edited) HTML output where you can see how the attributes from the R code translate to arguments in the HTML version: &lt;div class=&quot;container&quot;&gt; &lt;h2&gt; Old Faithful &lt;/h2&gt; &lt;div id=&quot;histogram&quot;&gt; &lt;/div&gt; &lt;div class=&quot;form-group shiny-input-container&quot;&gt; &lt;label class=&quot;control-label&quot; id=&quot;n-label&quot; for=&quot;n&quot;&gt; Number of bins: &lt;/label&gt; &lt;input class=&quot;js-range-slider&quot; id=&quot;n&quot; data-min=&quot;1&quot; data-max=&quot;50&quot; data-from=&quot;25&quot; data-step=&quot;1&quot; data-grid=&quot;true&quot;/&gt; &lt;/div&gt; &lt;/div&gt; The &lt;div&gt; HTML tag stands for division, and most opening tags are followed by a closing tag, i.e. &lt;/div&gt;. HTML defines a nested structure. You can see the outermost division with the container class. The second level header, the plot and the slider are nested inside this outermost division. This HTML snippet is going to be added to the body of the HTML page rendered by Shiny. The final HTML page will also contain all the JavaScript and CSS dependencies required to make the app interactive and styled properly. The Python UI uses the ui object imported from shiny. The setup is very similar to the R setup, but naming conventions are slightly different. Python uses the snake case naming convention (e.g. page_fixed, output_plot, and input_slider) whereas R uses the camel case naming convention (e.g. fixedPage, plotOutput, and sliderInput). app_ui = ui.page_fixed( ui.panel_title(&quot;Old Faithful&quot;), ui.output_plot(id = &quot;histogram&quot;), ui.input_slider( id=&quot;n&quot;, label=&quot;Number of bins:&quot;, min=1, max=50, value=25, ticks=True ), ) Printing the app_ui in Python gives the following (slightly edited) HTML output: &lt;html&gt; &lt;head&gt; &lt;/head&gt; &lt;body&gt; &lt;div class=&quot;container&quot;&gt; &lt;h2&gt; Old Faithful &lt;/h2&gt; &lt;div id=&quot;histogram&quot;&gt; &lt;/div&gt; &lt;div class=&quot;form-group shiny-input-container&quot;&gt; &lt;label class=&quot;control-label&quot; id=&quot;n-label&quot; for=&quot;n&quot;&gt; Number of bins: &lt;/label&gt; &lt;input class=&quot;js-range-slider&quot; id=&quot;n&quot; data-min=&quot;1&quot; data-max=&quot;50&quot; data-from=&quot;25&quot; data-step=&quot;1&quot; data-grid=&quot;true&quot;/&gt; &lt;/div&gt; &lt;/div&gt; &lt;/body&gt; &lt;/html&gt; The only difference relative to the R output is that you can see the &lt;html&gt;, &lt;head&gt; and &lt;body&gt; tags. Shiny will inject elements into the HTML head later. 5.1.2 The Server Function The server function contains the instructions for the reactivity needed for the Shiny app. The server function takes mainly two arguments: input and output. Sometimes the server function also takes session. These reactive objects are created by Shiny and passed to the server function. input is used to pass the control values, in this case, input$n, the number of histogram bins: server &lt;- function(input, output, session) { output$histogram &lt;- renderPlot( alt = &quot;Histogram of waiting times&quot;, { hist( x, breaks = seq(min(x), max(x), length.out = input$n + 1 ), freq = TRUE, col = &quot;#BB74DB&quot;, border = &quot;white&quot;, main = &quot;Histogram of waiting times&quot;, xlab = &quot;Waiting time to next eruption [mins]&quot;, ylab = &quot;Frequency&quot; ) box() } ) } The output object contains the reactive output objects, in our case the rendered plot. input and output together describe the state of the app. Changes in input (input$n here) will invalidate reactive objects that reference these reactive dependencies and cause the relevant render functions (renderPlot() here) to re-execute. We can see some differences in the Python version. Shiny for Python uses decorators (e.g. @render) instead of render functions and inputs are invoked as input.n(). See the Framework Comparisons section of the Shiny for Python documentation for a detailed overview of R vs. Python similarities and differences. def server(input, output, session): @render.plot(alt=&quot;Histogram of waiting times&quot;) def histogram(): plt.hist( x, bins = input.n(), density=False, color=&quot;#BB74DB&quot;, edgecolor=&quot;white&quot;) plt.title(&quot;Histogram of waiting times&quot;) plt.xlabel(&quot;Waiting time to next eruption [mins]&quot;) plt.ylabel(&quot;Frequency&quot;) The server function is called once for each Shiny session in Python. Like the R version of Shiny, it also takes in arguments: input, output, and session. The input object stores reactive values. For example, input.n() means that when a reactive value input.n() is changed, a reactive function that uses input.n() will be triggered to rerender. The reactive function for input.n() in the Python code is histogram which is made reactive with the render.plot Python decorator. Python decorators are a design pattern to modify a function by wrapping a function into another function. For example, the @render.plot decorator is a function that wraps the histogram function making it a reactive expression. The histogram function creates a plot, and the @render.plot attempts to retrieve the created plot by the histogram and renders it as histogram to the output object that can be called by a Shiny ui object. The use of the output object is similar to R, where reactive output objects such as histogram are stored. In short, like R Shiny, input and output together describe the state of the app. When changes are made to an input, their corresponding reactive expressions are re-executed and their results are stored in the output object. Finally, session refers to a connection made by a client to the Shiny application. A new session with a new websocket connection is created every time a web browser connects to the Shiny application. It should be noted that code outside the server function runs once per application startup and not per user session. 5.1.3 Shiny Express Python for Shiny has two different syntax options, Shiny Core that you saw in the previous sections, and Shiny Express. Shiny Core drew inspiration from the original Shiny for R framework, but is not a literal port of Shiny for R. Shiny Express was introduced quite recently, and is focused on making it easier for beginners to use Shiny, and might feel more natural to Python users. Shiny Core offers the separation between the UI and the server components, making it easier to organize code for larger Shiny apps. The server function declaration also helps separating code that should only run at startup vs. for each session. In Shiny Express, all of the code in the app file is executed for each session. There is only one Shiny syntax option in R. 5.2 Organizing Shiny Apps The previously presented faithful app is organized as a single file. The file contained all the globally scoped declarations at the top, the definition of the UI object and the server function, and ended with the Shiny app object. As Shiny apps grow from demo examples to full on data science projects, the increased complexity will necessitate the organization of the code. You can organize the code into multiple files, or even as a package. Let’s see the most common patterns. 5.2.1 Single file When Shiny is organized in a single file, the convention is to name it app.R. This way your IDE (RStudio or VS Code) will recognize that it is a Shiny app. Apart from this convenience, the file can be named anything, e.g. faithful_app.R. The single file follows the following structure: # Load libraries library(shiny) # Define global variables x &lt;- [...] # Define the UI app_ui &lt;- [...] # Define the server server &lt;- function(input, output, session) { [...] } # Assemble the Shiny app shinyApp(ui = app_ui, server = server) At the end of the file, we define the Shiny app using shinyApp(). To run the app in R, we either have to source the app.R or provide the file name as an argument to the runApp() function, e.g. runApp(\"r-shiny/app.R\"). The Python version takes a very similar form as a single file, usually named as app.py. # Load libraries from shiny import App, render, ui [...] # Define global variables x = [...] # Define the UI app_ui = [...] # Define the server def server(input, output, session): [...] # Assemble the Shiny app app = App(ui = app_ui, server = server) You can run the Python Shiny app in your IDE or by using the shiny run command in the terminal, shiny run --reload --launch-browser py-shiny/app.py. This will launch the app in the browser and the server will watch for changes in the app source code and rerender. The libraries and global variables will be accessible for all the Shiny sessions by sourcing the app file when you start the Shiny app. Variables defined inside the server functions will be defined for each session. This way, one user’s changes of the slider won’t affect the other user’s experience. However, if one user changes the globally defined variables (i.e. using the &lt;&lt;- assignment operator in R), those changes will be visible in every user’s session. 5.2.2 Multiple Files If your app is a bit more complex, you might have multiple files in the same directory. By convention, the directory contains at least a server.R file and ui.R file. Sometimes, there is a third file called global.R. The global.R file is used to load packages, data sets, set variables, or define functions that are available globally. The directory can also have a www folder inside that can store assets (files, images, icons). Another folder is called R that can hold R scripts that are sourced before the app starts up. This is usually the place to put helper functions and Shiny modules, which are also functions. If you prefer, you can use the source() function to explicitly source files as part of the global.R script. Just don’t put these files in the R folder to avoid sourcing them twice. The Bananas Shiny app is organized into multiple files. The source code for the different builds of the Bananas app is at https://github.com/h10y/bananas/. Download or clone the GitHub repository with git clone https://github.com/h10y/bananas.git. The repository should be inside the bananas folder. Here is how the folder structure looks like for the R version of the Bananas app: bananas/r-shiny ├── R │ └── functions.R ├── bananas-svm.rds ├── bananas.csv ├── dependencies.json ├── global.R ├── server.R └── ui.R The global.R file looks like this: # bananas/r-shiny/global.R library(shiny) library(plotly) library(e1071) x &lt;- read.csv(&quot;bananas.csv&quot;) x$ripeness &lt;- factor(x$ripeness, c(&quot;Under&quot;, &quot;Ripe&quot;, &quot;Very&quot;, &quot;Over&quot;)) m &lt;- readRDS(&quot;bananas-svm.rds&quot;) Apart from loading libraries, we read in a CSV file, set factor levels so that those print in a meaningful order instead of alphabetical. Finally, we load the model we trained earlier in 4.2.2. There is also the file functions.R in the R folder that gets sourced automatically. It is important to note, that functions defined inside the files of the R folder, or anything that you source() (e.g. source(\"R/functions.R\")) will be added to the global environment. If you want a sourced file to have local scope, you can include that for example inside your server function as source(\"functions.R\", local = TRUE). To run this app, you can click the Run App button the the IDE or use runApp(\"&lt;app-directory&gt;\") as long as the directory contains the server.R and the ui.R files. The choice between single vs. multiple files comes down to personal preference and the complexity of the Shiny app. You might start with a single file, but as the file gets larger, you might decide to save the pieces into their own files. Keeping Shiny apps in their own folder is generally a good idea irrespective of having single or multiple files in the folder. This way, changing your mind later won’t affect how you run the app. You can just use the same runApp(\"&lt;app-directory&gt;\") command, if you follow these basic naming conventions. The Python version of the Bananas app is also split into multiple files: bananas/py-shiny ├── app.py ├── bananas-svm.joblib ├── bananas.csv ├── functions.py └── requirements.txt Separating files works slightly different in Python. Instead of sourcing scripts inline like you saw for R, you must import the objects and variables from separated files similar to importing from libraries as Python considers a .py file as a “module”. These are the first few lines of the app.py file: # bananas/py-shiny/app.py from shiny import App, render, reactive, ui import functions [...] We import objects from shiny, then import everything from the functions.py file into the functions namespace which is used to define plotting helper functions. To call anything that appeared in the functions.py in app.py, we prepend functions. to any functions or objects in functions.py: # bananas/py-shiny/app.py [...] ternary = functions.go.FigureWidget( data=[ functions.trace_fun( x[x.ripeness == &quot;Under&quot;], &quot;#576a26&quot;, &quot;Under&quot; ), functions.trace_fun( x[x.ripeness == &quot;Ripe&quot;], &quot;#eece5a&quot;, &quot;Ripe&quot; ), functions.trace_fun( x[x.ripeness == &quot;Very&quot;], &quot;#966521&quot;, &quot;Very&quot; ), functions.trace_fun( x[x.ripeness == &quot;Over&quot;], &quot;#261d19&quot;, &quot;Over&quot; ), functions.trace_fun(pd.DataFrame([{ [...] To run a Python Shiny app that is in multiple files, you still need to specify the file that has the Shiny app object defined that you want to run, shiny run &lt;app-directory&gt;/app.py. Or if the file is called app.py and the app object is called app, you can use shiny run from the current working directory. As Shiny for Python apps become more widespread in the future, we will see many different patterns emerge with best practices for organizing files. 5.2.3 Shiny App with Nested File Structure Your app can grow more complex over time, and you might find the multiple-file structure described above to be limiting. You might have Shiny modules inside Shiny modules. Such a setup might lend itself to a hierarchical file structure. If this is the case, you can use the Rhino Shiny framework(Rhino) and the rhino R package (Żyła et al. 2024). This Shiny framework was inspired by importing and scoping conventions of the Python and JavaScript languages. Rhino enforces strong conventions using a nested file structure and modularized R code. Rhino also uses the box package (Rudolph 2024) that defines a hierarchical and composable module system for R. Here is the directory structure for the Rhino version of the Faithful app from inside the Faithful GitHub repository’s r-rhino folder: r-rhino ├── app │ ├── main.R │ └── static │ └── favicon.ico ├── app.R ├── config.yml ├── dependencies.R └── rhino.yml The app/static folder serves a similar purpose to the www folder. The R code itself is in the app.R folder, specifically the app/main.R file. You can see how the import statement is structured at the beginning, and how a Shiny module is used for the ui and server: box::use( shiny[fixedPage, moduleServer, NS, plotOutput, sliderInput, renderPlot, h2], graphics[hist, box], datasets[faithful], ) x &lt;- faithful$waiting #&#39; @export ui &lt;- function(id) { ns &lt;- NS(id) fixedPage( [...] ) } #&#39; @export server &lt;- function(id) { moduleServer(id, function(input, output, session) { output$histogram &lt;- renderPlot( [...] ) }) } To run this app, you can call shiny::runApp(), the app.R file contains a single line calling rhino::app() which creates the Shiny app object. The developers of the framework also released a very similar Python implementation called Tapyr. 5.2.4 Programmatic Cases In R, if you want to run the Shiny app as part of another function, you can supply a list with ui and server components (i.e. runApp(list(ui = app_ui, server = server))) or a Shiny app object created by the shinyApp() function (i.e. runApp(shinyApp(ui, server))). Note that when shinyApp() is used at the R console, the Shiny app object is automatically passed to the print() function, or more specifically, to the shiny:::print.shiny.appobj function, which runs the app with runApp(). If shinyApp() is called in the middle of a function, the value will not be passed to the print method and the app will not be run. That is why you have to run the app using runApp(). For example, we can write the following function where app_ui and server are defined above as part of the single-file faithful Shiny app. The ... passes possible other arguments to runApp such as the host or port that we will discuss later. run_app &lt;- function(...) { runApp( shinyApp( ui = app_ui, server = server ), ... ) } Start the app by typing run_app() into the console. 5.2.5 Shiny App as an R Package Extension packages are the fundamental building blocks of the R ecosystem. Apps can be hosted on the Comprehensive R Archive Network (CRAN), on GitHub, etc. The tooling around R packages makes checking and testing these packages easy. If you have R installed, you can run R CMD check &lt;package-name&gt; to test your package that might include a tests folder with unit tests. Including Shiny apps in R packages is quite commonplace nowadays. These apps might aid data visualization, or simplify calculations for not-so-technical users. Sometimes the Shiny app is not the main feature of a package, but rather it is more like an extension or a demo. In such cases, you might decide to put the Shiny app into the inst folder of the package. This will make the app available after installation, but the app’s code will skip any checks. A consequence is that some dependencies of the app might not be available, because that is not verified during standard checks. At the time of installation, the contents of the inst folder will be copied to the package’s root folder. Therefore, such an app can be started as e.g.  shiny::runApp(system.file(\"app\", package = \"faithful\")). This means that there is a package called faithful, and in the inst/app folder you can find the Shiny app. The r-package folder of the Faithful repository contains an R package called faithful. This is the folder structure of the package: faithful ├── DESCRIPTION ├── LICENSE ├── NAMESPACE ├── R │ └── run_app.R ├── inst │ └── app │ ├── global.R │ ├── server.R │ ├── ui.R │ └── www │ └── favicon.ico └── man └── run_app.Rd We will not teach you how to write an R package. For that, see R’s official documentation about Writing R Extensions, or Hadley and Bryan (2023). The most important parts of the R package are the functions inside the R folder and the DESCRIPTION file, that describes the dependencies of the package: Package: faithful Version: 0.0.1 Title: Old Faithful Shiny App Author: Peter Solymos Maintainer: Peter Solymos &lt;[...]&gt; Description: Old Faithful Shiny app. Imports: shiny License: MIT + file LICENSE Encoding: UTF-8 RoxygenNote: 7.3.1 The inst folder contains the Shiny app, the man folder has the help page for our run_app function. The run_app.R file has the following content: #&#39; Run the Shiny App #&#39; #&#39; @param ... Arguments passed to `shiny::runApp()`. #&#39; @export run_app &lt;- function(...) { shiny::runApp(system.file(&quot;app&quot;, package = &quot;faithful&quot;), ...) } The #' style comments are used to add the documentation next to the function definition, which describes how other parameters can be passed to the shiny::runApp function. The @export tag signifies that the run_app function should be added to the NAMESPACE file by the roxygen2 package (Wickham et al. 2024). Calling R CMD build faithful from inside the r-package folder will build the faithful_0.0.1.tar.gz source file. You can install this package using install.packages(\"faithful_0.0.1.tar.gz\", repos = NULL) from R or you can use the R command line utility: R CMD INSTALL faithful_0.0.1.tar.gz. Once the package is installed, you can call faithful::run_app() to start the Old Faithful example. If you want to include the app as part of the package’s functions, place it in the package’s R folder. In this case, shiny and all other packages will have to be mentioned in the package’s DESCRIPTION file, that describes the dependencies, as packages that the package imports from. Best practices can be found about writing R packages (Hadley and Bryan 2023) and about engineering Shiny apps using (Fay et al. 2021). You can not only test the underlying functions as part of the package, but you can apply Shiny specific testing tools, like shinytest2 (Schloerke 2024). An R package provides a structure to follow, and everything becomes a function. Including Shiny apps in R packages this way is much safer, and this is the approach that some of the most widely used Shiny development frameworks took. These are the golem (Fay et al. 2023), and the leprechaun (John Coene 2022) packages. 5.2.5.1 Golem The use and benefits of the Golem framework are described in the book Engineering Production-Grade Shiny Apps by Fay et al. (2021). Golem is an opinionated framework for building a production-ready Shiny apps by providing a series of tools for developing you app, with an emphasis on writing Shiny modules. A Golem app is contained inside an R package. You’ll have to know how to build a package, but this is the price to pay for having mature and trusted tools for testing your package from every aspect. Let’s review how the Golem structure compares to the previous setup. Look for the package inside the r-golem folder of the Faithful GitHub repository. We will call this R package faithfulGolem: # faithfulGolem ├── DESCRIPTION ├── LICENSE ├── NAMESPACE ├── R │ ├── app_config.R │ ├── app_server.R │ ├── app_ui.R │ ├── mod_histogram.R │ └── run_app.R ├── dev │ ├── 01_start.R │ ├── 02_dev.R │ ├── 03_deploy.R │ └── run_dev.R ├── inst │ ├── app │ │ └── www │ │ └── favicon.ico │ └── golem-config.yml └── man └── run_app.Rd The most important difference is that we see the UI and server added to the R folder as functions, instead of plain script files in the inst folder. The dev folder contains development related boilerplate code and functions to use when testing the package without the need to reinstall after every tiny change you make to the Shiny app or to the R package in general. The inst folder has the static content for the app with the www folder inside. The DESCRIPTION file looks like this: Package: faithfulGolem Title: Old Faithful Shiny App Version: 0.0.1 Author: Peter Solymos Maintainer: Peter Solymos &lt;[...]&gt; Description: Old Faithful Shiny app. License: MIT + file LICENSE Imports: config (&gt;= 0.3.2), golem (&gt;= 0.4.1), shiny (&gt;= 1.8.1.1) Encoding: UTF-8 RoxygenNote: 7.3.1 Notice that the config and golem packages are now part of the list of dependencies with the package versions explicitly mentioned to avoid possible backwards compatibility issues. Let’s take a look at the UI and server functions. The app_ui function returns the UI as a tags list object. You might notice that we use a module UI function here: app_ui &lt;- function(request) { tagList( # Leave this function for adding external resources golem_add_external_resources(), # Your application UI logic fixedPage( title = &quot;Old Faithful&quot;, h2(&quot;Old Faithful&quot;), mod_histogram_ui(&quot;histogram_1&quot;) ) ) } The app_server function loads the Old Faithful data set and calls the histogram module’s server function that uses the same \"histogram_1\" identified as the module UI function, plus it also takes the data set as an argument too: app_server &lt;- function(input, output, session) { x &lt;- datasets::faithful$waiting mod_histogram_server(&quot;histogram_1&quot;, x) } So what does this module look like? That is what you can find in the R/mod_histogram.R file that defines the mod_histogram_ui and mod_histogram_server functions: mod_histogram_ui &lt;- function(id) { ns &lt;- NS(id) tagList( plotOutput(outputId = ns(&quot;histogram&quot;)), sliderInput( inputId = ns(&quot;n&quot;), label = &quot;Number of bins:&quot;, min = 1, max = 50, value = 25, ticks = TRUE ) ) } mod_histogram_server &lt;- function(id, x) { moduleServer(id, function(input, output, session) { ns &lt;- session$ns output$histogram &lt;- renderPlot( alt = &quot;Histogram of waiting times&quot;, { graphics::hist( x, breaks = seq(min(x), max(x), length.out = input$n + 1 ), freq = TRUE, col = &quot;#BB74DB&quot;, border = &quot;white&quot;, main = &quot;Histogram of waiting times&quot;, xlab = &quot;Waiting time to next eruption [mins]&quot;, ylab = &quot;Frequency&quot; ) graphics::box() } ) }) } After building, checking, and installing the faithfulGolem R package, you’ll be able to start the Shiny app by calling faithfulGolem::run_app() from R. 5.2.5.2 Leprechaun The leprechaun R package (John Coene 2022) uses a similar philosophy to creating Shiny applications as packages. It comes with a full set of functions that help you with modules, custom CSS, and JavaScript files. When using this package, you will notice that leprechaun does not become a dependency in the DESCRIPTION file, unlike in the case of golem. Apart from this and some organization choices, the two packages and the workflow provided by them are very similar. Choose that helps you more in terms of your app’s specific needs. Say we name the the R package containing the Old Faithful example as faithfulLeprechaun. This folder is inside the r-leprechaun folder of the Faithful repository. The main functions that are defined should be already familiar: # R/ui.R ui &lt;- function(req) { fixedPage( [...] ) } # R/server.R server &lt;- function(input, output, session) { x &lt;- datasets::faithful$waiting output$histogram &lt;- renderPlot( [...] ) } # R/run.R run &lt;- function(...) { shinyApp( ui = ui, server = server, ... ) } After the package is installed, the way to run the app is to call the faithfulLeprechaun::run() function. 5.2.6 Dynamic Documents Dynamic documents stem from the literate programming paradigm (Knuth 1992), where natural language (like English) is interspersed with computer code snippets. Nowadays, dynamic documents are used to create technical reports, slide decks for presentations, and books, like the one you are reading. Markdown is a common plain-text format for such dynamic documents, because it can be compiled into many different formats using Pandoc. R Markdown builds upon previous literate programming examples, e.g. Sweave that mixes R and (Leisch 2002), and the flexibility provided by Pandoc and the markdown format. R Markdown contains chunks of embedded R (or other) code between opening and closing triple backticks. Underneath, you can find the rmarkdown (Allaire et al. 2024) and knitr (Xie 2024) R packages at work. A more recent iteration of this idea is Quarto. Quarto is an open-source scientific and technical publishing system that can include code chunks in many different formats frequently used by data scientists, e.g. R, Python, Julia, Observable. Both R Markdown and Quarto let you to use Shiny inside the documents to build lightweight apps without worrying too much about a user interface. Such interactive HTML documents cannot provide the same flexibility for designing your apps as a standard Shiny app would, but it works wonders for simpler use cases. Let’s review how you can use Shiny in R Markdown (.Rmd) and Quarto (.qmd) documents. 5.2.6.1 R Markdown Markdown files usually begin with a header that defines metadata for the document, like the title, the author, etc. The header is between triple dashes (---) and is written in YAML format (YAML stands for YAML Ain’t Markup Language). We’d like to include the Old Faithful example in an R Markdown document. So we create a file called index.Rmd. Look for the file inside the rmd-shiny folder of the Faithful repository. In the YAML header we need to specify an output format that produces HTML, e.g. html_document, and the runtime to be set to shiny: --- title: &quot;Old Faithful&quot; output: html_document runtime: shiny --- The first code chunk would contain the data set definition and a knitr option to set the echo to false (do not print the code) so we don’t have to set it for every chunk: ```{r include=FALSE} knitr::opts_chunk$set(echo = FALSE) x &lt;- faithful$waiting ``` Next comes a code chunk with the slider widget: ```{r} sliderInput( inputId = &quot;n&quot;, label = &quot;Number of bins:&quot;, min = 1, max = 50, value = 25, ticks = TRUE ) ``` Finally, we render the plot output: ```{r} renderPlot( alt = &quot;Histogram of waiting times&quot;, { hist( x, breaks = seq(min(x), max(x), length.out = input$n + 1), freq = TRUE, col = &quot;#BB74DB&quot;, border = &quot;white&quot;, main = &quot;Histogram of waiting times&quot;, xlab = &quot;Waiting time to next eruption [mins]&quot;, ylab = &quot;Frequency&quot; ) box() } ) ``` The output format can be any format that creates an HTML file. So for example, you can use ioslides_presentation to create a slideshow with Shiny widgets and interactivity. But because Shiny is involved, you need a server to run the document. To render and run the document and the app inside it you can use rmarkdown::run(\"index.Rmd\") from inside the rmd-shiny folder. As a result, the rmarkdown package will extract the code chunks to create a server definition and uses the index.html output file to stich in the reactive elements. When you start the document, you will notice that it always renders the document at startup. Not only that, but it also requires a full document render for each end user’s browser session when deployed. This startup time for the users can be reduced if we render the HTML only once. Running expensive data import and manipulation tasks only once would also greatly help the startup times. The runtime for this is called shinyrmd (or its alias, shiny_prerendered): --- title: &quot;Old Faithful&quot; output: flexdashboard::flex_dashboard runtime: shinyrmd --- We’ll use the flexdashboard (Aden-Buie et al. 2023) package to give the document more of a dashboard look and feel. This R Markdown version of the Faithful app is inside the rmd-prerendered folder of the repository. The execution of pre-rendered Shiny documents is divided into two execution contexts, the rendering of the user interface and data, and the serving of the document to the users. To indicate the rendering context, you can use context=\"render\" chunk option, but this can be omitted because this is the default context for all R code chunks. The \"render\" is analogous of the ui.R file. For the first chunk, we define context=\"setup\" to mark code that is shared between the UI and the server. This is analogous to the global.R file. ```{r context=&quot;setup&quot;,include=FALSE} knitr::opts_chunk$set(echo = FALSE) x &lt;- faithful$waiting ``` We put the slider widget in the sidebar using the \"render\" context: Column {.sidebar} ------------------------------------------------------------- ```{r context=&quot;render&quot;} sliderInput( inputId = &quot;n&quot;, label = &quot;Number of bins:&quot;, min = 1, max = 50, value = 25, ticks = TRUE ) ``` The plot output element goes into the main panel, still as part of the \"render\" context: Column ------------------------------------------------------------- ```{r context=&quot;render&quot;} plotOutput(&quot;histogram&quot;) ``` Finally, we define the \"server\" context for the reactive output. This code is run when the interactive document is served and this is the same code that we would put into the server.R file: ```{r context=&quot;server&quot;} output$histogram &lt;- renderPlot( alt = &quot;Histogram of waiting times&quot;, { hist( x, breaks = seq(min(x), max(x), length.out = input$n + 1), freq = TRUE, col = &quot;#BB74DB&quot;, border = &quot;white&quot;, main = &quot;Histogram of waiting times&quot;, xlab = &quot;Waiting time to next eruption [mins]&quot;, ylab = &quot;Frequency&quot; ) box() } ) ``` The \"render\" and \"server\" contexts are run in separate R sessions. The first one is run when rendering happens, the second one is run many times, once for each user. A consequence of this context separation is that you cannot access variables created in “render” chunks within “server” chunks, and the other way around. To render the document, we use rmarkdown::render(\"index.Rmd\"), then use rmarkdown::run(\"index.Rmd\") to run the dashboard from inside the rmd-prerendered folder. Set the RMARKDOWN_RUN_PRERENDER environment variable to 0 to prevent any pre-rendering from happening, e.g. with Sys.setenv(RMARKDOWN_RUN_PRERENDER=0). You can include Python code chunks in your R Markdown documents. Python code is evaluated using the reticulate package (Ushey, Allaire, and Tang 2024). But you cannot include Shiny for Python in R Markdown. For that, you have Quarto. 5.2.6.2 Quarto with R Quarto is very similar to R Markdown in many respects. You can think of it as a generalized version of R Markdown that natively supports different programming languages to run code chunks. You will find the YAML header familiar. To use the Shiny runtime, we define server: shiny. The format: html means to produce HTML output. The execute part refers to global options, echo: false means that we don’t want to code to be echoed into the document. The basic Quarto example for the Faithful example in R is inside the quarto-r-shiny folder. Let’s start with the following header information in a file called index.qmd: --- title: &quot;Old Faithful&quot; execute: echo: false format: html server: shiny --- The language is specified after the triple backticks, here {r} means R. What is different from R Markdown is that chunk options are defined as special comments prefaced with #| at the top of the code block instead of following the language declaration inside the curly brackets. UI elements belong to the render context, which is something we do not have to specify: ```{r} plotOutput(&quot;histogram&quot;) sliderInput( inputId = &quot;n&quot;, label = &quot;Number of bins:&quot;, min = 1, max = 50, value = 25, ticks = TRUE ) ``` The server-start context will share code and data across multiple user sessions. It will execute when the document is first run and will not re-execute for every new user. This is like our global.R file. ```{r} #| context: server-start x &lt;- faithful$waiting ``` We can set the context to server for the next chunk: ```{r} #| context: server output$histogram &lt;- renderPlot( alt = &quot;Histogram of waiting times&quot;, { hist( x, breaks = seq(min(x), max(x), length.out = input$n + 1), freq = TRUE, col = &quot;#BB74DB&quot;, border = &quot;white&quot;, main = &quot;Histogram of waiting times&quot;, xlab = &quot;Waiting time to next eruption [mins]&quot;, ylab = &quot;Frequency&quot; ) box() } ) ``` To render and serve this document we have to use the command quarto serve index.qmd from inside the folder. Pre-rendering our document to speed up startup times for the users is really straightforward. We do not have to change the server type. We only have to first render the document with quarto render index.qmd. Then we can serve it with a flag that will tell Quarto not to render it again: quarto serve index.qmd --no-render. If you wanted to split the .qmd file into multiple files corresponding to the evaluation contexts, you can have the header and the UI definition in the index.qmd file. Put the code for the server-start context into the global.R file. The server.R file should return the server function: # server.R function(input, output, session) { output$histogram &lt;- renderPlot( alt = &quot;Histogram of waiting times&quot;, { hist( x, breaks = seq(min(x), max(x), length.out = input$n + 1), freq = TRUE, col = &quot;#BB74DB&quot;, border = &quot;white&quot;, main = &quot;Histogram of waiting times&quot;, xlab = &quot;Waiting time to next eruption [mins]&quot;, ylab = &quot;Frequency&quot; ) box() } ) } Find this Quarto example inside the quarto-r-shiny-multifile folder. 5.2.6.3 Quarto with Python The Python version or our Quarto-based Shiny app is very similar to the R version. No change in the header. The code chunks will have {python} defined instead of {r}, and of course you have to copy the Python code inside the chunks. See the quarto-py-shiny folder of the Faithful repository. --- title: &quot;Old Faithful&quot; execute: echo: false format: html server: shiny --- The next chunk contains the setup and loads libraries: ```{python} import seaborn as sns import matplotlib.pyplot as plt from shiny import App, render, ui faithful = sns.load_dataset(&quot;geyser&quot;) x = faithful.waiting ``` The UI elements, like the slider input control, come next: ```{python} ui.input_slider( id=&quot;n&quot;, label=&quot;Number of bins:&quot;, min=1, max=50, value=25, ticks=True) ``` Finally, the rendered plot: ```{python} @render.plot(alt=&quot;Histogram of waiting times&quot;) def histogram(): plt.hist( x, bins = input.n(), density=False, color=&quot;#BB74DB&quot;, edgecolor=&quot;white&quot;) plt.title(&quot;Histogram of waiting times&quot;) plt.xlabel(&quot;Waiting time to next eruption [mins]&quot;) plt.ylabel(&quot;Frequency&quot;) ``` Rendering and serving the Python document is the same that you used for the R version: quarto serve index.qmd will do both, so for a pre rendered version, use quarto render index.qmd first and the serve with the --no-render flag. 5.2.7 Shinylive Using Shinylive, you can run Shiny applications entirely in a web browser, i.e. on the client side, without the need for a separate server running in R or Python. This is achieved by R or Python running in the browser. The Python implementation of Shinylive uses WebAssembly (Wasm) and Pyodide. Wasm is a binary format for compiled programs that can run in a web browser, whereas Pyodide is a port of Python and many Python packages compiled to Wasm. The Shinylive version of the Old Faithful example can be viewed at https://h10y.github.io/faithful/. 5.2.7.1 Python Shinylive We will create Python Shinylive version of the Faithful app following the posit-dev/py-shinylive GitHub repository. First, export the app inside the py-shiny folder with the single-file app and put the Shinylive version in the py-shinylive folder: shinylive export py-shiny py-shinylive The Shiny live version will consist of static files, which means that we can copy these files to any static hosting site (like Netlify or GitHub Pages), and a browser will be able to display the contents irrespective of the underlying operating system, and without the need to have a Python available. Can we view the output locally? If you double click on the index.html sitting in the py-shinylive folder, you will most likely get an error in the browser. To read the error you have to find the developer tools (often opened by pressing F12 on your keyboard) and check the error messages. You will see something like this: Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at file:///[...]/shinylive/shinylive.js. (Reason: CORS request not http). For security reasons, browsers restrict cross-origin HTTP requests initiated from scripts. But what is this cross-origin resource sharing (CORS)? If you load the HTML file (like our index.html) from a source, the default CORS behavior will expect any other files, like images, or JavaScript/CSS scripts to have the same origin. By origin we mean the domain and subdomain. Now the problem here is that you are viewing a local version of the files, which will set the protocol part of the URIs to be file:// instead of http:// as indicated in the message. Although the folder following the file protocol is the same, but this is considered as having opaque origins by most browsers and therefore will be disallowed. If you want to view the files locally, you do it through a local server. That way all files are served from the same scheme and domain (localhost) and have the same origin. Start the server as: python3 -m http.server --directory py-shinylive Now visit http://localhost:8000/ in your browser, and the CORS error should be gone (port 8000 is the default port for http.server). 5.2.7.2 R Shinylive Shinylive for R uses similar technology built on WebAssembly using the WebR R package. Here is how to create an R Shinylive version of the Faithful app following the posit-dev/r-shinylive GitHub repository using the interactive R console: shinylive::export(&quot;r-shiny&quot;, &quot;r-shinylive&quot;) httpuv::runStaticServer(&quot;r-shinylive&quot;) The first line compiles the Shiny app that is inside the r-shiny folder into the Shinylive version in the r-shinylive folder. The second command will start the server and open the page in the browser. You will see a random port used on localhost, e.g. http://127.0.0.1:7446. 5.2.7.3 Shinylive in Quarto We can include Shinylive (R or Python) apps in Quarto. For that, we need to install the Shinylive quarto extension inside the Quarto project folder with quarto add quarto-ext/shinylive. The header of the Quarto document should list the shinylive extension under filters: --- title: &quot;Old Faithful&quot; format: html filters: - shinylive --- You can include the Shiny application code into a code chunk marked with {shinylive-r} or {shinylive-python}. Use the comment standalone: true which tells Quarto that the block contains a complete Shiny app, and not only parts of it, as we saw before. For R, we include the full single-file app. The viewerHeight: 500 comment is needed to have enough space for the UI: ```{shinylive-r} #| standalone: true #| viewerHeight: 500 [...] ``` The Python version uses {shinylive-python} and the Python version of the single-file Old Faithful Shiny app: ```{shinylive-python} #| standalone: true #| viewerHeight: 500 [...] ``` To view the Quarto document in your browser, use quarto preview index.qmd. This will work with the R and the Python versions alike. The Quarto Shinylive examples can be found inside the quarto-r-shinylive and quarto-py-shinylive folders of the Faithful repository. 5.3 Running Shiny Apps Locally When you are developing your app locally, you likely want to run the app and check the look and see the changes that you’ve made. In the previous chapter we already used some of the commands needed to run the code. Let’s review the different ways of running Shiny for R and Python apps locally. Running your app locally is necessary for testing. Of course testing goes way beyond just opening up the app in the browser. We will not cover best practices for testing your app. If you are interested, you can read about R package based development in Fay et al. (2021) or check out the documentation for the shinytest2 R package (Schloerke 2024). For testing related the Python version, see the Tapyr project that uses pytest and playwright for validation and testing. 5.3.1 R When the app is in a single R file, you should name it app.R just like we did previously for the faithful example. If you have multiple files, make sure that you have the server.R and ui.R files in the same directory. If you are using other frameworks, an app.R file usually serves as an entrypoint that your IDE will recognize. This way, you can run it easily inside of the RStudio IDE (Fig. 5.2) or VS Code|index{VS COde} with the Shiny extension (Fig. 5.3) by pushing the “▷ Run App” button. Clicking on button would run the app in either a simple browser window tab inside your IDE, or in a separate browser window, depending on your settings. Besides the app showing up in the browser, you can also see some messages appearing in your R console. If you inspect the console output, you should see something like this: Running Shiny app ----------------- shiny::runApp(&quot;r-shiny/app.R&quot;, port = 52938) Loading required package: shiny shiny devmode - Turning on shiny autoreload. To disable, call `options(shiny.autoreload = FALSE)` This message is displayed once every 8 hours. Listening on http://127.0.0.1:52938 What does this mean? Pushing the Run App button led to running the runApp() command. This started a web server on localhost (127.0.0.1) listening on port 52938 (your port number might be different). If you visit the http://127.0.0.1:52938 address in your browser, you would see the Shiny app with the slider and the histogram. Stop the app by closing the app window in RStudio or using CTRL+C. Running the app this way will allow you to keep the server running while making changes to the app source code. Your changes will trigger a reload so you can immediately see the results. You can disable this behavior by turning off the auto-reload option with options(shiny.autoreload = FALSE). Figure 5.2: Running an R Shiny app in the RStudio IDE. Figure 5.3: Running an R Shiny app in the VS Code IDE with the Shiny extension. The runApp() function can take different types of arguments to run the same app. What you saw above was serving the app from the single file. If you name the single file something else, e.g. my-app.R, you can provide the path to a single file as runApp(\"&lt;app-directory&gt;/my-app.R\"). You can start the Shiny app from the terminal using the command R -q -e 'runApp(\"&lt;app-directory&gt;/my-app.R\")' where the -q flag means to suppress the usual opening message, and -e instructs R to execute the expression following it. You can also specify the port number as an argument, e.g. R -q -e \"runApp(..., path = 8080)\" will start the web server on port 8080. Running these lines will start the Shiny server locally that you can visit in the browser. To be precise, the shinyApp() R function returns the app object which is run either by implicitly calling the print() method on it when running in the R console. You can also pass the app object to the runApp() function. Stop the server by CTRL+C. R -q -e &quot;shiny::runApp(&quot;r&quot;, port = 8080)&quot; &gt; shiny::runApp(&quot;r&quot;, port = 8080) Loading required package: shiny Listening on http://127.0.0.1:8080 This pattern might be unusual for you if you are using R mostly in interactive mode through an IDE. You will see this pattern in the next chapters when we call R from the terminal shell. This is how we can start the web server process in non-interactive mode. 5.3.2 Python You can run the Python app from the RStudio IDE (Fig. 5.4) or VS Code (Fig. 5.5) by pushing the same “▷ Run App” button. You’ll see something like this in your console with localhost and a randomly picked and available port number (52938). python -m shiny run --port 52938 --reload [...] py-shiny/app.py INFO: Will watch for changes in these directories: [&#39;py-shiny&#39;] INFO: Uvicorn running on http://127.0.0.1:52938 (Press CTRL+C to quit) INFO: Started reloader process [85924] using WatchFiles INFO: Started server process [85926] INFO: Waiting for application startup. INFO: Application startup complete. INFO: 127.0.0.1:56050 - &quot;GET [...] HTTP/1.1&quot; 200 OK INFO: (&#39;127.0.0.1&#39;, 56053) - &quot;WebSocket /websocket/&quot; [accepted] INFO: connection open Running the Shiny app in Python relies on the Uvicorn web server library that can handle websocket connections. Figure 5.4: Running a Python Shiny app in the RSudio IDE. Figure 5.5: Running a Python Shiny app in the VS Code IDE with the Shiny extension. The other port number (56053) is for the websocket connection created for the session. If you open another browser window pointing to http://127.0.0.1:52938, you’ll see another websocket connection opening for the new session: # Opening another browser tab INFO: 127.0.0.1:56194 - &quot;GET / HTTP/1.1&quot; 200 OK INFO: (&#39;127.0.0.1&#39;, 56196) - &quot;WebSocket /websocket/&quot; [accepted] Use App(app_ui, server, debug=False) to suppress the messages. From the terminal, you can run the single app file from the terminal with shiny run --port 8080 &lt;app-directory&gt;/app.py on port 8080. If you change the output of the App() statement from the default app = App(...) to faithful_app = App(...), you have to define the app as well not just the file: shiny run &lt;app-directory&gt;/app.py:faithful_app. If the file is called app.py and the app object is called app, you can omit the file name and use shiny run, in this case app.py:app is assumed in the current working directory. Trying to run both the R and Python versions on the same port at the same time will not be possible. If you want to run both, use different port numbers, e.g.  8080 and 8081. The shiny run command starts the app. Use the --launch-browser flag to automatically launch the app in a web browser. The --reload flag means that the Python process restarts and the browser reloads when you make and save changes to the python/app.py file. Use CTRL+C to stop the process. 5.3.3 The Shiny App Lifecycle The traditional way of serving Shiny apps involves a server that runs an R or Python process, and each client connects to this server and keeps an open websocket connection as long as they are using the application. Let’s take a closer look at this to better understand what is happening under the hood. Shiny for R relies on the httpuv (Cheng et al. 2024) package to handle connections. Whenever a new user connects to the Shiny app a new session is started and communication between the client and the user session will be happening through the websocket connection. The websocket allows two-way communication which is the basis of Shiny’s reactivity. The JavaScript code on the client side can communicate with the the R process via this connection. In Python, the connections are handled by Uvicorn, and the messages – as we saw before – reveal the port numbers used for the different user sessions. Why is this important? Because user sessions having their own ports is the basis for isolating these sessions from one another. Users will not be able so access data from another session, unless data is leaked through the global environment (which should be avoided). The Shiny app life cycle can be described as follows (Fig. 5.6): Server start: after calling runApp() in R or shiny run for Python, the httpuv or Uvicorn server is started and is now listening on a random or a pre-defined port (e.g. 8080). Server ready: the application code is sourced including loading the required libraries, data sets, everything from the global scope; if users try to connect to the app before it is ready they will see an error message. Client connects to the app via the port over HTTP protocol. New session created: the backend server (httpuv or Uvicorn) starts a user session and runs the server function inside that session; a websocket connection is created for two-way communication. Client-server communication happening while the user is using the app: the server sends the rendered HTML content to the client, including the JavaScript code that will communicate with the server to send and receive data through the websocket connection. When the client detects that the websocket connection is lost, it will try to reconnect to the server. After a certain amount if inactivity, or in the case of disconnected client, the websocket connection and the user session will get terminated and the client browser will “gray out”. You can find more information about the Shiny app life cycle in Granjon (2022) and J. Coene (2021). Figure 5.6: The Shiny app life cycle with websocket connection. For Shinylive applications, the lifecycle does not include a websocket connection, and relies purely on HTTP(S) between the client and the server. The server will only send the requested resources to the client, and it will not do any other work. It will just “serve” these static files. The client browser will do the heavy lifting by rendering the HTML and running the Web Assembly binary that will take care of the reactivity. Such an application will not time out until the browser tab is closed. 5.4 Sharing the Shiny App Code The source code for the app can be shared with collaborators, clients, and users. They can run the app themselves if they are savvy enough. When the audiences of a Shiny app are R or Python users, it makes sense to share the app as a Gist, a GitHub repository, or a zip file. However, sharing Shiny apps this ways leaves installing dependencies up to the user. You can email the files or the link to your users, you can even send a USB drive or a CD ROM in the mail. The shiny R package comes with a few useful functions that makes sharing your apps with technical users a bit easier. You can use the runUrl(), runGitHub(), and runGist() functions to run app from specific URLs, GitHub repositories, and Gists, respectively. runUrl() can be pointed at R files or compressed files, such as .zip. Another option is distributing your R Shiny app as an R package. This option takes care of dependency management. You can install the R package from different sources, like GitHub, and you can also have it hosted on CRAN. However, the recipients will have to be able to install the package from source, which implies familiarity with R and package management. Sharing the app source code is a low effort option on your part, but might be a high effort option for the ones you are sharing your app with. 5.5 Summary We reviewed all the different ways of how Shiny apps can be organized during development, from standalone R and Python applications to being part of dynamic documents. We also reviewed options for sharing your app’s code with others. Sharing the app source code has several issues when you are sharing it with non-technical users. First off, they will have to have an R or Python runtime environment. Second, they will have to have all the right dependencies installed, sometimes with specific versions of the libraries. As you will see in the next chapter, sharing the app as a Docker image is also an option. This might help with having a runtime environment and managing dependencies, but again, your users will need to understand and use Docker and that can be often too much to ask for. So the real reason we are talking about Docker is that it can help you host the app. References Aden-Buie, Garrick, Carson Sievert, Richard Iannone, JJ Allaire, and Barbara Borges. 2023. Flexdashboard: R Markdown Format for Flexible Dashboards. https://CRAN.R-project.org/package=flexdashboard. Allaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2024. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown. Cheng, Joe, Winston Chang, Steve Reid, James Brown, Bob Trower, and Alexander Peslyak. 2024. Httpuv: HTTP and WebSocket Server Library. https://CRAN.R-project.org/package=httpuv. Coene, J. 2021. Javascript for r. Chapman &amp; Hall/CRC the r Series. CRC Press. https://books.google.ch/books?id=ntUxEAAAQBAJ. Coene, John. 2022. Leprechaun: Create Simple ’Shiny’ Applications as Packages. https://CRAN.R-project.org/package=leprechaun. Fay, Colin, Vincent Guyader, Sébastien Rochette, and Cervan Girard. 2023. Golem: A Framework for Robust Shiny Applications. https://CRAN.R-project.org/package=golem. Fay, Colin, Sébastien Rochette, Vincent Guyader, and Cervan Girard. 2021. Engineering Production-Grade Shiny Apps. Chapman &amp; Hall. https://engineering-shiny.org/. Granjon, David. 2022. Outstanding User Interfaces with Shiny. Chapman &amp; Hall. https://unleash-shiny.rinterface.com/. Hadley, Wickham, and Jennifer Bryan. 2023. R Packages. 2nd ed. O’Reilly Media, Inc. https://r-pkgs.org/. Knuth, Donald. 1992. Literate Programming. Center for the Study of Language; Information—CSLI. Leisch, Friedrich. 2002. “Sweave, Part i: Mixing r and LaTeX.” R News 2: 28–31. Rudolph, Konrad. 2024. Box: Write Reusable, Composable and Modular r Code. https://CRAN.R-project.org/package=box. Schloerke, Barret. 2024. Shinytest2: Testing for Shiny Applications. https://CRAN.R-project.org/package=shinytest2. Ushey, Kevin, JJ Allaire, and Yuan Tang. 2024. Reticulate: Interface to ’Python’. https://CRAN.R-project.org/package=reticulate. Wickham, Hadley, Peter Danenberg, Gábor Csárdi, and Manuel Eugster. 2024. Roxygen2: In-Line Documentation for r. https://CRAN.R-project.org/package=roxygen2. Xie, Yihui. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/. Żyła, Kamil, Jakub Nowicki, Leszek Siemiński, Marek Rogala, Recle Vibal, Tymoteusz Makowski, and Rodrigo Basa. 2024. Rhino: A Framework for Enterprise Shiny Applications. https://CRAN.R-project.org/package=rhino. "],["6-part2-containerizing-shiny-apps.html", "Chapter 6 Containerizing Shiny Apps 6.1 Docker Concepts 6.2 Working with Existing Images 6.3 Building a New Image 6.4 Managing Images 6.5 Sharing Images 6.6 The Dockerfile 6.7 Parent Images 6.8 Installing System Libraries 6.9 Installing R Packages 6.10 Python Requirements 6.11 Dynamic Shiny Apps 6.12 Static Shiny Apps 6.13 Image Analysis 6.14 Containers 6.15 Best Practices 6.16 Summary", " Chapter 6 Containerizing Shiny Apps Containerization is a topic that is of increasing interest to data scientists and is a key feature for being able to cover the R and Python aspects of Shiny hosting in parallel. Once the container image is built, deployment and hosting become independent of the language that the app was written in. Tooling around this task has made huge advances over the past two years, and thanks to this, the topic is now accessible to a wider audience. Containerization enables you to separate your applications from your infrastructure so you can deliver software faster. Containerization help makes an application platform independent by creating virtual environments in a container. To run a specific application, you will need to develop an image for that platform which is then published on a container registry. The developed image can be pulled from the registry to run a virtualized container environment for the application. Docker is one of the most popular tools for creating and managing containers for Shiny apps. This chapter will outline the needed concepts for containerizing your shiny application. Learning Docker seems daunting at first, but it is an incredibly powerful piece of technology once you get the hang of it. It is also the building block of the modern web. Docker is not the only tooling for containerizing applications. Docker’s licensing model has recently changed and can require a paid license for commercial use. Therefore, there are alternatives of the Docker Engine such as using Podman. However, Podman is much more technical to use than Docker. All the general advantages of containerized applications apply to Shiny apps. Docker provides isolation to applications. Images are immutable: once build they cannot be changed, and if the app is working, it will work the same in the future. Another important consideration is scaling. Shiny apps are single-threaded, but running multiple instances of the same image can serve many users at the same time. Let’s dive into the details of how to achieve these. 6.1 Docker Concepts Containers provide portability, consistency, and are used for packaging, deploying, and running cloud-native data science applications. Docker is the most popular virtualization environment to deliver software in containers. Docker is also well supported for Python and R. Among the many use cases, Docker is most commonly used to deploy reproducible workflows and to provide isolation for Shiny apps. Containers bundle their own software, libraries and configuration files and are isolated from one another. Containers are the run-time environments or instances defined by container images. Let’s review the most important concepts. Figure 6.1 illustrates how all the Docker-related concepts all fit together. Figure 6.1: Docker architecture. Follow the solid, scattered and dotted lines to see how the Docker command line interface (CLI) interacts with the the Docker daemon and the container registry for various commands. You as the user, will use the command line as the client to the Docker Engine which exists on a host machine. The Docker Engine interfaces with the container registry to pull the necessary images for building a local copy of an image on the host for running an instance of a container. 6.1.1 Docker Engine The Docker Engine is a client-server application that includes a server (a long-running daemon process called dockerd that listens to API requests), an application programming interface (REST API) that specifies the interface that programs can use to talk to the daemon process, and a command-line interface (CLI) that is the client-side of Docker. The CLI uses the REST API to control or interact with the Docker daemon. The daemon creates and manages Docker objects, such as images, containers, volumes, networks, etc. 6.1.2 Container Registries A Docker registry stores Docker images. Docker Hub is a public registry and Docker is configured to look for images on Docker Hub by default. There are many other registries, or users can have their own private registries. You will see some examples later. Strictly speaking, container registries are for images and not containers. 6.1.3 Images An image is a read-only template with instructions for creating a Docker container. You can view an image as a set of compressed files and metadata describing how these files – also called image layers – fit together. An image can be based on another image with additional customization on top of this so-called base* or a parent** image. A base images is an image created from “scratch”, whereas a parent image is just another image that serves as the foundation for a new image. You might see the term base image used for both situations when reading tutorials. Don’t get confused, the Docker lingo has a few inconsistencies that we just have to accept and move on. 6.1.4 The Dockerfile Docker builds images by reading the instructions from a file called Dockerfile. A Dockerfile is a text document that contains all the commands to assemble an image using the docker build CLI command. You will learn more about the Dockerfile as part of the worked Shiny examples later. 6.1.5 Containers A container is a runnable instance of an image. Users can create, start, stop a container using the Docker API or CLI. It is also possible to connect a container to networks or attach storage to it. By default, a container is isolated from other containers and the host machine. The degree of isolation can be controlled by the user and depends on whether it is connected to networks, storage, other containers, or the host machine. 6.1.6 The Docker Command Line The most common Docker CLI commands are: docker login: log into a Docker registry, docker pull: pull an image from a registry, docker build: build a Docker image based on a Dockerfile, docker push: push a locally built image to a Docker registry, docker run: run a command in a new container based on an image. You will learn more about these commands in the subsequent sections. 6.2 Working with Existing Images Let’s learn how to work with an existing image. Such an image is stored in a container registry where we can pull it from if we know its name. 6.2.1 Image Names and Tags Image names follow the pattern &lt;host&gt;/&lt;path&gt;:&lt;tag&gt;. The optional &lt;host&gt; name specifies where the image is located. If you don’t specify a host name, the commands will use Docker’s public registry docker.io, aka the Docker Hub. The &lt;path&gt; can be an “official” image, like ubuntu. The &lt;tag&gt; is a human-readable identifier that is often a specific version of an image. If not specified, the latest tag will be used. So ubuntu as an image name will be identical to docker.io/ubuntu:latest. It is important to note that the latest tag only means “latest” in the sense of the last image that was tagged as latest or was untagged. If you use a different tag, like v1, the image with the latest tag will not get updated as well. So don’t let this tag fool you. It is strongly recommended in production to always explicitly use a tag that is not latest but a specific version. The path is usually more structured and consists of slash-separated components. It often looks like &lt;namespace&gt;/&lt;repository&gt; where the namespace specifies the user account or organization to which the image belongs to. The base R image maintained by the Rocker project is named as rocker/r-base where rocker is the organization namespace, r-base is the repository. Another example is the R version of the Old Faithful example which has the image name ghcr.io/h10y/faithful/r-shiny:latest. This means: ghcr.io is the GitHub Container Registry host name, h10y is the GitHub organization, faithful is the GitHub repository, r-shiny is the Shiny app build, latest is the version tag. 6.2.2 Pulling an Image You can use the docker pull &lt;image-name&gt; command to pull an image from a public registry. For example docker pull ubuntu:24.04 will pull the 24.04 version of the “official” Ubuntu image from the Docker Hub. docker pull rocker/r-base:4.4.1 will pull the image with R version 4.1.1. Pull the R Shiny version of the Old Faithful as: docker pull ghcr.io/h10y/faithful/r-shiny # Using default tag: latest # latest: Pulling from h10y/faithful/r-shiny # Digest: sha256:12e[...]4ea You can see from the messages that the latest tag was applied because we did not specify the tag. We can also see the SHA256 digest, that is a unique and immutable identifier. The name can change, or multiple names can refer to the same image (i.e. a set of layers and their manifest). But the image digest will be the same. To “pin” the exact version, you can use the &lt;image-name&gt;@sha256:12e[...]4ea pattern (use the actual digest copied from your screen without the [...]): docker pull ghcr.io/h10y/faithful/r-shiny@sha256:12e[...]4ea To pull all images from a repository, you can use the --all-tags flag: docker image pull --all-tags ghcr.io/h10y/faithful/r-shiny This will pull not only the latest, but also the image tagged as main named after the Git branch. Use the docker images command to list the images. 6.2.3 Docker Login You don’t need to authenticate for public images, but in case you are trying to pull a private image from a private repository, you need to log into the container registry. Such private repositories are common and are available on Docker Hub, the GitHub or GitLab container registries. More on the different container registries later. To log in to the GitHub container registry, use: docker login ghcr.io This command will ask for our credentials interactively. If you want, you can provide your username and password. But it is usually recommended to use an access token instead of your password because the token can have more restricted scopes, i.e. only used to (read) access the container registry which is a lot more secure. You can also set expiry dates and can revoke these tokens any time without having to change login passwords elsewhere. Let’s say that you saved your GitHub token value in a file ~/my_token.txt in the root of your home folder (~). You can pass the PAT value to the docker login command via the standard input as: cat ~/my_token.txt | docker login \\ --username &lt;username&gt; \\ --password-stdin where &lt;username&gt; is your GitHub username. 6.2.4 Running a Container The next command is docker run which runs a command in a new container. It pulls the image if needed before starting the container. Try the following command. It will pull the latest image for the Python build of the Old Faithful example app, then it will start a new container: docker run -p 8080:3838 ghcr.io/h10y/faithful/py-shiny The -p is a shorthand for --publish that instructs Docker to publish a container’s port to the host port. In our example, 3838 is the container’s port which is mapped to port 8080 of the host machine. As a result, you can visit http://127.0.0.1:8080 in your browser to see the Python Shiny app. Hit CTRL+C in the terminal to stop the container. We will learn about container ports in a bit, but in essence it is just a channel that is used to send information back and forth. 6.3 Building a New Image So far you saw how to use the basic Docker commands to pull and run images. Now you’ll build a Docker image by recreating the Old Faithful Shiny app that we worked with before. In our examples, we will use the following setup: a file named Dockerfile sits next to a folder named app, and the Shiny app files like app.R or app.py are in this folder. This setup is convenient because we can copy all the files from the app folder without having to worry about copying files that should not be there. ├── Dockerfile └── app └── ... You can follow along the examples by downloading or cloning the GitHub repository with git clone https://github.com/h10y/faithful.git. All the different builds of the Old Faithful app from Chapter 5 will have a Dockerfile and instructions in the README.md files within each folder. 6.3.1 R for Shiny For our R Shiny example within the r-shiny folder, this is what is inside the Dockerfile: FROM rocker/r2u:24.04 RUN R -q -e &quot;install.packages(&#39;shiny&#39;)&quot; RUN groupadd app &amp;&amp; useradd -g app app WORKDIR /home/app COPY app . RUN chown app:app -R /home/app USER app EXPOSE 3838 CMD [&quot;R&quot;, &quot;-e&quot;, &quot;shiny::runApp(host=&#39;0.0.0.0&#39;, port=3838)&quot;] We will explain the Dockerfile instructions in the next section. For now, you can use the docker build command to build the image from the Dockerfile. You will have to be in the same directory as the Dockerfile, this place is what we’ll call as the build context. This is what the . at the end of the command stands for: docker build -t r-shiny:v1 . The context here specifies the current directory (.), but it can be any relative or absolute filepath. Files and directories inside the context directory are available to the builder, so it can load them when needed. You can use a .dockerignore file to list files and directories that should be ignored within the build context. It is similar to the .gitignore file. The instructions are taken from the Dockerfile at the root of the build context. If you want to specify a different file, do so by providing the path to the file using the -f (or --file) option as docker build -f Dockerfile2 .. The -t argument (same as --tag) is followed by the image name (r-shiny-test) and the tag (v1). If you do not specify the image name/tag at image build (i.e. docker build .), Docker will not tag the image but it will have an image ID that you can use later to tag the image with docker tag &lt;image-id&gt; r-shiny-test:v1. You can apply multiple tags as: docker build -t r-shiny:v1 -t r-shiny:latest . 6.3.2 Buildx and BuildKit While the builder is running, you’ll see lots of messages printed as Docker goes through the instructions from the Dockerfile. As of Docker Engine 23.0 and Docker Desktop 4.19, Buildx is the default build client and user interface. Buildx brings extended build capabilities with BuildKit. BuildKit is the server that handles the build execution, e.g. it communicates with registries, instructs the Docker Engine and accesses the local file system. You can enable the use of BuildKit on older Docker systems by setting the environment variable DOCKER_BUILDKIT=1. The Buildx output is nicer and it provides you with timings for every step of your Dockerfile: [+] Building 32.4s (12/12) FINISHED =&gt; [internal] load build definition from Dockerfile 0.0s =&gt; =&gt; transferring dockerfile: 282B 0.0s =&gt; [internal] load metadata for docker.io/rocker/r2u:24.04 1.2s =&gt; [auth] rocker/r2u:pull token for registry-1.docker.io 0.0s =&gt; [internal] load .dockerignore 0.0s =&gt; =&gt; transferring context: 2B 0.0s =&gt; [1/6] FROM docker.io/rocker/r2u:24.04@sha256:f327[...]dd73 9.2s =&gt; =&gt; resolve docker.io/rocker/r2u:24.04@sha256:f327[...]dd73 0.0s [...] =&gt; [internal] load build context 0.0s =&gt; =&gt; transferring context: 845B 0.0s =&gt; [2/6] RUN groupadd app &amp;&amp; useradd -g app app 0.7s =&gt; [3/6] RUN R -q -e &quot;install.packages(&#39;shiny&#39;)&quot; 20.9s =&gt; [4/6] WORKDIR /home/app 0.0s =&gt; [5/6] COPY app . 0.0s =&gt; [6/6] RUN chown app:app -R /home/app 0.1s =&gt; exporting to image 0.3s =&gt; =&gt; exporting layers 0.3s =&gt; =&gt; writing image sha256:4d10[...]bab7 0.0s =&gt; =&gt; naming to docker.io/library/r-shiny:v1 0.0s Sometimes you want to inspect the output and do not only want the collapsed output. Add the --progress=plain to the build command to see all the output. This comes handy when troubleshooting the build. BuildKit also offers other nice features, for example setting the target platform(s) for the build via the --platform option. The default value is the platform of the BuildKit daemon where the build runs, i.e. your laptop or a server. This can be important for Mac OS X users on Apple Silicone (M1 and above), because the default ARM64 build will have poor performance or might fail on other platforms on AMD64 machines. Use the --platform=linux/arm64 to build the image for AMD64 architecture. You can also build for multiple architectures at once with docker build --platform linux/amd64,linux/arm64 .. See 3.1.5 for enabling virtualization on Mac OS X to enable builds for multiple platforms. 6.3.3 Inspecting the Image The output of the build is an image that has a SHA256 hash that can be used as a unique identifier. The image is made up of image layers. These layers are created by the instructions from the Dockerfile. If you run the build command again you will notice that instead of 32 seconds, it will take almost no time to build the image. This is because the layers are cached by default and Docker smartly evaluates which instructions and files have changed since the last build. Sometimes the cache gets tangled, or you just want to make sure that the error is not a caching issue. In this case use the --no-cache flag with docker build. You can use the docker history r-shiny:v1 command to see how the image was built and you can see the sizes for every layer. Intermediate layers have a size of 0B and these do not contribute to the overall image size. The layers created 2 hours ago are the layers we created, the layers created 2 weeks ago are the layers from the parent image rocker/r2u:24.04, whereas the layers created 2 months ago are the official ubuntu:24.04 image layers that form the parent image of the rocker/r2u:24.04 one: IMAGE CREATED CREATED BY SIZE 4d[...]52 2 hours ago CMD [&quot;R&quot; &quot;-e&quot; &quot;shiny::runApp(host=&#39;0.0.0.0&#39;, 0B &lt;missing&gt; 2 hours ago EXPOSE map[3838/tcp:{}] 0B &lt;missing&gt; 2 hours ago USER app 0B &lt;missing&gt; 2 hours ago RUN /bin/sh -c chown app:app -R /home/app # 780B &lt;missing&gt; 2 hours ago COPY app . # buildkit 780B &lt;missing&gt; 2 hours ago WORKDIR /home/app 0B &lt;missing&gt; 2 hours ago RUN /bin/sh -c R -q -e &quot;install.packages(&#39;sh 109MB &lt;missing&gt; 2 hours ago RUN /bin/sh -c groupadd app &amp;&amp; useradd -g ap 5.14kB &lt;missing&gt; 2 weeks ago RUN /bin/sh -c apt-get update &amp;&amp; apt 642MB &lt;missing&gt; 2 weeks ago ENV TZ=UTC 0B &lt;missing&gt; 2 weeks ago ENV DEBIAN_FRONTEND=noninteractive 0B &lt;missing&gt; 2 weeks ago ENV LANG=en_US.UTF-8 0B &lt;missing&gt; 2 weeks ago ENV LC_ALL=en_US.UTF-8 0B &lt;missing&gt; 2 weeks ago RUN /bin/sh -c useradd -s /bin/bash -m docke 81.6MB &lt;missing&gt; 2 weeks ago LABEL org.label-schema.license=GPL-2.0 org.l 0B &lt;missing&gt; 2 months ago /bin/sh -c #(nop) CMD [&quot;/bin/bash&quot;] 0B &lt;missing&gt; 2 months ago /bin/sh -c #(nop) ADD file:ac9d5a9d5b9b1217 76.2MB &lt;missing&gt; 2 months ago /bin/sh -c #(nop) LABEL org.opencontainers. 0B &lt;missing&gt; 2 months ago /bin/sh -c #(nop) LABEL org.opencontainers. 0B &lt;missing&gt; 2 months ago /bin/sh -c #(nop) ARG LAUNCHPAD_BUILD_ARCH 0B &lt;missing&gt; 2 months ago /bin/sh -c #(nop) ARG RELEASE 0B The docker inspect r-shiny:v1 returns a long JSON output that is the metadata of the image. It also has the SHA256 hash of the image. Here is the greatly simplified output: [ { &quot;Id&quot;: &quot;sha256:4d10[...]bab7&quot;, &quot;RepoTags&quot;: [&quot;r-shiny:v1&quot;], &quot;Created&quot;: &quot;2024-07-05T04:59:01.123398172Z&quot;, &quot;Config&quot;: { &quot;User&quot;: &quot;app&quot;, &quot;ExposedPorts&quot;: {&quot;3838/tcp&quot;: {}}, &quot;Cmd&quot;: [&quot;R&quot;,&quot;-e&quot;, &quot;shiny::runApp(host=&#39;0.0.0.0&#39;, port=3838)&quot;], &quot;Volumes&quot;: null, &quot;WorkingDir&quot;: &quot;/home/app&quot;, &quot;Entrypoint&quot;: null, }, &quot;Architecture&quot;: &quot;amd64&quot;, &quot;Os&quot;: &quot;linux&quot;, &quot;Size&quot;: 909132976, &quot;Metadata&quot;: { &quot;LastTagTime&quot;: &quot;2024-07-05T06:20:22.2764725Z&quot; } } ] Once the docker image is built, you can run the container to make sure the app is working as expected: docker run -p 8080:3838 r-shiny:v1 6.3.4 Python for Shiny You can find the Python for Shiny example in the py-shiny folder of the Old Faithful example repository. The Dockerfile for the Python version looks like this: FROM python:3.9 COPY app/requirements.txt . RUN pip install --no-cache-dir --upgrade -r requirements.txt RUN groupadd app &amp;&amp; useradd -g app app WORKDIR /home/app COPY app . RUN chown app:app -R /home/app USER app EXPOSE 3838 RUN mkdir .config ENV MPLCONFIGDIR=/home/app/.config ENV HOME=/home/app CMD [&quot;uvicorn&quot;, &quot;app:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;3838&quot;] We’ll explain each line shortly. To build and check the Docker image, use the following commands: export DOCKER_DEFAULT_PLATFORM=linux/amd64 docker build -t py-shiny:v1 . docker run -p 8080:3838 py-shiny:v1 The DOCKER_DEFAULT_PLATFORM environment variable is not strictly necessary, but it can save you some headaches on Mac OS X when the platform for the parent image is not matching the local ARM64 architecture of your Apple Silicone. 6.4 Managing Images There are a few commands that you need to know to manage your Docker images in the absence of the Docker Desktop graphical user interface. This will pay off later when you have no such luxuries on a server. To list the Docker images, use the docker images command. It will give you a quick summary of the images: REPOSITORY TAG IMAGE ID CREATED SIZE py-shiny v1 ed11a2980c07 5 seconds ago 1.24GB r-shiny v1 4d10f42d6a52 About an hour ago 909MB Size is the space taken up by the image and all its parent images. You can filter the output, for example docker images --filter=reference=\"py-*\" will give you images whose name starts with py-, whereas docker images --filter=reference=\"*:v1\" will list images that are tagged with v1. Use the docker rmi &lt;image-name-or-id&gt; to remove an image based on its name or the image ID. The docker system df command will give you a concise summary of disk usage by the Docker daemon including images, containers, and volumes: TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 2 1 1.457GB 1.355GB (98%) Containers 1 1 0B 0B Local Volumes 0 0 0B 0B Build Cache 117 0 3.005GB 3.005GB If you build images during development while keeping the image name and tag the same you will end up with “dangling” images that are untagged and are not used any longer. Dangling images can accumulate over time and can fill up the available space that Docker Desktop is allocating for images. Use docker system prune to clean up these dangling images. The command docker system prune --all will remove all unused images and containers. The Docker Desktop uses a finite amount of disk space that can fill up. Do the cleanup or go to the Docker Desktop settings and under Resources you should be able to change the virtual disk limit. You can check the RAM, CPU, and disk usage by looking at the bottom of the Docker Desktop window. 6.5 Sharing Images As we saw, Docker images are just compressed files linked by metadata. You should be able to copy these files and move them around. The docker save command lets you save an image to a compressed tar file: docker save -o r-shiny-v1.tar r-shiny:v1 Next, you take this tar file, copy it to another server and load it with: docker load --input r-shiny-v1.tar This restores both the image and the tags. Now imagine that you are managing more than two machines, or you want to share the Docker image with others so that they can use it or to serve as a parent image. The save/copy/load workflow becomes cumbersome quickly. In this case, using a registry might be a much better idea. There are many options to choose from, and you can even host your own registry. 6.5.1 Pushing Images Let’s tag the r-shiny image so that it has a host defined: docker tag r-shiny:v1 ghcr.io/h10y/faithful/r-shiny:latest Now we can push the locally built Docker image to a container registry: docker push ghcr.io/h10y/faithful/r-shiny:latest Note that this command will not work on your machine because you do not have write access to the ghcr.io/h10y/faithful repository. You need to create an image name that would let you push to your own personal Docker Hub account as an example. The image tag should start with the registry name unless you are pushing to Docker Hub. When the image tag is not specified, Docker will treat the new image as :latest automatically. 6.5.2 Docker Registries A Docker registry stores Docker images. This is where we push images to and pull images from. Docker Hub is a public registry and Docker is configured to look for images on Docker Hub by default. Docker Hub is a service provided by Docker for finding and sharing container images. The canonical host name for Docker Hub is docker.io. This is the default registry when you don’t specify a registry host as part of the image name. There are many other registries out there besides Docker Hub. Here is a non-exhaustive list of options. The GitHub Container Registry (GHCR) is available as part of GitHub Packages for free and paid plans, even for private repositories under the free plan. This registry requires no authentication for public images, otherwise you have to authenticate using your GitHub token. The visibility of the images inherits the repository visibility but can be changed by the owner. The host name for GHCR is ghcr.io. An alternative to GitHub is GitLab (host name registry.gitlab.com), that has provided registry support for its free (public and private) repositories long before GitHub. The registry is tightly integrated with GitLab’s CI/CD pipelines. This registry also needs login with a token for private images. Heroku is a platform provider and it also comes with a Docker registry (host name is registry.heroku.com) where the Docker-based deployments push the images to. Every major cloud provider offers a Docker container registry that is integrated with their other offerings. Latency should be minimal due to network proximity to the servers: Amazon Elastic Container Registry Azure Container Registry Google Container Registry DigitalOcean Container Registry Other common alternatives for container registries include the JFrog Container Registry, Harbor, and Scaleway. Although these services are called “container registry”, but strictly speaking they store container images. 6.5.3 Log In to a Registry When you work with private registries or private images, you need to log in with the docker login command. For Docker Hub, just type docker login. For all other registries, type in the registry URL as well, e.g. docker login ghcr.io. The Docker CLI then will prompt you for your username and password (or access token). You can log in programmatically by providing your username and the password through standard input from a file: cat ~/my_password.txt | docker login -u USER --password-stdin The my_password.txt in this example is is a simple text file with the token inside and it can be found in the root of your home folder (~). Change the file path and file name as needed. You can also use an environment variable to store your token value that you can pass to the login command as: export TOKEN=&lt;your-token-value&gt; echo $TOKEN | docker login ghcr.io -u USER --password-stdin Notice the white space before the export statement, use double spaces so that the command after the spaces will not be saved in your shell history. The history allows you to recall previous commands by pushing the up arrow key. The shell history is really just a text file, so copy pasting secrets into the terminal will leave a trace. Use this trick for sensitive information. With one of these approaches you can log into any public or private repository for which you have credentials. The credentials will be stored locally in $HOME/.docker/config.json on Linux and Mac or in %USERPROFILE%/.docker/config.json on Windows. After login, there is no need to re-authenticate until you log out with docker logout. It is always a good idea to use a token instead of your password. Tokens can have limited scope (i.e. only for pulling images), and can be revoked at any time without it impacting other areas of your life. Note that docker login requires users to use sudo or be the root user. 6.5.4 Local Registry You might not want the Docker images to leave your computer because you need an air gapped environment, or you are setting up a registry within your virtual private network (VPN). In these situations, you can host your own container registry. If you want a registry hosted on your machine, just pull the registry image. The next command will pull the registry image, and run the similarly named container in the background on port 5000: docker run -d \\ -p 5000:5000 \\ --restart=always \\ --name registry \\ registry:2 Giving a container a name makes it easier to remove the container later, this way you don’t have to find the container ID. The restart policy always restarts the container if it stops, but not when it is manually stopped. The -d flag will start the container in a background process, so you get back the shell prompt, and you will not see the container log messages. Tag an image with the host name of your local registry, localhost:5000, and push the image: docker tag r-shiny:v1 localhost:5000/r-shiny:v1 docker push localhost:5000/r-shiny:v1 To test if it worked, remove the images from your local Docker system. If you use the -f flag and specify the image ID then the docker rmi command untags and removes all images that match that ID (get the image ID from docker images): docker rmi -f &lt;image_id&gt; Now you can pull the image from your local registry: docker pull localhost:5000/r-shiny:v1 The next command stops and removes the registry container. It is a daemonized (background) process, so CTRL+C won’t work. The -v option makes sure to remove anonymous volumes associated with the container which is often used to mount a volume from your hard drive into the container where the images are stored: docker container stop registry &amp;&amp; \\ docker container rm -v registry If you want your registry to be accessed over a public network, then you need to think about security and access control. You’ll have to set up transport layer security (TLS) for HTTPS and user authentication, which are advanced topics and we recommend using a commercial container registry that we listed above and use private repositories to control access to your images. 6.6 The Dockerfile It is time to review the Dockerfiles line by line and learn about each of the different types of instructions and their uses. We organize the sections according to functional steps based on the Dockerfiles for our R and Python apps. The full Dockerfile reference can be found at https://docs.docker.com/reference/dockerfile/. 6.6.1 The Parent Image The FROM instruction initializes a new build stage and sets the base (FROM SCRATCH) or parent image (e.g. FROM ubuntu:24.04). For the R version we used the FROM rocker/r2u:24.04 and for the Python version we used FROM python:3.9. We will review the different parent images and how to use multiple parent images in the same Dockerfile as part of a multi-stage build later. 6.6.2 Metadata The LABEL instruction is optional, it adds metadata to an image, e.g. who to contact in case of issues or questions: LABEL maintainer=&quot;USER &lt;user@example.com&gt;&quot; We’ll talk more about labels as part of continuous integration and continuous delivery (CI/CD) /FIXME: add ref here/. 6.6.3 Dependencies We often use the RUN instruction to install dependencies and use other shell commands to set permissions to files, etc. RUN executes a command in a new layer on top of the current image. We used the RUN R -q -e \"install.packages('shiny')\" to install the shiny R package, whereas the Python version used the requirements.txt alongside the pip command as: COPY app/requirements.txt . RUN pip install --no-cache-dir --upgrade -r requirements.txt We also used RUN to add a user called app to a Linux user group called app. This is needed because you do not want to run containers as the root user in production. Running the container with root privileges allows unrestricted use which is to be avoided. Although you can find lots of examples on the Internet where the container is run as root, this is generally considered bad practice. This is how we created the non-root user: RUN groupadd app &amp;&amp; useradd -g app app 6.6.4 Directories and Files Next we changed the working directory to /home/app that is he home folder of the non-privileged app user: WORKDIR /home/app The WORKDIR instruction sets the working directory for any RUN, CMD, ENTRYPOINT, COPY and ADD instructions that follow it in the Dockerfile. The COPY instruction copies new files or directories from the source and adds them to the file system of the container at the destination path. COPY app . The source here is the app folder inside the Docker build context. The contents of the folder, including the Shiny app files, are copied. The destination path . refers to the current work directory defined previously, in this case, the /home/app folder. Use an alternative format if the paths contain whitespace: COPY [\"dir with space\", \".\"]. You would almost always use COPY in your Dockerfile, but a very similar instruction is ADD. ADD allows the source to be a URL, a Git repository, or a compressed file. Wildcards, such as * for multiple characters and ? for single character, are supported in COPY and ADD instructions. For example COPY app/*.py . will copy only the Python scripts and nothing else. Normally when the source does not exist docker build exits with an error. An interesting feature of wildcards is that the build does not error if there are no matching results. For example COPY *renv.lock . will copy the renv.lock file if it exists, but the build won’t stop if it does not. The owner of the files and directories at the destination is the root user. If you want to set the user and the group so that the non-root user will be able to access these resources you can use the optional --chown flag that stands for change owner: COPY --chown app:app app . Here the --chown app:app sets the user and the group values to app. This is equivalent to the following combination of COPY and RUN: COPY app . RUN chown app:app -R /home/app Similarly, use the --chmod flag to define read/write/execute permissions. 6.6.5 Switching User The USER instruction sets the user name to use as the default user for the ENTRYPOINT and CMD commands. We used USER app to switch to the non-root app user. 6.6.6 Expose a Port The EXPOSE instruction defines the port that Docker container listens on at runtime. We chose port 3838 with the EXPOSE 3838 instruction. This is the container port that we connect to using the docker run -p 8080:3838 &lt;image-name&gt; command. You can pick any port, but remember that exposing a lower port number, like 80 (the standard HTTP port) will require elevated privileges. In general, we recommend using port numbers 1024 and above. Using lower ports will result in failures with a non-root user, such as our app user. 6.6.7 Variables The ENV instruction sets the default values for environment variables. We set two variables for the Python app to allow configs to be written for matplotlib by the app user: ENV MPLCONFIGDIR=/home/app/.config ENV HOME=/home/app These environment variables will be part of the final image, so do not use ENV to add secrets to the image at build time. Such environment variables should be added at runtime, e.g. with docker run --env TOKEN=&lt;token-value&gt; &lt;image-name&gt; or using a file as docker run --env-file .env &lt;image-name&gt; which will read the variables from the .env file. The ARG instruction defines a variable that users can pass at build-time. For example, adding ARG GITHUB_PAT to the Dockerfile would allow you to use the remotes::install_github() function to install an R package from a private GitHub repository. You can provide the token value to docker build as: export GITHUB_PAT=&lt;your-token&gt; docker build -t &lt;image-tag&gt; --build-arg=&quot;GITHUB_PAT=${GITHUB_PAT}&quot; . The token value will not be available in the container at runtime. 6.6.8 Executable and Command We got to the end of the Dockerfile. This is where we define the process that is executed inside the container at run time via the ENTRYPOINT instruction. This is often omitted. In that case, the default executable is set to /bin/sh -c that is the shell executable. Shell is a basic command-line interpreter and the -c flag indicates that the shell will read the commands to execute from a string. This string is provided through the CMD instruction. For example we can add CMD uvicorn app:app --host 0.0.0.0 --port 3838 as the default set of arguments supplied to the ENTRYPOINT process to start the Python Shiny app. The RUN, CMD, and ENTRYPOINT instructions have two possible forms. The shell form is used mostly with the RUN instruction because it allows useful shell features like piping the output and chaining commands. The shell form is written without square brackets and it would look like this for the R Shiny app: CMD R -e &quot;shiny::runApp(host=&#39;0.0.0.0&#39;, port=3838)&quot; This command will execute as a child processes of the shell, and as such, signals like CTRL+C will not be forwarded to the child process by the shell. This is why it is recommended to use the so called “exec” form for the CMD and ENTRYPOINT instructions. The “exec” form is written between square brackets. Here it is for the R version: CMD [&quot;R&quot;, &quot;-e&quot;, &quot;shiny::runApp(host=&#39;0.0.0.0&#39;, port=3838)&quot;] And for the Python version: CMD [&quot;uvicorn&quot;, &quot;app:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;3838&quot;] When we discussed local hosting of the Shiny apps we did not review all the possible arguments for the R and Python commands. Two options here beg for introduction. The host defines the IP address that the app listens on. The default host value is 127.0.0.1 (also known as localhost or loopback address). If we leave the host at its default value, we will not be able to access the container from outside because localhost can only be accessed from the same address. This is the reason why we need to set it to 0.0.0.0 which can be accessed from outside of the container. The other important argument is the TCP port that the application is listening on. If the prot is not provided for the R Shiny command, Shiny will pick a random port number. We obviously do not want to guess this port, so we need to set it. The 3838 port number is the same as the number we exposed via the EXPOSE 3838 instruction. It is possible to use an environment variable for the port number and substitute it in the CMD command: ENV PORT=3838 CMD [&quot;R&quot;, &quot;-e&quot;, &quot;shiny::runApp(host=&#39;0.0.0.0&#39;, port=$PORT)&quot;] This way the default value is set to 3838, but you can override it at runtime as docker run --env PORT=5000 &lt;image-name&gt;. 6.7 Parent Images We have reviewed Docker basics and how to dockerize a very simple Shiny app. For anything that is a little bit more complex, you will have to manage dependencies. Dependency management is one of the most important aspects of app development with Docker. And it begins with finding the right parent image as the basis of the rest of your Dockerfile. The ground zero for Docker images is the reserved and explicitly empty image called scratch. FROM scratch is used for hosting super minimal images containing a single binary executable or as the foundation of common base images such as debian, ubuntu or alpine. Debian is a Linux distribution that’s composed entirely of free and open-source software and is a community project. Ubuntu is derived from Debian and is commercially backed by Canonical. Ubuntu uses the same APT packaging system as Debian and many packages and libraries from Debian repositories. Both of these Linux distributions are loved for their versatility and reliability, and the huge user base ensures first class community support. Alpine Linux is a minimal distribution independent of Debian and other distributions. It was designed with a focus on simplicity, security, and efficiency. This distribution has a very compact size, and therefore is a popular choice for embedded systems and IoT devices. This distribution is also community maintained. Here are the sizes for these three images. Debian is the largest, Ubuntu in the middle, and the Alpine being more than 10 times smaller: REPOSITORY TAG IMAGE ID CREATED SIZE debian 12.6 7b34f2fc561c 7 days ago 117MB ubuntu 24.04 35a88802559d 4 weeks ago 78.1MB alpine 3.20 a606584aa9aa 2 weeks ago 7.8MB Many of the commonly used R and Python parent images use Debian/Ubuntu or Alpine as the starting point. The general trade-off between these two lineages comes down to convenience vs. minimalism. Ubuntu and its derivatives tend to be much larger in size, but build times can be considerably faster due to very mature package management system and the availability of pre-built binaries. Alpine-based images, however, tend to be much smaller, almost bare bones. Alpine uses different compilers that Ubuntu so you’ll often have to build and compile your packages from source. This can be tedious and time consuming. However, its small size reduces the surface area for potential attackers and as a result the images tend to be less vulnerable. The final image size is important to consider, but images based on the same parent image share lots of their layers anyways, so the images can pack much tighter on your hard drive than you might think based on their size without subtracting common layers. Also,the size advantage of the Alpine distribution evaporates quickly as you start adding R and Python libraries. Some packages will take up more space than the parent image itself. 6.7.1 Popular Parent Images for R Let’s see some of the most popular base R images based. Here is the output from docker images after pulling each of these Docker images: REPOSITORY TAG IMAGE ID CREATED SIZE r-base 4.4.1 16511f39cdb4 3 weeks ago 833MB rocker/r-base 4.4.1 22b431698084 3 weeks ago 878MB rocker/r-ver 4.4.1 9bb36eff1caa 3 weeks ago 843MB rocker/shiny 4.4.1 a90ccd5c09b9 3 weeks ago 1.58GB rocker/r2u 24.04 1441545ed6df 2 weeks ago 800MB rhub/r-minimal 4.4.1 1e280d0205b7 3 weeks ago 44.9MB The official r-base image is an R installation built on top of Debian. It is maintained by the Rocker community (https://rocker-project.org/, Boettiger and Eddelbuettel (2017)). This r-base image is like the rocker/r-base, these two images are built from the same Dockerfile, but with different build tools. The Debian Linux distribution is more cutting edge than Ubuntu. This means it has unstable repos added and it receives updates faster. It is for those who like to live on the cutting edge of development. For those who value stability more, the Ubuntu based images could be better suited. Such as the Rocker versioned stack, rocker/r-ver, which emphasizes reproducibility. This stack has both AMD64 and experimental ARM64 support for R version 4.1.0 and later. For the AMD64 platform, it serves compiled binaries of R packages that makes package installs speedy. The default CRAN mirror for rocker/r-ver is set to the Posit Public Package Manager (P3M, https://p3m.dev/, previously called RStudio Package Manager or RSPM). To ensure reproducibility, the non-latest R version images install R packages from a fixed snapshot of the CRAN mirror at a given date. So you’ll end up with the same package versions no matter when you build your image. The rocker/shiny image is based on the rocker/r-ver stack and comes with Shiny related packages and Shiny Server Open Source installed. This makes it the beefiest of all the images presented. It has 68 packages available instead of the 31 within the r-base and r-ver stacks (14 base and 15 recommended packages). The images so far have been tagged by the R version that is inside the image, e.g. 4.4.1. The rocker/r2u is based on Ubuntu as well, and it brings Ubuntu binaries for CRAN packages fully integrated with the system package manager (apt). When you use install.packages() it will call apt in the background. This has the advantage that system dependencies are fully resolved, i.e. no need to guess and manually install them. Installations are also reversible. It uses the CRAN mirror at https://r2u.stat.illinois.edu. Keep in mind that packages and R itself are generally the highest available version. Therefore, the image tag is not based on the R version but based on the Ubuntu LTS (Long Term Support) version, like 24.04. All the Rocker images pack utilities that help with command line tasks, such as installing packages via install.r and installGithub.r, all part of the littler project (Eddelbuettel and Horner 2024). There is even a command line tool for Shiny, so instead of CMD [\"R\", \"-e\", \"shiny::runApp(host='0.0.0.0', port=3838)\"] you can use CMD [\"shiny.r\", \"-o\", \"0.0.0.0\", \"-p\", \"3838\"] in your Dockerfile. Finally, the rhub/r-minimal image is based on Alpine Linux and is the tiniest available image for R. This feat is achieved by not having recommended R packages installed (it has a total of 14 required packages), it does not have any documentation or translations, no X11 window support. It does not even have C, C++ or Fortran compilers. So if an R package relies on compiled code, first you have to install a compiler, then later uninstall it to keep the image size minimal. The installr script provided as part of the image helps with the installation and clean-up of build time dependencies, see installr -h for the available options. If you are looking for Linux distributions other than what we listed so far (Debian, Ubuntu, Alpine), take a look at the rstudio/r-base images that bring versioned base R to many Linux distributions, e.g. CentOS, Rocky Linux, and OpenSUSE. 6.7.2 Popular Parent Images for Python The official Python images are maintained by the Python community and are either based on Debian or Alpine Linux. Here are the most commonly used variants: REPOSITORY TAG IMAGE ID CREATED SIZE python 3.9 8912c37cec43 12 days ago 996MB python 3.9-slim b4045d7da52e 12 days ago 125MB python 3.9-alpine 893ee28ab004 12 days ago 53.4MB The python:&lt;version&gt; image is the largest and is the most general supporting all kinds of use cases and is recommended as a parent image. It contains the common Debian packages and compilers. The python:&lt;version&gt;-slim version contains only the minimal Debian packages needed to run Python itself. It does not contain the compilers for modules written in other languages. This is the reason for the image’s reduced size. The python:&lt;version&gt;-alpine is based on Alpine and therefore is the smallest. It is similarly bare bones as the minimal R image. 6.8 Installing System Libraries System libraries are required for different purposes. Some libraries are needed during build time. While others are needed at run time. Say your R or Python package requires compilation or needs to dynamically link to other system libraries. In these cases you have to build your package using compilers (C, C++, Fortran, Rust) and other build time dependencies. System libraries used at build time includes header files and tend to have extra dependencies. These build time system libraries are named with a *-dev or *-devel postfix. Once your R or Python package has been compiled, you don’t need the build time libraries any more. However, you need the run time libraries. For example if your package needs to be built with libcurl4-openssl-dev, the run time dependency becomes libcurl4. The run time dependencies tend to be much smaller and have fewer dependencies. These will have no conflict with other run time libraries because of the lack of headers included. 6.8.1 Manually Installation FIXME: Review the Python specific parts. The Python Wheels project offers binary Python packages. R binaries can be found on CRAN for Windows and Mac OS X. But CRAN does not offer binaries for various Linux distributions for the obvious complexity involved in that. The Posit Public Package Manager provides pre-built binary packages for R and Python. It supports various Linux distributions, including Debian and Ubuntu. The R Universe project provide binaries for CRAN and GitHub packages for Windows, Mac OS X, and in most cases for WebAssembly that is suitable for Shinylive applications. The R Universe project only provides binaries for R packages on Ubuntu using the latest R version. R and Python packages, once compiled into a binary file, provide metadata about the run time dependencies. You can find the required system libraries on the website of a given repository or package manager. Alternatively, you can try installing the package without its requirements and follow the string of error messages to see what is it that you need to install as a prerequisite. This GitHub repository lists system requirements for R packages: rstudio/r-system-requirements. The primary purpose of this database is to support the Posit Public Package Manager. To get programmatic access to the database, you can call the Posit Public Package Manager’s API to request the system requirements. For example, for the curl R package (Ooms 2024), you can query the API as https://p3m.dev/__api__/repos/1/packages/curl/sysreqs?distribution=ubuntu. Replace the curl and ubuntu parts to get results for other R packages and Linux distributions. The HTTP GET request will result in a JSON response listing the libraries with the install script needed on Ubuntu (try pasting the link into the browser address line): { &quot;name&quot;: &quot;curl&quot;, &quot;install_scripts&quot;: [ &quot;apt-get install -y libcurl4-openssl-dev&quot;, &quot;apt-get install -y libssl-dev&quot; ] } You can also utilize the pak R package (Csárdi and Hester 2024) to query system requirements: pak::pkg_sysreqs(&quot;curl&quot;, sysreqs_platform=&quot;ubuntu&quot;) # -- Install scripts ---------------------------- Ubuntu NA -- # apt-get -y update # apt-get -y install libcurl4-openssl-dev libssl-dev # # -- Packages and their system dependencies ------------------ # curl - libcurl4-openssl-dev, libssl-dev Include these libraries in your Dockerfile as: RUN apt-get update &amp;&amp; \\ apt-get install -y --no-install-recommends \\ libcurl4-openssl-dev \\ libssl-dev \\ &amp;&amp; rm -rf /var/lib/apt/lists/* When using apt-get (the older and stable version of apt) the first command is always apt-get update which look for updates in the the package lists of the package repositories. This way the system will know if an update is necessary and where to find the individual packages. Next apt-get install &lt;package-name&gt; is called with a few flags: -y means that we answer yes to all the prompts, whereas --no-install-recommends will prevent unnecessary recommended packages from being installed. The last bit cleans up the package lists downloaded by apt-get update which are stored in the /var/lib/apt/lists folder. All these three commands are chained together with &amp;&amp; and we used \\ for breaking up single line commands to multiple lines for better readability (the backslash escapes a newline character). This arrangement helps organize the packages and you can also comment them out as needed. You could put all of these chained commands in a separate RUN line, but that is not recommended. Having a single RUN instruction will lead to a single image layer. But what is most important is that the update and install steps should not be separated. Imagine that you update the package lists and now the resulting layer is added to the Docker cache. The next time you add another package to install and rebuild your image. The update command is cached and will not be rerun by default. As a result, you might end up with an outdated version of the package. 6.8.2 Automated Dependency Resolution with r2u If you are using R on Ubuntu, the r2u project greatly facilitate dependency management. It uses the Debian package format for R packages for latest R version on various Ubuntu LTS platforms. This resolves all the system dependencies through using the .deb binary package format that combines together the pre-built binary package with the metadata about the dependencies. Then the Debian/Ubuntu package manager (apt) can do the rest. The binary packages are base on P3M where available or built natively. Selected BioConductor packages are also built natively on the project servers. The server hosting the .deb files is set up as a proper apt repository with a signed Release file containing metadata that can be used to cryptographically validate every package in the repository. This file guarantees that the packages you receive are the one you expected and there has been no tampering with it during download. Because the R packages now live as first class citizens on Ubuntu, uninstalling packages would not unnecessarily remove the shared dependencies that other packages depend on. With the r2u setup and using the rocker/r2u images, you can simply call install.packages(\"curl\") and apt will sort out the dependencies for you in the background. 6.8.3 Dependencies on Alpine Linux The Alpine Linux has a package manager called apk that is different from Debian’s and Ubuntu’s apt. This likely means that you might have to work harder to find all the Alpine-specific dependencies. You can still use the tools previously mentioned, but will have to find the library for Alpine. You can also follow the breadcrumbs of the error messages of missing dependencies. However, the general idea is similar when it comes to working with the Dockerfile: FROM rhub/r-minimal:4.4.1 RUN apk add --no-cache --update-cache \\ --repository http://nl.alpinelinux.org/alpine/v3.11/main \\ autoconf=2.69-r2 \\ automake=1.16.1-r0 &amp;&amp; \\ installr -d \\ -t &quot;libsodium-dev curl-dev linux-headers gfortran autoconf automake&quot; \\ -a libsodium \\ shiny plotly e1071 [...] First we declare the minimal parent image, then use a RUN instruction to add libraries from the Alpine repository. autoconf and automake is required for building the R packages (shiny, plotly, and e1071). This example is taken from the minimal Dockerfile for the Bananas example app. The next pieces uses the istallr utility that comes with the rhub/r-minimal image and it’s usage is explained on the GitHub site: https://github.com/r-hub/r-minimal. The -d flag will install C and C++ compilers (gcc, musl-dev,g++) temporarily, i.e. those will be removed after the successful compilation and installation of the R packages. The -t option lists Alpine packages to be installed temporarily (a.k.a. build time dependencies), and the -a option lists Alpine packages to keep (a.k.a. run time dependencies). The built in cleanup feature keeps the image sizes small in line with the goal of the parent image’s purpose. You can also list not only CRAN packages but different “remotes”, like tidyverse/ggplot2 for development version of ggplot2 from GitHub, or local::. to install from a local directory. The rest of the Dockerfile follow the same patterns as what we saw before. Adding a non-root user, defining the work directory, copying the Shiny app files from the build context to the image’s file system, setting owners for the files, switching to the non-root user, exposing port 3838 and defining the default command to run that calls shiny::runApp(): [...] RUN addgroup --system app &amp;&amp; \\ adduser --system --ingroup app app WORKDIR /home/app COPY app . RUN chown app:app -R /home/app USER app EXPOSE 3838 CMD [&quot;R&quot;, &quot;-e&quot;, &quot;shiny::runApp(host=&#39;0.0.0.0&#39;, port=3838)&quot;] The Dockerfile for the Faithful Shiny app (https://github.com/h10y/faithful) is somewhat simpler: FROM rhub/r-minimal:4.4.1 RUN installr -d \\ -t &quot;zlib-dev cairo-dev&quot; \\ -a &quot;cairo font-liberation&quot; \\ Cairo \\ shiny RUN addgroup --system app &amp;&amp; \\ adduser --system --ingroup app app WORKDIR /home/app COPY app . RUN chown app:app -R /home/app USER app EXPOSE 3838 CMD [&quot;R&quot;, &quot;-e&quot;, &quot;shiny::runApp(host=&#39;0.0.0.0&#39;, port=3838)&quot;] The reason why we need the Cairo package (Urbanek and Horner 2023) besides shiny is because the minimal image has no X11 graphic device support which is used to display base R plots, like histograms. Instead of X11, we can use the Cairo Graphics Library. You can see how we list the build time cairo-dev library and the run time cairo for the R package. The installation of the system packages and the general lack of available binary packages contributes to the longer build times. The Faithful example took 5 minutes to build with rhub/r-minimal compared to 16 seconds with rocker/r2u and the final image size was 230 MB compared to 975 MB. The Bananas app took 10.4 minutes vs. 28 seconds and resulted in an image of size 113 MB as opposed to 909 MB. As you can see, the image build time is significantly longer, while the minimal image size is multiplying relative to the parent image’s 45 MB size. The absolute increase in the Ubuntu based image’s size is similar but it is less noticeable in relative terms. Your Shiny apps are likely to have more dependencies that the examples we are using in this book. You will have to decide if the benefits that a minimal image provides will overweight the increased complexity and development time required to maintain minimalist Shiny images. 6.9 Installing R Packages The wealth of contributed R packages can supercharge Shiny app development. This also means that you have to manage these dependencies. In the previous sections, we have already hinted at installing these packages. We will now see the different ways of installing R packages. 6.9.1 Explicitly Stating Dependencies The first approach is to use RUN instructions in the Dockerfile to install the required packages. You can use the R -q -e \"&lt;expression&gt;\" pattern to use any R expression you would normally use in your R sessions to install packages. The -q flag stands for quiet and means to not print out the startup message. The -e option means to execute the R expression that follows it and then exit. The Rscript -e \"&lt;expression&gt;\" is another way to evaluate the expressions without echoing the expression itself. Most often we use the built in install.packages() function and the different options from the remotes package (Csárdi et al. 2024). RUN R -q -e &quot;install.packages(c(&#39;shiny&#39;, &#39;remotes&#39;))&quot; RUN R -q -e &quot;remotes::install_github(&#39;tidyverse/ggplot2&#39;)&quot; RUN R -q -e &quot;remotes::install_local(&#39;.&#39;)&quot; The Rocker images have the littler command line utility installed. The previous code piece can be written as: RUN install2.r --error --skipinstalled shiny remotes RUN installGithub tidyverse/ggplot2 RUN install2.r &quot;.&quot; The --error flag will throw an error when installation is unsuccessful instead of a warning, --skipinstalled will skip installing already installed packages. If installing from a private GitHub repository, remotes is going to use the GITHUB_PAT environment variable to authenticate with GitHub to be able to download and install the package. You have to add the following line to your Dockerfile before the RUN instructions that define the install from GitHub: ARG GITHUB_PAT To pass the GITHUB_PAT at image build, use the following commands: export GITHUB_PAT=ghp_xxxxxxxxxx docker build -t &lt;image-name&gt; --build-arg=&quot;GITHUB_PAT=${GITHUB_PAT}&quot; . The pak R package (Csárdi and Hester 2024) is planned to replace remotes in the future. It provides very similar functionality and improvements, like parallel downloads, caching, safe dependency solver for packages and system dependencies using the P3M database by creating an installation plan before downloading any packages. Not relevant for Linux containers, but pak can even handle locked package DLLs on Windows. Let’s see how these common install statements would look using pak (you don’t need remotes because of using pak): RUN R -q -e &quot;install.packages(&#39;pak&#39;)&quot; RUN R -q -e &quot;pak::pkg_install(&#39;shiny&#39;))&quot; RUN R -q -e &quot;pak::pkg_install(&#39;tidyverse/ggplot2&#39;)&quot; RUN R -q -e &quot;pak::pkg_install(&#39;.&#39;)&quot; The installr utility shipped with the rhub/r-minimal image uses pak to install packages. The -p flag will remove pak after the installation. You can do: RUN installr -p shiny tidyverse/ggplot2 local::. You can pin package versions using Git references, e.g. tidyverse/ggplot2@v1.0.0, or using remotes::install_version(). These functions allow you to provide repositories as argument. But more often, the repositories are set globally as part of options(). You can include the list of repositories in the Rprofile.site file: # Rprofile.site local({ r &lt;- getOption(&quot;repos&quot;) r[&quot;p3m&quot;] &lt;- &quot;https://p3m.dev/cran/__linux__/noble/2024-07-09&quot; r[&quot;archive&quot;] &lt;- &quot;https://cranhaven.r-universe.dev&quot; r[&quot;CRAN&quot;] &lt;- &quot;https://cloud.r-project.org&quot; options(repos = r) }) The P3M repository is set for Ubuntu 24.04 (Noble) with a snapshot of CRAN taken on July 9th, 2024. This effectively freezes package versions to enhance reproducibility. This way of pining maximum package versions is also called “time travel”. The CRAN Haven repository is for recently archived CRAN packages. These archived packages might find their way back to CRAN after addressing the issues for which those got archived. This repository can handle such temporary inconveniences. COPY this file into the /etc/R folder on the Rocker images: COPY Rprofile.site /etc/R As you develop your Shiny app, you will have to occasionally update the Dockerfile to add manually in any new dependencies. If you forget to do this you will be reminded by an error the next time you build your Docker image and run your app. 6.9.2 Using the DESCRIPTION File You have seen how to use remotes::install_local() to install a dependency from a local directory or a .tar.gz source file. The list of packages to be installed is determined by the DESCRIPTION file that is at the base of any package folder. It lists the dependencies of the packages. Packages listed under the Imports field are installed and needed by the package. Other packages listed under the Suggests field are needed for development but are not essential for using the package. A commonly used hack in the R community is to highjack the package development tooling to simplify installation of the codebase even if it is not necessarily structured as a package. The only thing you need for remotes::install_local() and similar functions to work is the DESCRIPTION file. You don’t even need all the customary fields like title, description, or package maintainer in this file. For the Bananas app, we need shiny, plotly and the e1071 package listed under Imports. Put this in the DESCRIPTION file: Imports: e1071, plotly, shiny In the Dockerfile you need to COPY the DESCRIPTION file into the file system of the image, then call the remotes::install_deps() function. The upgrade='never' argument will prevent installing newer versions of existing packages, thus cutting down unnecessary install time: FROM rocker/r2u:24.04 COPY app/DESCRIPTION . RUN R -q -e &quot;remotes::install_deps(upgrade=&#39;never&#39;)&quot; [...] You can use the Rprofile.site to specify your preferred repositories. The remotes functions will respect those settings. Using the DESCRIPTION file lets you record the dependencies outside of your Dockerfile. As you develop your Shiny app, you have to update the DESCRIPTION. When the file changes, Docker will invalidate the cache and install the dependencies with the new packages included. 6.9.3 Using renv The renv package (Ushey and Wickham 2024) is a dependency management toolkit for R. You can create and manage R libraries in your local project and record the state of these libraries to a lockfile. This lockfile can be used later to restore the project, thus making projects more isolated, portable, and reproducible. If you are using renv with your Shiny projects, you are probably already familiar with the workflow. You can discover dependencies with renv::init() and occasionally save the state of these libraries to a lockfile with renv::snapshot(). The nice thing about this approach is that the exact version of each package is recorded that makes Docker builds reproducible as well. The renv package has a few different snapshot modes. The default is called “implicit”. This mode adds the intersection of all your installed packages and those used in your project as inferred by renv::dependencies() to the lockfile. The other mode is called “explicit” snapshot that only captures packages that are listed in the project DESCRIPTION. The “custom” model lets you specify filters for modifying the implicit snapshot, so that you will not end up with the kitchen sink of packages in your Docker image. Read the Using renv with Docker vignette of renv for more useful tips. Once you have the lockfile in your project folder, you can use it like this: [...] RUN install.r renv COPY app/renv.lock . RUN R -q -e &quot;options(renv.consent=TRUE);renv::restore()&quot; [...] You have to install renv, copy the renv.lock file over, and use the renv::restore() command. The renv.consent option gives consent to renv to write and update certain files. renv pins the exact package versions in the lockfile. This is necessary for reproducibility. But full reproducibility is much harder than just the version of R and the package versions. You have to think about the operating system, your system dependencies, and even the hardware (Rodrigues 2023). Exact package versions could take quite long to install. The reason for that is that binary version of the packages might disappear from the package repositories. In that case, renv will install it from source and it will need possible build time dependencies. The older the lockfile gets, the more problematic it can be to install it without hiccups. 6.9.4 Using deps renv goes to great lengths to make your R projects perfectly reproducible. This requires knowing the exact package versions and the source where it was installed from (CRAN, remotes, local files). This information is registered in the lock file, which serves as the manifest for recreating the exact replica of the environment. Full reproducibility is often required for reports, markdown-based documents, and scripts. These are loosely defined projects combined with strict version requirements, often erring on the side of “more dependencies are safer”. On the other end of the spectrum, you have package-based development. This is the main use case for dependency management-oriented packages, such as remotes and pak. In this case, exact versions are managed only to the extent of avoiding breaking changes (given that testing can surface these). So what we have is a package-based workflow combined with a “no breaking changes” philosophy to version requirements. This approach often leads to leaner installation. If you are developing your Shiny app as an R package, then the package-based development is probably the way to go. You already have a DESCRIPTION file, so just keep developing. But what if you are not writing an R package and wanted to combine the best of both approaches? A loosely defined project with just strict-enough version requirements without having to manage a DESCRIPTION file. Why would you need a DESCRIPTION file when you have no R package? Also, there is a lot that a DESCRIPTION file won’t do for you. You can manage dependencies with the deps (Sólymos 2024) package by decorating your existing R code with special, roxygen-style comments. For example, here is how you can specify a remote, and alternative CRAN-like repository, pin a package version, or install from a local source: #&#39; @remote analythium/rconfig@CRAN-v0.1.3 rconfig::config() #&#39; @repo sf https://r-spatial.r-universe.dev library(sf) #&#39; @ver rgl 0.108.3 library(rgl) #&#39; @local mypackage_0.1.0.tar.gz library(mypackage) You can exclude development packages with the @dev decorator and list system requirements following the @sys decorator. deps helps to find all dependencies from our files using renv::dependencies(). It writes these dependencies into the dependencies.json file, including the information contained in the comments when using the deps::create() function. The decorators make your intent explicit, just like if we were writing an R package. But we do not need to manually write these into a file and keep it up-to-date. We can just rerun create() to update the JSON manifest file. create() crawls the project directory for package dependencies. It will amend the dependency list and package sources based on the comments. The other function in deps is install() which looks for the dependencies.json file in the root of the project directory (or runs create() when the JSON file is not found) and performs dependency installation according to the instructions in the JSON file. Here is the Dockerfile usage where first we install the deps package and copy the dependencies.json. The next RUN instruction is needed if you had system requirements specified via @sys. The jq package is used to parse the JSON and install any of these libraries. Finally, the line with deps::install() that performs the R package installation based on the JSON file: [...] RUN install.r deps COPY app/dependencies.json . RUN apt-get update &amp;&amp; \\ apt-get install -y --no-install-recommends \\ jq RUN apt-get update &amp;&amp; \\ apt-get install -y --no-install-recommends \\ $( jq -r &#39;.sysreqs | join(&quot; &quot;)&#39; dependencies.json ) RUN R -q -e &quot;deps::install()&quot; [...] The deps package comes with a small command line utility that can be added to the dockerfile to simplify the installation process: FROM rocker/r2u:24.04 RUN install.r pak rconfig deps RUN cp -p \\ $(R RHOME)/site-library/deps/examples/03-cli/deps-cli.R \\ /usr/local/bin/deps-cli RUN chmod +x /usr/local/bin/deps-cli COPY app . RUN deps-cli all [...] The deps-cli all command will analyze dependencies and install system and R dependencies in 1 line. It also looks for the following files before attempting to auto-detect dependencies in the absence of the dependencies.json: renv.lock, pkg.lock, and DESCRIPTION. pkg.lock is the lockfile created by pak. 6.10 Python Requirements FIXME: Add content here. renv https://cran.r-project.org/web/packages/renv/vignettes/python.html pip Conda, venv? Docker’s purpose is isolation, so multiple venv’s do not make sense in containers according to some posts. Maybe explain that here? mention shinylive installs how to reuse .cache by mounting it: https://testdriven.io/blog/docker-best-practices/#cache-python-packages-to-the-docker-host 6.11 Dynamic Shiny Apps Dynamic Shiny app require a runtime environment on a host server that can support the HTTP and websocket connections required for R and Python to communicate with the client (Fig. 5.6). Now that we have reviewed Shiny app development and general principles of working with Docker, we can dive into specific examples. We will use the Old Faithful example. You can follow along using the code in the repository at https://github.com/h10y/faithful. There are many examples organized into folders inside the repo. Consult the readme file about each of these. In general, we provide Docker images for the dynamic Shiny app examples in the form of h10y/faithful/&lt;folder-name&gt;:latest. You can use docker run to pull and start the Shiny app as: docker run -p 8080:3838 ghcr.io/h10y/faithful/&lt;folder-name&gt;:latest Visit http://localhost:8080 in your browser to try the app. 6.11.1 R The simplest containerized app follows the Dockerfile examples for R that you have seen so far. We provide two Dockerfiles, one for the rocker/r2u and one for the rhub/r-minimal image within the r-shiny folder. Follow this example if your Shiny app consists of a single or multiple files. Put those files in the app folder. Pick your favorite way of specifying your dependencies, and edit the Dockerfile accordingly. The CMD instruction for this type of setup is shiny::runApp(). FROM rocker/r2u:24.04 # Add here your dependencies RUN R -q -e &quot;install.packages(&#39;shiny&#39;)&quot; RUN groupadd app &amp;&amp; useradd -g app app WORKDIR /home/app COPY app . RUN chown app:app -R /home/app USER app EXPOSE 3838 CMD [&quot;R&quot;, &quot;-e&quot;, &quot;shiny::runApp(host=&#39;0.0.0.0&#39;, port=3838)&quot;] If your Shiny app is organized as nested files following the structure expected by the rhino package, check the r-rhino folder for an example. rhino relies on the renv package for dependency management, so edit the Dockerfile accordingly. The CMD instruction for a Rhino app is the same as for the r-shiny setup, because Rhiny uses an app.R file as its entrypoint that is being recognized by Posit products as a Shiny app. shiny::runApp() will also recognize it as a Shiny app. The app.R file has a single command, rhino::app(), that returns a Shiny app object. If you follow a package based development for your Shiny app, check out the r-package, r-golem, and r-leprechaun folders for Dockerfile examples. Here is the r-package example that does not follow any specific framework, but uses the DESCRIPTION file to define its dependencies that will be picked up by remotes::install_local(). At the end, we call a function from the package itself to launch the Shiny app, i.e. faithful::run_app(): FROM rocker/r2u:24.04 RUN groupadd app &amp;&amp; useradd -g app app RUN R -q -e &quot;install.packages(&#39;remotes&#39;)&quot; COPY faithful faithful RUN R -q -e &quot;remotes::install_local(&#39;faithful&#39;)&quot; USER app EXPOSE 3838 CMD [&quot;R&quot;, &quot;-e&quot;, &quot;faithful::run_app(host=&#39;0.0.0.0&#39;, port=3838)&quot;] The install.packages('remotes') line is not necessary for the rocker/r2u image because it comes preinstalled. But we left the line there so that the Dockerfiles can be used with other parent images that might not have remotes available. The Golem and Leprechaun framework based Dockerfiles are slight variations of this. Here is the one with Golem: [...] COPY faithfulGolem faithfulGolem RUN R -q -e &quot;remotes::install_local(&#39;faithfulGolem&#39;)&quot; [...] CMD [&quot;R&quot;, &quot;-e&quot;, &quot;faithfulGolem::run_app( \\ options=list(host=&#39;0.0.0.0&#39;, port=3838))&quot;] The same with Leprechaun: [...] COPY faithfulLeprechaun faithfulLeprechaun RUN R -q -e &quot;remotes::install_local(&#39;faithfulLeprechaun&#39;)&quot; [...] CMD [&quot;R&quot;, &quot;-e&quot;, &quot;faithfulLeprechaun::run( \\ options=list(host=&#39;0.0.0.0&#39;, port=3838))&quot;] 6.11.2 Python The Python version of the app follows similar principles. You can find it in the py-shiny folder. The parent image is python:3.9, and dependency management is done via pip and the requirements.txt file. Edit this file as the starting point for your single or multiple file Python for Shiny apps. The CMD instruction calls uvicorn to host the app: FROM python:3.9 # Add here your dependencies COPY app/requirements.txt . RUN pip install --no-cache-dir --upgrade -r requirements.txt RUN groupadd app &amp;&amp; useradd -g app app WORKDIR /home/app COPY app . RUN chown app:app -R /home/app USER app EXPOSE 3838 RUN mkdir .config ENV MPLCONFIGDIR=/home/app/.config ENV HOME=/home/app CMD [&quot;uvicorn&quot;, &quot;app:app&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--port&quot;, &quot;3838&quot;] 6.11.3 R Markdown Hosting R Markdown documents with the runtime: shiny is very similar to hosting regular Shiny apps. You need pandoc as a system requirement that you can install with the image operating system’s package manager. The CMD instruction uses rmarkdown::run() to render and run the document every time a user connects to it: FROM rocker/r2u:24.04 RUN apt-get update &amp;&amp; \\ apt-get install -y --no-install-recommends \\ pandoc \\ &amp;&amp; rm -rf /var/lib/apt/lists/* RUN R -q -e &quot;install.packages(c(&#39;shiny&#39;, &#39;rmarkdown&#39;))&quot; RUN groupadd app &amp;&amp; useradd -g app app WORKDIR /home/app COPY app . RUN chown app:app -R /home/app USER app EXPOSE 3838 CMD [&quot;R&quot;, &quot;-e&quot;, &quot;rmarkdown::run( \\ shiny_args = list(port = 3838, host = &#39;0.0.0.0&#39;))&quot;] The prerendered option with runtime: shinyrmd require a rendering step by calling rmarkdown::render() before the final rmarkdown::run() in the CMD. Note that the RMARKDOWN_RUN_PRERENDER is set to 0 that tells rmarkdown to not render the document for every user. The HTML is rendered only once so only the reactive components need to be dealt with: FROM rocker/r2u:24.04 RUN apt-get update &amp;&amp; \\ apt-get install -y --no-install-recommends \\ pandoc \\ &amp;&amp; rm -rf /var/lib/apt/lists/* RUN R -q -e &quot;install.packages(c(&#39;shiny&#39;, &#39;rmarkdown&#39;, &#39;deps&#39;))&quot; RUN groupadd app &amp;&amp; useradd -g app app WORKDIR /home/app COPY app . RUN R -q -e &quot;deps::install(ask=FALSE)&quot; RUN R -e &quot;rmarkdown::render(&#39;index.Rmd&#39;)&quot; RUN chown app:app -R /home/app USER app EXPOSE 3838 ENV RMARKDOWN_RUN_PRERENDER=0 CMD [&quot;R&quot;, &quot;-e&quot;, &quot;rmarkdown::run( \\ shiny_args = list(port = 3838, host = &#39;0.0.0.0&#39;))&quot;] 6.11.4 Quarto with R Using Quarto with R to build and image is conceptually very similar to R Markdown. First we install quarto. There are many ways of installing it, this is one option. You need curl to download the Quarto installer, we use gdebi here to install Quarto from the downloaded .deb package. Once we have Quarto installed, we remove the Gdebi build time dependency. FROM rocker/r2u:24.04 RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ pandoc \\ curl \\ gdebi-core \\ &amp;&amp; rm -rf /var/lib/apt/lists/* RUN curl -LO https://quarto.org/download/latest/quarto-linux-amd64.deb RUN gdebi --non-interactive quarto-linux-amd64.deb RUN apt-get purge -y gdebi-core &amp;&amp; apt-get autoremove -y RUN install.r quarto shiny deps RUN groupadd app &amp;&amp; useradd -g app app WORKDIR /home/app COPY app . RUN R -q -e &quot;deps::install(ask=FALSE)&quot; RUN quarto render index.qmd RUN chown app:app -R /home/app USER app EXPOSE 3838 CMD [&quot;quarto&quot;, &quot;serve&quot;, &quot;index.qmd&quot;, &quot;--port&quot;, &quot;3838&quot;, \\ &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--no-render&quot;] The prerendering uses quarto render before quarto serve with the --no-render flag in the CMD instruction at the end. 6.11.5 Quarto with Python The Python version with Quarto works very similarly, the only difference is the pip install part based in the requirements.txt file. Otherwise, the Python code will be COPY-ed over as part of the .qmd file. The rendering and serving steps are the same: FROM python:3.9 RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ pandoc \\ curl \\ gdebi-core \\ &amp;&amp; rm -rf /var/lib/apt/lists/* RUN curl -LO https://quarto.org/download/latest/quarto-linux-amd64.deb RUN gdebi --non-interactive quarto-linux-amd64.deb RUN gdebi --non-interactive quarto-linux-amd64.deb COPY app/requirements.txt . RUN pip install --no-cache-dir --upgrade -r requirements.txt RUN groupadd app &amp;&amp; useradd -g app app WORKDIR /home/app COPY app . RUN quarto render index.qmd RUN chown app:app -R /home/app USER app EXPOSE 3838 CMD [&quot;quarto&quot;, &quot;serve&quot;, &quot;index.qmd&quot;, &quot;--port&quot;, &quot;3838&quot;, \\ &quot;--host&quot;, &quot;0.0.0.0&quot;, &quot;--no-render&quot;] 6.11.6 Shiny Server Shiny Server is part of the rocker/shiny image which is commonly used Docker parent image for hosting Shiny apps. Shiny Server’s Professional version was originally the enterprise self hosting option for Shiny apps before Posit Connect. Nowadays, there is only an Open Source version available. We will talk about non-containerized Shiny Server as a hosting option later. A nice feature of a containerized Shiny Server compared to previously discussed options is that it can host multiple Shiny apps, even R, R Markdown, Python, and Quarto apps together in the same container. FIXME: Provide link to this example. We’ll use the following setup, combining multiple version of our three example apps. We include the apps and their version in a hierarchical folder structure within the apps folder. Besides these files we have an index.html file in the root of the apps folder which will provide links to the different apps inside the nested folders. The Dockerfile sits next to the apps folder: ├── apps │ ├── bananas │ │ └── r-shiny │ ├── faithful │ │ ├── py-shiny │ │ ├── quarto-r-shiny │ │ ├── r-shiny │ │ └── rmd-shiny │ ├── lbtest │ │ ├── py-shiny │ │ └── r-shiny │ └── index.html └── Dockerfile We use the rocker/shiny:4.4.1 parent image, install pip for Python 3 because we need it to install requirements for the Python Shiny app (pip is not part of the parent image but you already have Python). We use install2.r to manually install R package dependencies. Shiny Server comes with a few apps pre-installed inside the /srv/shiny-server folder. We remove all these files before we COPY the contents from the apps directory over to the image. Once all the files are there, we install R dependencies by relying on the deps packages crawling feature: deps::install(ask=FALSE) will create a temporary dependencies.json file based on exploring all the dependencies inside the newly populated /srv/shiny-server folder. Next we install Python dependencies. Finally, in CMD we call the executable /usr/bin/shiny-server that will start the Shiny Server. It will by default listen on port 3838. FROM rocker/shiny:4.4.1 RUN apt-get update &amp;&amp; apt-get install -y \\ --no-install-recommends \\ python3-pip \\ &amp;&amp; rm -rf /var/lib/apt/lists/* RUN install2.r --error --skipinstalled \\ shiny \\ bslib \\ rmarkdown \\ quarto \\ deps RUN rm -rf /srv/shiny-server/* COPY ./apps /srv/shiny-server/ RUN R -q -e &quot;deps::install(&#39;/srv/shiny-server&#39;,ask=FALSE)&quot; RUN pip install --no-cache-dir --upgrade \\ -r /srv/shiny-server/faithful/py-shiny/requirements.txt USER shiny EXPOSE 3838 CMD [&quot;/usr/bin/shiny-server&quot;] You can build the image and start a container as: docker build -t shiny-server . docker run -p 8080:3838 shiny-server Visit http://localhost:8080 to see the landing page (Fig. 6.2. Use the links to navigate the different apps. Figure 6.2: Landing page for the containerized Shiny Server deployment. 6.12 Static Shiny Apps Static Shiny apps can be rendered on your local machine as explained in 5.2. You’d use shinylive::export(\"&lt;input-folder&gt;\", \"&lt;output-folder&gt;\") in R and shinylive export &lt;input-folder&gt; &lt;output-folder&gt; in Python to render the static pages for Shinylive based on a Shiny app inside the &lt;input-folder&gt; folder. The Quarto examples that used Shinylive were rendered similarly with the quarto render &lt;input-file&gt; --output-dir &lt;output-folder&gt; command. As you saw before, you need to have a proper webserver hosting the pages and the required CSS and JavaScript assets to avoid cross-origin resource sharing (CORS) issues. You can upload the static files to any file hosting service, like GitHub or GitLab Pages, Netlify, or the DigitalOcean App Platform. These platforms all offer a free tier for hosting static assets. The static pages we rendered based on the Old Faithful example are deployed to GitHub Pages at https://h10y.github.io/faithful/. You can also set up your self hosted virtual server in the cloud to host static files, as you will see in later chapters. Apart from static hosting, it is possible to put the static files in a Docker image behind a server process. This option assumes that you have all the build time requirements for rendering the Shinylive or Quarto with Shinylive documents. You can use R or Python as well to serve the static contents using the much bulkier parent images of rocker/r2u or python:3.9. You can use the httpuv R package or the http.server Python module. Although this is clearly doable, it is not necessary and will result in a much larger Docker image. The same outcome can be achieved by using a slimmer parent image and using a multi-stage Docker build process. We’ll use the OpenFaaS Watchdog for static hosting. It is a single binary file that is built for multiple architectures and is able to serve static assets among other use cases. It is well suited for containerized deployments because it provides a healthcheck mechanism and exposes metrics for observability. You may notice in this example that we have two FROM instructions. The first one takes the watchdog image, the second one uses a small Alpine linux image. We COPY the contents from the app folder of the build context to the /home/app folder of the Alpine image’s file system as we did before. Then we copy the fwatchdog executable file from the watchdog image as well. The only thing why the Dockerfile looks a bit verbose is because we want to create a non-privileged user as we did before to harden our image’s security. FROM ghcr.io/openfaas/of-watchdog:0.10.1 AS watchdog FROM alpine:3.20 RUN adduser -D app USER app WORKDIR /home/app COPY app . COPY --from=watchdog /fwatchdog . ENV mode=&quot;static&quot; ENV static_path=&quot;/home/app&quot; HEALTHCHECK --interval=3s CMD [ -e /tmp/.lock ] || exit 1 CMD [&quot;./fwatchdog&quot;] The same Dockerfile would work for all static Shiny output whether that is coming from Shinylive or Quarto with Shinylive. Just have these files in the app folder and you are done. The FROM &lt;image&gt; AS &lt;stage&gt; specification allows us to later specify which parent image we refer to. This is done via the --from=&lt;stage&gt; flag for the COPY instruction. The multiple images used in the same Dockerfile are called stages, and the a build is referred to as a multi-stage build. It is often used to use a bulkier image for build purposes and a leaner image to only copy specific files to. In this case we copy the static contents and an executable file called watchdog. The environment variables tell the watchdog process to use the static mode and that the contents are in the /home/app folder. The HEALTHCHECK specifies that we should check the existence of the /tmp/.lock file every 3 seconds. This lockfile is created by the watchdog process and its existence is proof of a healthy server. When you run your containers using container orchestration tools like Kubernetes, a failed healthcheck results in creating a new container and removing the old one. 6.12.1 Multi-stage Builds A common use case for multi-stage builds is to run the building/rendering process in the first stage of the build, and copy over the results – in our case the static files – from the first stage to the final slimmer image (Fig. 6.3. Figure 6.3: Docker layers for single (left) and multi-stage builds (right). Dashed lines are temporary layers. A Python Shinylive example demonstrates this use case of the multi-stage build capabilities of Docker. The first stage is called the builder and is based on the general python:3.9 image. We save the rendered output into the /root/output folder. FROM python:3.9 AS builder WORKDIR /root RUN pip install shinylive COPY py-shiny/app app RUN pip install \\ --no-cache-dir --upgrade -r /root/app/requirements.txt RUN shinylive export app output [...] The same idea works for Quarto with Shinylive: FROM python:3.9 AS builder RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\ pandoc \\ curl \\ gdebi-core \\ &amp;&amp; rm -rf /var/lib/apt/lists/* RUN curl -LO https://quarto.org/download/latest/quarto-linux-amd64.deb RUN gdebi --non-interactive quarto-linux-amd64.deb RUN apt-get purge -y gdebi-core &amp;&amp; apt-get autoremove -y RUN pip install shinylive WORKDIR /root/app COPY quarto-py-shinylive/index.qmd /root/app/index.qmd COPY quarto-py-shiny/app/requirements.txt /root/app/requirements.txt RUN pip install --no-cache-dir --upgrade -r requirements.txt RUN quarto add quarto-ext/shinylive --no-prompt RUN quarto render /root/app/index.qmd --output-dir /root/output [...] The R version is slightly different because of the different packages that one has to install. Refer to the previous Quarto examples or the example repository for the corresponding changes. The second part for the Dockerfile for both the Shinylive and the Quarto with Shinylive examples (R and Python alike) is the following: [...] FROM ghcr.io/openfaas/of-watchdog:0.10.1 AS watchdog FROM alpine:3.20 RUN adduser -D app USER app WORKDIR /home/app COPY --from=builder /root/output /home/app COPY --from=watchdog /fwatchdog . ENV mode=&quot;static&quot; ENV static_path=&quot;/home/app&quot; HEALTHCHECK --interval=3s CMD [ -e /tmp/.lock ] || exit 1 CMD [&quot;./fwatchdog&quot;] This is identical to the previous example where you had the static files rendered locally and outside of the Docker image, and then included with COPY app .. Instead, you have COPY --from=builder /root/output /home/app here which means: grab the files from the output folder of the build stage. 6.13 Image Analysis Image analysis is a two part process. First, we extract the Software Bill of Material (SBOM) and other image metadata, these are called attestations. Second, we evaluate these data against vulnerability data from security advisories. Docker Scout can perform image analysis among other things and is available by default for Docker Hub repositories. Some of these features are experimental and are not universally supported in all registries, so we won’t cover everything in detail. You can consult the Docker CLI reference for docker scout. 6.13.1 Attestations Attestations describe how an image was built, and what it contains. The first kind of attestation is referred to as provenance. You can add the minimum set of provenance information to an image using BuildKit as: docker buildx build -t &lt;image-name&gt;:&lt;image-tag&gt; \\ --attest type=provenance,mode=min . Then you can inspect image provenance information with: docker buildx imagetools inspect &lt;image-name&gt;:&lt;image-tag&gt; \\ --format &quot;{{ json .Provenance.SLSA }}&quot; The second kind of attestation is the Software Bill of Material (SBOM). The SBOM is a complete inventory of a codebase, the license and version information. The docker scout command can scan images and create an SBOM. Here is the list view of the SBOM from the rocker/r2u:24.04 image: docker scout sbom --format list rocker/r2u:24.04 # Name Version Type # ------------------------------------------------------------------ # KernSmooth 2.23-24 cran # MASS 7.3-61 cran # Matrix 1.7-0 cran # acl 2.3.2-1build1 deb # [...] # zlib 1:1.3.dfsg-3.1ubuntu2 deb # zlib1g 1:1.3.dfsg-3.1ubuntu2 deb # zlib1g-dev 1:1.3.dfsg-3.1ubuntu2 deb Write the SBOM information into a JSON file for further processing: docker scout sbom rocker/r2u:24.04 &gt; sbom-r2u.json 6.13.2 Vulnerability Scanning Image analysis uses image SBOMs to understand what packages and versions an image contains. Knowing the exact versions of your packages is not only good for reproducibility but is also the foundation for vulnerability scanning. Docker Scout compares the SBOM to a list of known vulnerabilities. The quick view gives a high level overview of different levels of vulnerabilities. Vulnerabilities are ordered by severity: critical (C), high (H), medium (M) or low (L). There are no critical vulnerabilities known for the rocker/r2u image and there are 2 vulnerabilities classified as high but no critical vulnerabilities. docker scout quickview rocker/r2u:24.04 # ✓ Pulled # ✓ Image stored for indexing # ✓ Indexed 435 packages # # i Base image was auto-detected. To get more accurate results, # build images with max-mode provenance attestations. # Review docs.docker.com ↗ for more information. # # Target │ rocker/r2u:24.04 │ 0C 2H 206M 44L # digest │ f3272f6d118c │ # Base image │ ubuntu:24.04 │ 0C 0H 6M 6L # Refreshed base image │ ubuntu:24.04 │ 0C 0H 2M 6L # Updated base image │ ubuntu:24.10 │ 0C 0H 0M 0L The CVEs subcommand analyzes an image for vulnerabilities, e.g.  docker scout cves rocker/r2u:24.04. CVE is short for Common Vulnerabilities and Exposures. You might want to filter this result to show only critical and high vulnerability packages. It is not wise to ignore critical vulnerabilities. In such cases you should find alternatives and not include such packages in your image. Critical vulnerabilities can shake the whole software industry and are usually patched in a short amount of time, or at least a workaround is suggested. Get only critical CVEs as: docker scout cves --format only-packages --only-vuln-packages \\ --only-severity critical rocker/r2u:24.04 # ✓ SBOM of image already cached, 435 packages indexed # ✓ No vulnerable package detected # # Name Version Type Vulnerabilities Here is how to check for critical and high vulnerabilities: docker scout cves --format only-packages --only-vuln-packages \\ --only-severity critical,high rocker/r2u:24.04 # ✓ SBOM of image already cached, 435 packages indexed # ✗ Detected 1 vulnerable package with 2 vulnerabilities # # Name Version Type Vulnerabilities # -------------------------------------------------------- # linux 6.8.0-35.35 deb 0C 2H 0M 0L You can see that the 2 high CVEs are part of Linux itself and are not coming from packages that r2u adds on top of the Ubuntu parent image. 6.14 Containers You have learnt how to build images using the Dockerfile to contain a Shiny app. A “live” version of this image is called the container, that is the runtime instance of the docker image. Besides the image, it consists of a set of instructions that you specify before or during run time, and an execution environment. Let’s see how you can create, start, and manage Docker containers. 6.14.1 Docker Run The docker run command is a versatile tool because it can not only create and run a new container from an image, but it can also pull the image if needed. You have seen that we usually set the -p or --port option and map the host port 8080 to the container port 3838 as e.g. -p 8080:3838. Setting the port is needed when running a web application, such as Shiny. This way you can view the application in your browser. When you start a Docker container it executes a command that you specified before in the Docker image’s configuration, the Dockerfile. The default settings from the image usually work well, but you can also change them if needed. You may set or override many of the instructions from your Dockerfile: --expose exposes a port or a range of ports, --user provides a username to be used, --workdir sets the working directory inside the container, --entrypoint overwrites the default ENTRYPOINT instruction. It is common to use the --rm flag to automatically remove the container and its associated anonymous volumes when it exits. This way, when you hit CTRL+C, it will not only stop the container, but it will also remove it and docker ps -a will not list it any more. This is best suited for development. You can provide environment variables through the -e or --env option or provide a file with the variables using --env-file. Specifying the platform via --platform is needed when working with different architectures, such as ARM64 and AMD64. Setting resources available for the container is possible with the --cpus (number of CPUs) and setting memory limits by --memory. Docker containers and their file systems are considered ephemeral, which means they are not expected to persist data for long. Therefore, it is recommended to rely on external storage (databases, object stores) for anything that needs to persist and you do not want it to disappear when the container is deleted. Docker can persist data on the file system using bind mounts or volumes. Bind mounts may be stored anywhere on the host system and you can specify this via the --mount option in docker run. Compared to mounts, volumes are stored in a part of the host filesystem which is managed by Docker and other processes should not modify this part of the filesystem. You can specify volumes with the --volume option. Persisting data on the file system is an advanced topic that we’ll see some examples of later. We mention it here because managing file systems is also part of the magic of docker run. One more important flag is the -d or --detach flag. This starts the container as a background process. You get back your terminal and can start typing other commands. It can be a good idea to also add a name to the container so we can find it easier without looking for its ID: docker run \\ --rm \\ -d \\ -p 8080:3838 \\ --name r-shiny \\ --restart=always \\ ghcr.io/h10y/faithful/r-shiny:latest docker ps # CONTAINER ID IMAGE # 592caa564860 ghcr.io/h10y/faithful/r-shiny:latest # COMMAND CREATED STATUS # &quot;R -e &#39;shiny::runApp...&quot; 15 seconds ago Up 14 seconds # PORTS NAMES # 0.0.0.0:8080-&gt;3838/tcp r-shiny The docker ps command lists running containers. You see not only the info we provided with docker run or that were defined in the Dockerfile, but also for how long the container has been running (time since its creation) and also the status of the container. The docker run command is equivalent of first creating a container that consumes no resources yet with docker create &lt;image-name&gt; and then starting this container with docker start &lt;container-name-or-id&gt;, but it is much more convenient to use docker run. When the container is running in the background, you cannot stop it with CTRL+C. You have to manage it using the container ID or the container name. To stop the container, use docker stop &lt;container-name-or-id&gt;. This will “gracefully” shut down the Shiny app by sending a so called SIGTERM signal to it. If you use docker kill &lt;container-name-or-id&gt; instead, the process will be abruptly killed with a so called SIGKILL signal. Try docker stop first. None of these commands will remove the container. This means you can start it again with docker start &lt;container-name-or-id&gt;, or remove it with docker rm &lt;container-name-or-id&gt;. Notice the subtle difference between docker rm (remove a container) and docker rmi (remove an image). Most of the docker commands have aliases, use these if you want to be more specific, e.g. docker rmi is an alias for docker image rm, whereas docker rm is an alias for docker container rm. If for some reason, the container running in the background experiences an issue, like a unexpected user input, or it runs out of memory, the container will be stopped by default. If you want a different behavior, use the --restart to specify a restart policy: on-failure: restart only if the container exits with a non-zero exit status, unless-stopped: restart the container unless it is explicitly stopped or Docker itself is stopped or restarted, always: the Docker daemon tries to restart the container indefinitely irrespective of the cause. A non-zero exit status and running out of resources are clear signs of the app not running as expected. You will see in a bit how to troubleshoot using the Docker logs. But in less serious cases, we might not know the “health” of the container without looking at the logs or a user telling us that something is not right. This is where health checks come in. Before we introduce health checks, let’s stop the container: docker stop r-shiny We used the --rm flag, so the container will be removed after being stopped. If we haven’t used the --rm flag, we would still see it listed with docker ps -a. 6.14.2 Health Check Shiny apps, just like any other web application, can crash for many different reasons. It might be due to a bug in the code, your user might do something unexpected. Or simply, the app runs out of resources. This most often means that it runs out of memory. In these cases, it becomes unresponsive, although the process inside is still running. From a user experience standpoint, an exited process is the same as a non-exited but unresponsive process. From a monitoring perspective, one is easier to detect than the other. The purpose of the health check is to report on the status of our application, so that we know when it is “unhealthy”, which means users cannot access it. The HEALTHCHECK Dockerfile instruction checks a container’s health on startup. It is written as HEALTHCHECK &lt;options&gt; CMD &lt;command&gt;. The options determine how often the health check command is run (--interval, default is 30s), the timeout duration (--timeout, default is 30s). If checks take longer than the timeout, it is considered to have failed. There is also a start period (--start-period, default is 0s) After this period there are by default 3 retries (--retries) 5s apart (--start-interval). The container is not considered unhealth during the set number of retries. If a health check succeeds during the start period, the container is considered started. The health check command’s exit status indicates the health status of the container. The exit status can be either 0 (healthy) or 1 (unhealthy). Different commands might return different exit statuses. This is why you often see the health check command formatted as &lt;command&gt; || exit 1 which will force any non-0 exit status to be 1. For web applications, the simplest command would be to see if the main page responds to a request. If we start the shiny app on port 3838, the Curl command curl --fail http://localhost:3838 returns the HTML of the Shiny app or it fails due to the --fail flag (same as -f). The health check instruction could look like this: HEALTHCHECK --interval=15s \\ CMD curl --fail http://localhost:3838 || exit 1 The curl command might or might not be installed in your image. The ubuntu:24.04 parent image does not ship curl, so you have to add this to the RUN instructions that install system requirements. Alternatively, you can use bash (which is available on ubuntu:24.04 but not on alpine:3.20): HEALTHCHECK CMD bash -c &#39;:&gt; /dev/tcp/127.0.0.1/3838&#39; || exit 1 This bash command redirects (&gt;) a null value (:) to the TCP port 3838 on the local host, which is available from inside the container. Let’s try adding a health check to the ghcr.io/h10y/faithful/r-shiny image and see how we might be able to see the health status. We can add the command through the --health-cmd option of the docker run: docker run --rm -d -p 8080:3838 \\ --name healthy \\ --health-cmd=&quot;bash -c &#39;:&gt; /dev/tcp/127.0.0.1/3838&#39; || exit 1&quot; \\ ghcr.io/h10y/faithful/r-shiny:latest Let’s start another container with a health check targeting a different port, this will cause the check to fail. Note that the healthy container is already using the 8080 port, we map the unhealthy container to port 8081: docker run --rm -d -p 8081:3838 \\ --name unhealthy \\ --health-cmd=&quot;bash -c &#39;:&gt; /dev/tcp/127.0.0.1/4949&#39; || exit 1&quot; \\ ghcr.io/h10y/faithful/r-shiny:latest Try docker ps a few times. The status for the healthy image should be (healthy). The unhealthy image should display (health: starting), which would switch to (unhealthy) after the start period. You can also access the health status with the docker inspect command. You’ll need the jq command line tool that can parse JSON output: docker inspect --format &quot;{{json .State.Health }}&quot; healthy | jq # { # &quot;Status&quot;: &quot;healthy&quot;, # &quot;FailingStreak&quot;: 0, # &quot;Log&quot;: [ # { # &quot;Start&quot;: &quot;2024-07-17T08:29:36.969572509Z&quot;, # &quot;End&quot;: &quot;2024-07-17T08:29:37.0504063Z&quot;, # &quot;ExitCode&quot;: 0, # &quot;Output&quot;: &quot;&quot; # }, # [...] # } The output shows the health check results and the failing streak (how many times the check has failed in a row). A 0 failing streak is your target. The same command for the unhealthy container would show a very unhealthy failing streak: docker inspect --format &quot;{{json .State.Health }}&quot; unhealthy | jq # { # &quot;Status&quot;: &quot;unhealthy&quot;, # &quot;FailingStreak&quot;: 10, # &quot;Log&quot;: [ # { # &quot;Start&quot;: &quot;2024-07-17T08:30:13.47309747Z&quot;, # &quot;End&quot;: &quot;2024-07-17T08:30:13.572182554Z&quot;, # &quot;ExitCode&quot;: 1, # &quot;Output&quot;: &quot;bash: connect: Connection refused # bash: line 1: /dev/tcp/127.0.0.1/4949: Connection refused&quot; # }, # [...] # } Health checks can be more complex than pinging the Shiny app endpoint. For example if your application depends on connecting to external databases and APIs, you might want to include multiple checks in a single script file. These can be shell scripts, R, or Python scripts and can also rely on environment variables that contain access tokens during the container run time. You can include this script file in your image and add the corresponding instruction as: HEALTHCHECK CMD ./healthcheck.sh 6.14.3 Container Life Cycle You have seen the some of the possible statuses of containers. Let’s review the container life cycle in a bit more details so that you get a better sense of how to manage containers that are in different states. Figure 6.4 illustrates the container life cycle. Figure 6.4: The Docker container life cycle. Containers can be in one of seven states: created, running, paused, restarting, exited, removing, and dead. First the container is created, which means it has never been started. A freshly created container does not yet consumes resources. After the startup, the container is either running or exited. An exited status can mean that the container finished its job and exited with status code 0, or instead of running, the container failed to start and exited with an error code 1. The exited container does not consumes resources. A running container is started by either docker start or docker run. It might have a web server listening to incoming traffic, as is the case with Shiny. The start is considered successfully if the container is up for at least 10 seconds and Docker has started monitoring it. The health check does not immediately affect the state of the container as long as the main process is running, but the information might be used by the orchestration service (i.e. Kubernetes) to decide the fate of the container and replace it with a new instance. A container that is no longer running enters the exited state. This might be due to a CTRL+C signal, an exit code of 1. It can be due to the container running out of resources, for example exceeding the set memory limit. You can also use the docker stop to stop the container. When using docker stop, the main process inside the container will receive a SIGTERM signal, and after a grace period, a SIGKILL. The docker kill command sends SIGKILL immediately to stop the container. An exited container can be started again with docker start. It can also enter the restarting state when the orchestrator tries to restart a failed container due to a set restart policy. The restart policy will not apply to containers stopped manually. A running container can enter the restarting state due to the designated restart policy or by calling the docker restart command on a running container. After that, the container will enter the running (or the exited) state. If you pause a running container with docker pause it will be considered paused. All processes are suspended for an indefinite time, memory is still allocated to store the state of the running container, but no CPU resources are used. You can unpause the container with docker unpause. You can remove a container with docker rm. You might have to force the removal of a running container with the --force (or -f) flag. The forced stop will use SIGKILL, thus is equivalent to first stopping the container with docker kill and then removing it. The container will be in removing while the container is in the process of being removed. After the container is removed it will no longer exist unless there was a problem with the removal process. For example the container was only partially removed because resources were kept busy by an external process. Such a “defunct” container is called a dead container and it cannot be started, restarted, only removed. You can check the status of the two containers that we were using for the health check (we called them healthy and unhealthy) as: docker inspect --format &#39;{{.State.Status}}&#39; healthy # running docker inspect --format &#39;{{.State.Status}}&#39; unhealthy # running The --format instruction tells Docker to give us only the State related parts of the overall object returned by the docker inspect command. Within that list, we are interested in the Status property. This is what the .State.Status notation means. There are other interesting properties as part of the State. Status and Health are two properties that we have seen. But there are others: docker inspect --format &quot;{{json .State }}&quot; healthy | jq # { # &quot;Status&quot;: &quot;running&quot;, # &quot;Running&quot;: true, # &quot;Paused&quot;: false, # &quot;Restarting&quot;: false, # &quot;OOMKilled&quot;: false, # &quot;Dead&quot;: false, # &quot;Pid&quot;: 570, # &quot;ExitCode&quot;: 0, # &quot;Error&quot;: &quot;&quot;, # &quot;StartedAt&quot;: &quot;2024-07-18T10:20:34.182018084Z&quot;, # &quot;FinishedAt&quot;: &quot;0001-01-01T00:00:00Z&quot;, # &quot;Health&quot;: { # &quot;Status&quot;: &quot;healthy&quot;, # &quot;FailingStreak&quot;: 0, # &quot;Log&quot;: [ # [...] # ] # } # } 6.14.4 Managing Containers Here we summarize the most important commands related to containers. The docker ps command lists the containers. If you have container running, you will see those listed with status Up (i.e. running). docker ps # CONTAINER ID IMAGE # c31e8c365534 ghcr.io/h10y/faithful/r-shiny:latest # c2a7f34d38bc ghcr.io/h10y/faithful/r-shiny:latest # COMMAND CREATED STATUS # &quot;R -e &#39;shiny::runApp...&quot; 14 minutes ago Up 14 minutes (unhealthy) # &quot;R -e &#39;shiny::runApp...&quot; 14 minutes ago Up 14 minutes (healthy) # PORTS NAMES # 0.0.0.0:8081-&gt;3838/tcp unhealthy # 0.0.0.0:8080-&gt;3838/tcp healthy docker container stats displays a live stream of the containers’ resource usage statistics (hit CTRL+C to exit): CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % c8c4dad4e371 unhealthy 0.18% 94.35MiB / 7.657GiB 1.20% 3b44c40aadb7 healthy 0.11% 122MiB / 7.657GiB 1.56% Use docker logs &lt;container-name-or-id&gt; will print (all) the logs for a given container. Logs are made up of the container’s STDOUT and STDERR. To print only the tail of the logs use docker logs -n 10 &lt;container-name-or-id&gt; that will print the last 10 lines. To follow the logs in real time, use docker logs -f &lt;container-name-or-id&gt;. The docker exec command executes a command in a running container. For example docker exec -it healthy sh will start a shell in the healthy container we still have running. The -it flag stands for the combination of --interactive (keep standard input, STDIN, open) and --tty (pseudo “teletypewriter”) so we can use the shell interactively. Start poking around, try typing a few commands: whoami should return app as the user name, pwd should return /home/app as per our Dockerfile instructions, env lists environment variables. Exit the container’s shell with exit. To evaluate a command in the container, try docker exec -it healthy sh -c \"whoami\". Let’s stop the containers with docker stop healthy unhealthy (yes, you can pass an array of container names to docker stop). You can also stop all running containers with docker stop $(docker ps -q) and all (running and stopped) containers with docker stop $(docker ps -a -q). The $(...) shell expression executes the command within the parentheses and inserts the output and $(docker ps -q) will print out the container IDs. If you stopped all the containers, you will not see any running containers listed with docker ps. To see the stopped but not removed containers, use the docker ps -a command. We started the healthy and unhealthy containers with the --rm flag, so those were removed after being stopped. As a result, not even docker ps -a will list them. Sometimes you need to be able to manage containers because the kill signal is not properly relayed to the container when using CTRL+C. This happens when the CMD instruction is provided in shell form (i.e. CMD R -e \"shiny::runApp()\" instead of CMD [\"R\", \"-e\", \"shiny::runApp()\"]). The shell form runs as a child process of /bin/sh -c (default ENTRYPOINT), and the executable does not receive Unix signals. If this happens, you need to find a way to stop the container. These are the most commonly used commands with containers: docker container stop &lt;container-id&gt;: gracefully stop a running container (wait for the process to stop), docker container start &lt;container-id&gt;: start a stopped container, docker container restart &lt;container-id&gt;: restart a container, docker container rm &lt;container-id&gt;; remove a container, docker container kill &lt;container-id&gt;: kill a container (abruptly terminate the entry point process). docker container rm --force &lt;container-id&gt; will remove running containers too. You can make sure the container is removed after CTRL+C if you add the --rm option to the docker run command and it will automatically remove the container when it exits. 6.14.5 Docker Compose You have seen how to manage a single container. But in practice, we often manage multiple containers: multiple replicas of the same app, different applications, and services that help with sending traffic to the right places, collect diagnostic information, provide a layer of security etc. Managing all this complexity with Docker on a single container basis is going to be a problem. It is not impossible, but it will be difficult and error prone, and as a result less secure. Docker Compose is a tool for defining and running multi-container applications. Docker Compose is declarative in nature, it uses a single, comprehensible YAML configuration file to define the expected state of your system. The YAML defines the services, networks, and volumes. Version 1 of the Docker Compose project stopped receiving updates from July 2023. Compose Version 2 is included with any new install of Docker Desktop. Version 2 uses BuildKit, and has continued new-feature development. You might see commands starting with docker-compose. That used to be the command for Version 1. It is now an alias for docker compose by default. It is important to be aware of this historical difference because most examples that you find online might refer to the use of Version 1 and docker-compose. We will use the recommended Version 2 and docker compose for our examples to make this distinction clear. 6.14.6 The Compose File You will see older tutorials using docker-compose.yml which refers to Version 1 of Docker Compose. Version 2 still supports this file naming, but compose.yaml is recommended to make the distinction clear. Create an empty text file named compose.yaml and copy-pase this into it. services: faithful: image: &quot;ghcr.io/h10y/faithful/py-shiny:main&quot; ports: - &quot;8080:3838&quot; bananas: image: &quot;ghcr.io/h10y/bananas/r-shiny:main&quot; ports: - &quot;8081:3838&quot; environment: - DEBUG=1 FIXME: provide link to the example. The Compose file specification has several top level elements: version is obsolete but can be important for backwards compatibility. name is a value to override the default project name that is derived from the base name of the project directory. services must be defined, it is the abstract definition of a computing resource within an application that can be “composed” together and modified independently from other components. networks defines how the services communicate with each other. By default, each container for a service joins the default network and is reachable by other containers. volumes are persistent data stores. In our simple example we only use services and define two Shiny apps. You will see more complex examples later. Services are listed by name, each service is followed by their attributes. Attributes are very similar to the command line options we saw for docker run. See the Compose file specification for all the details. The compose file can also define a service via a Dockerfile under the build attribute. The image will be built and started by Compose. Similarly, you can define the image attribute for pulling the image from a registry. The ports attribute should look familiar by now. It is used to define the port mappings between the host machine (left side of the colon) and the containers (right side of the colon). Notice the double quotes in the YAML file. Some characters, like * or : have special meaning in YAML, thus values containing these should be double quoted. We defined two services, the Python version of the Faithful example and the R version of the Bananas app. You see environment variables defined for the bananas service. 6.14.7 Compose Command Line You can use Docker Compose through the docker compose command of the Docker Command Line Interface (CLI), and its subcommands. Let’s review the most important commands. Change your working directory so that the compose.yaml file is in the root of that folder. Start all the services defined in your compose.yaml file as: docker compose up # [+] Running 10/10 # ✔ bananas Pulled 13.8s # [...] # ✔ faithful Pulled 1.0s # [+] Running 3/3 # ✔ Network 03-compose_default Created 0.1s # ✔ Container 03-compose-faithful-1 Created 0.3s # ✔ Container 03-compose-bananas-1 Created 0.3s # [...] # bananas-1 | Listening on http://0.0.0.0:3838 # [...] # faithful-1 | INFO: Uvicorn running on http://0.0.0.0:3838 [...] You’ll see logs appearing in your terminal. First about pulling the images if those are not yet available, or if a newer version can be found. Visit http://localhost:8080 to see the Faithful app and http://localhost:8081 to see the Bananas app. Hit CTRL+C in the terminal to stop the containers. Similarly to docker run, we can use the -d (or --detach) flag to start the containers in the background as docker compose up -d. You’ll get back your terminal. Use docker compose ls to list currently running Compose projects: FIXME: edit the compose project name according to example repo name. docker compose ls # NAME STATUS CONFIG FILES # 03-compose running(2) compose.yaml Use docker compose ps to list the containers for the current Compose project: docker compose ps # NAME IMAGE # 03-compose-bananas-1 ghcr.io/h10y/bananas/r-shiny:main # 03-compose-faithful-1 ghcr.io/h10y/faithful/py-shiny:main # COMMAND SERVICE # &quot;R -e &#39;shiny::runApp…&quot; bananas # &quot;uvicorn app:app --h…&quot; faithful # CREATED STATUS PORTS # 11 minutes ago Up 17 seconds 0.0.0.0:8081-&gt;3838/tcp # 11 minutes ago Up 17 seconds 0.0.0.0:8080-&gt;3838/tcp Use docker compose logs to get visibility into the logs when containers are running in detached mode. Logs can grow long. Use the -n option to show the tail of the logs: docker compose logs -n 10 will show the last 10 lines of the logs; docker compose logs -n 10 bananas will show the last 10 lines of the logs for the Bananas app, you have to use the service name as defined in the YAML configuration. If you want to follow the logs in real time, use docker compose logs -f for all the logs or docker compose logs -f &lt;service-name&gt; for a given service. Hit CTRL+C to get back into the terminal. To poke around in the running containers, use docker compose exec. docker compose exec bananas sh will give you a shell inside the container. Let’s type the env command to see the environment variable DEBUG that we defined in the Compose file: $ env # HOSTNAME=4f87a297b85c # DEBUG=1 # HOME=/home/app # TERM=xterm # PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin # LANG=en_US.UTF-8 # DEBIAN_FRONTEND=noninteractive # LC_ALL=en_US.UTF-8 # PWD=/home/app # TZ=UTC Type exit to exit the shell. Now let’s suppose that you want to change the DEBUG variable to 0 to turn off the debugging mode of the app. Edit the config.yaml file and change the value of 1 to 0. Save your changes. Type docker compose up -d to apply the changes. This will recreate the Bananas service: docker compose up -d # [+] Running 1/2 # ✔ Container 03-compose-faithful-1 Running 0.0s # ⠏ Container 03-compose-bananas-1 Recreate # Wait for a few seconds ... # [+] Running 2/2 # ✔ Container 03-compose-faithful-1 Running 0.0s # ✔ Container 03-compose-bananas-1 Started 10.5s Type docker compose exec bananas sh -c \"env\" to list the environment variables. You should see the new value DEBUG=0. Stop the containers with docker compose down: docker compose down # [+] Running 3/3 # ✔ Container 03-compose-bananas-1 Removed 10.2s # ✔ Container 03-compose-faithful-1 Removed 0.5s # ✔ Network 03-compose_default Removed 0.1s 6.14.8 Container Orchestration Complexity does not stop at managing multiple containers. The number of containers might not stay constant as you might to demand by scaling up and down. You might also want to roll out new versions by causing minimal disruption in the system. Even Docker Compose might be insufficient for use cases at such scale. This is where container orchestration comes in. Container orchestration makes this complexity manageable for development and operations (DevOps). Container orchestration automates the provisioning, deployment, scaling, and management of containerized applications. It also abstracts away the underlying infrastructure. Kubernetes is a widely used container orchestration platform. Running Shiny apps with Kubernetes is an advanced topic, but all that you have learnt about containerized application development will be useful if you ever need to use Kubernetes for your production deployment. 6.15 Best Practices No matter the use case, Docker images start with a parent image. What parent image you use? How do you add new layers to it? These decisions will determine how quickly you can iterate while in development, and the size of the final image you send to production. But it is not only about your experience, but about possible issues that may arise and security implications that might matter even more. Let’s review best practices for Dockerfiles and building images that apply not only to containerized Shiny app development but to any containerized application and workflow. These will all improve the developer experience and the quality of the final Docker images. 6.15.1 Parent Images Decide which is the right parent image for the FROM instruction of your image. The type of the application might dictate this, e.g. R or Python. If you have fewer dependencies, try using a lean image variant. If you need build time tools see if a more general and usually larger image is the best starting point. You might still be able to strip away some fat by leveraging multi-stage builds. You should also pin the version of your base image. Otherwise, using the latest image tag might surprise you if you try to rebuild your image a few years later. Although Linux systems are generally considered very robust, best practices and the security landscape is evolving constantly. For example, if you used adduser to create a new non-privileged user in your image using Ubuntu 22.04 or earlier, it would fail on 24.04 because it has useradd instead as the recommended function to use. Not a big hurdle, but the image nevertheless would not build as is because now the latest tag refer to the latest version. Pinning the parent image version might also help with other dependencies as well, e.g. in the Rocker versioned stack of images you’ll have a maximum R package version also pinned that was available at the time of the R versioned image. 6.15.2 Minimize Dependencies Minimizing dependencies is advantageous for many reasons. It results in a smaller final image size, but more importantly, it presents smaller attack surface for malicious actors. Smaller is usually safer. This is especially true when the image is being used as part of client-facing applications on the Internet. Internal tools and dev containers are less of a concern because these cannot be accessed by the public. Avoid installing “nice to have” packages and do not start from general-purpose parent images aimed at interactive use. Images for Shiny apps and other web services benefit from keeping the images as lean as possible by adding only those R packages and system requirements that are absolutely necessary. You should also uninstall unnecessary build time libraries. Multi-stage builds can be helpful to only include artifacts that are needed. 6.15.3 Cache and Order Layers When building an image, Docker executes each instruction in the order specified in the Dockerfile. Docker looks for an existing image in its cache that it can reuse, rather than creating a new (duplicate) image layer. Only the instructions RUN, COPY, ADD create layers. For the RUN instructions the command string from the Dockerfile is used to find a match from an existing image. For the ADD and COPY instructions, the contents of the file(s) in the image are examined and a checksum is calculated for each file, however he last-modified and last-accessed times of the file(s) are not considered in these checksums. You can chain RUN instructions to create a single layer. This is especially important when using apt-get update &amp;&amp; apt-get install so that the package lists will be used to find the most up-to-date candidates to install. When troubleshooting, use docker run --no-cache &lt;image-name&gt; to disregard the cache. Caching can be useful is when installing dependencies. Here is a simplified snippet of a Dockerfile to illustrate cache invalidation: ## Install dependencies COPY &lt;dependencies&gt; . RUN &lt;dependency-install-command&gt; ## Copy the app COPY app . What would happen if we switched the two blocks? ## Copy the app and &lt;dependencies&gt; COPY app . ## Install dependencies RUN &lt;dependency-install-command&gt; You would have to wait for the build to reinstall all the packages whenever the app files have changed. This is because once the cache is invalidated all subsequent Dockerfile commands generate new images instead of using the cache. In general, start with the instructions that change less frequently. Putting your dependencies before the application source code sounds trivial. But you might have to use parts of your code that define the dependencies through a lock file, etc. In this case you have to copy the lock file separately, install dependencies, then copy the rest of the source code in another COPY instruction. 6.15.4 Switch to Non-root User By default, Docker containers run as the root user. Root privileges allow unrestricted use which is to be avoided in production. Although you can find lots of examples on the Internet where the container is run as root (Eng and Hindle 2021), this is generally considered bad practice or a “code smell”. Some parent images come with a non-root user already defined. For example some of the Rocker images have a non-privileged docker user that you can use. Otherwise, create a new group and user with something like this: [...] RUN groupadd &lt;group&gt; &amp;&amp; useradd -g &lt;group&gt; &lt;user&gt; WORKDIR /home/&lt;user&gt; COPY app . RUN chown &lt;user&gt;:&lt;group&gt; -R /home/&lt;user&gt; USER &lt;user&gt; [...] 6.15.5 No Secrets Avoid hard coding sensitive information into the Dockerfile or the image. Do not set environment variables with ENV that store passwords or tokens. Set these at run time with environment variables. Also do not store such sensitive information in files that you copy. Add such files to .gitignore and .dockerignore to help prevent leaking secrets. Here is an example .dockerignore file that will exclude common files used for storing secrets: **/.env **/.aws **/.ssh 6.15.6 Shell vs. Exec Syntax Use the exec array syntax for CMD and ENTRYPOINT instructions, e.g. CMD [\"R\", \"-e\", \"shiny::runApp()\"]. This will ensure that when you hit CTRL+C to stop the container process it will actually be signalled properly and you won’t have to abruptly kill the container. Use the shell syntax only for RUN instructions, e.g.  RUN R -e \"install.packages('shiny')\". 6.15.7 Log to STDOUT and STDERR Your apps should writ logs to standard output (STDOUT) and standard error (STDERR) instead of writing these messages to a file. This way you will be able to use standard Docker tools for reding and collecting the logs later. 6.15.8 Include HEALTHCHECK Use the HEALTHCHECK instruction to determine if the process running in your container is “healthy” and not just up and running. 6.15.9 Version Your Images Use timestamps, Git commit hashes, or semantic versioning for your image tags, or a combination of these. You can avoid overwriting existing images and will be able to roll back changes when you send some bugs into production. You can automate this using GitHub actions as you’ll see later. 6.15.10 Readability Chain commands in your RUN instructions and sort multiline arguments using \\. This will also help in updating your Dockerfile and track the changes with Git. The difference will be only a line instead of the whole statement. 6.15.11 More Tips Linters, like Hadolint use many more rules for Dockerfiles that are worth consulting as an extension of this best practices list. Hadolint is also available as a VS Code extension. Here are some other resources that are worth checking out to improve your Dockerfiles and Docker images, and to secure your containerized applications: Building best practices by Docker Hadolint rules 6.16 Summary The use of Docker with other open source software such as R nd Python has been transformative over the past decade (Boettiger and Eddelbuettel 2017; Nüst et al. 2020; Eng and Hindle 2021). You can find examples for almost anything ranging from interactive data science to asychronous APIs in Kubernetes. With the newfound ability to wrap any Shiny app in a Docker container, you’ll be able to deploy these images to many different hosting platforms. Of course, there is a lot more to learn, e.g. about handling dependencies, persisting data across sessions and containers, and so on. We’ll cover these use cases in due time. Until then, celebrate this milestone, check out further readings, and try to containerize some of your own Shiny apps. You can also share Docker images with others. This, however, will require the recipient of your app to have Docker installed and be able to run it locally. In the next Part, we’ll cover options for hosting your app, so that others will only need a browser to be able to access it. No R, Python, or Docker runtime environment is needed on the user’s part. Hosting the app for your users will also be the preferred option in case you do not want to share the source code or the Docker image with the users. Further reading: Docker glossary Docker reference Docker Compose References Boettiger, Carl, and Dirk Eddelbuettel. 2017. “An Introduction to Rocker: Docker Containers for R.” The R Journal 9 (2): 527–36. https://doi.org/10.32614/RJ-2017-065. Csárdi, Gábor, and Jim Hester. 2024. Pak: Another Approach to Package Installation. https://CRAN.R-project.org/package=pak. Csárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan, and Dan Tenenbaum. 2024. Remotes: R Package Installation from Remote Repositories, Including ’GitHub’. https://CRAN.R-project.org/package=remotes. Eddelbuettel, Dirk, and Jeff Horner. 2024. Littler: R at the Command-Line via ’r’. https://CRAN.R-project.org/package=littler. Eng, Kalvin, and Abram Hindle. 2021. “Revisiting Dockerfiles in Open Source Software over Time.” In 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), 449–59. https://doi.org/10.1109/MSR52588.2021.00057. Nüst, Daniel, Dirk Eddelbuettel, Dom Bennett, Robrecht Cannoodt, Dav Clark, Gergely Daróczi, Mark Edmondson, et al. 2020. “The Rockerverse: Packages and Applications for Containerisation with R.” The R Journal 12 (1): 437–61. https://doi.org/10.32614/RJ-2020-007. Ooms, Jeroen. 2024. Curl: A Modern and Flexible Web Client for r. https://CRAN.R-project.org/package=curl. Rodrigues, Bruno. 2023. Building Reproducible Analytical Pipelines with R. https://raps-with-r.dev/. Sólymos, Péter. 2024. Deps: Dependency Management with ’Roxygen’-Style Comments. https://hub.analythium.io/deps/. Urbanek, Simon, and Jeffrey Horner. 2023. Cairo: R Graphics Device Using Cairo Graphics Library for Creating High-Quality Bitmap (PNG, JPEG, TIFF), Vector (PDF, SVG, PostScript) and Display (X11 and Win32) Output. https://CRAN.R-project.org/package=Cairo. Ushey, Kevin, and Hadley Wickham. 2024. Renv: Project Environments. https://CRAN.R-project.org/package=renv. "],["references.html", "References", " References Aden-Buie, Garrick, Carson Sievert, Richard Iannone, JJ Allaire, and Barbara Borges. 2023. Flexdashboard: R Markdown Format for Flexible Dashboards. https://CRAN.R-project.org/package=flexdashboard. Allaire, JJ, Yihui Xie, Christophe Dervieux, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, et al. 2024. Rmarkdown: Dynamic Documents for r. https://github.com/rstudio/rmarkdown. Boettiger, Carl, and Dirk Eddelbuettel. 2017. “An Introduction to Rocker: Docker Containers for R.” The R Journal 9 (2): 527–36. https://doi.org/10.32614/RJ-2017-065. Chang, Winston, Joe Cheng, JJ Allaire, Carson Sievert, Barret Schloerke, Yihui Xie, Jeff Allen, Jonathan McPherson, Alan Dipert, and Barbara Borges. 2024. Shiny: Web Application Framework for r. https://CRAN.R-project.org/package=shiny. Cheng, Joe, Winston Chang, Steve Reid, James Brown, Bob Trower, and Alexander Peslyak. 2024. Httpuv: HTTP and WebSocket Server Library. https://CRAN.R-project.org/package=httpuv. Coene, J. 2021. Javascript for r. Chapman &amp; Hall/CRC the r Series. CRC Press. https://books.google.ch/books?id=ntUxEAAAQBAJ. Coene, John. 2022. Leprechaun: Create Simple ’Shiny’ Applications as Packages. https://CRAN.R-project.org/package=leprechaun. Csárdi, Gábor, and Jim Hester. 2024. Pak: Another Approach to Package Installation. https://CRAN.R-project.org/package=pak. Csárdi, Gábor, Jim Hester, Hadley Wickham, Winston Chang, Martin Morgan, and Dan Tenenbaum. 2024. Remotes: R Package Installation from Remote Repositories, Including ’GitHub’. https://CRAN.R-project.org/package=remotes. Eddelbuettel, Dirk, and Jeff Horner. 2024. Littler: R at the Command-Line via ’r’. https://CRAN.R-project.org/package=littler. Eng, Kalvin, and Abram Hindle. 2021. “Revisiting Dockerfiles in Open Source Software over Time.” In 2021 IEEE/ACM 18th International Conference on Mining Software Repositories (MSR), 449–59. https://doi.org/10.1109/MSR52588.2021.00057. Fay, Colin, Vincent Guyader, Sébastien Rochette, and Cervan Girard. 2023. Golem: A Framework for Robust Shiny Applications. https://CRAN.R-project.org/package=golem. Fay, Colin, Sébastien Rochette, Vincent Guyader, and Cervan Girard. 2021. Engineering Production-Grade Shiny Apps. Chapman &amp; Hall. https://engineering-shiny.org/. Gold, Alex. 2024. DevOps for Data Science. Chapman &amp; Hall. https://do4ds.com/. Granjon, David. 2022. Outstanding User Interfaces with Shiny. Chapman &amp; Hall. https://unleash-shiny.rinterface.com/. Hadley, Wickham, and Jennifer Bryan. 2023. R Packages. 2nd ed. O’Reilly Media, Inc. https://r-pkgs.org/. Hunter, J. D. 2007. “Matplotlib: A 2D Graphics Environment.” Computing in Science &amp; Engineering 9 (3): 90–95. https://doi.org/10.1109/MCSE.2007.55. Knuth, Donald. 1992. Literate Programming. Center for the Study of Language; Information—CSLI. Lai, Randy. 2023. Languageserver: Language Server Protocol. https://CRAN.R-project.org/package=languageserver. Leisch, Friedrich. 2002. “Sweave, Part i: Mixing r and LaTeX.” R News 2: 28–31. Meyer, David, Evgenia Dimitriadou, Kurt Hornik, Andreas Weingessel, and Friedrich Leisch. 2023. E1071: Misc Functions of the Department of Statistics, Probability Theory Group (Formerly: E1071), TU Wien. https://CRAN.R-project.org/package=e1071. Nüst, Daniel, Dirk Eddelbuettel, Dom Bennett, Robrecht Cannoodt, Dav Clark, Gergely Daróczi, Mark Edmondson, et al. 2020. “The Rockerverse: Packages and Applications for Containerisation with R.” The R Journal 12 (1): 437–61. https://doi.org/10.32614/RJ-2020-007. Ooms, Jeroen. 2024. Curl: A Modern and Flexible Web Client for r. https://CRAN.R-project.org/package=curl. Pedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30. R Core Team. 2024. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Rodrigues, Bruno. 2023. Building Reproducible Analytical Pipelines with R. https://raps-with-r.dev/. Rudolph, Konrad. 2024. Box: Write Reusable, Composable and Modular r Code. https://CRAN.R-project.org/package=box. Schloerke, Barret. 2024. Shinytest2: Testing for Shiny Applications. https://CRAN.R-project.org/package=shinytest2. Sólymos, Péter. 2024. Deps: Dependency Management with ’Roxygen’-Style Comments. https://hub.analythium.io/deps/. Urbanek, Simon, and Jeffrey Horner. 2023. Cairo: R Graphics Device Using Cairo Graphics Library for Creating High-Quality Bitmap (PNG, JPEG, TIFF), Vector (PDF, SVG, PostScript) and Display (X11 and Win32) Output. https://CRAN.R-project.org/package=Cairo. Ushey, Kevin, JJ Allaire, and Yuan Tang. 2024. Reticulate: Interface to ’Python’. https://CRAN.R-project.org/package=reticulate. Ushey, Kevin, and Hadley Wickham. 2024. Renv: Project Environments. https://CRAN.R-project.org/package=renv. Waskom, Michael L. 2021. “Seaborn: Statistical Data Visualization.” Journal of Open Source Software 6 (60): 3021. https://doi.org/10.21105/joss.03021. Wickham, Hadley. 2021. Mastering Shiny. O’Reilly Media, Inc. https://mastering-shiny.org/. Wickham, Hadley, Peter Danenberg, Gábor Csárdi, and Manuel Eugster. 2024. Roxygen2: In-Line Documentation for r. https://CRAN.R-project.org/package=roxygen2. Xie, Yihui. 2024. Knitr: A General-Purpose Package for Dynamic Report Generation in r. https://yihui.org/knitr/. Żyła, Kamil, Jakub Nowicki, Leszek Siemiński, Marek Rogala, Recle Vibal, Tymoteusz Makowski, and Rodrigo Basa. 2024. Rhino: A Framework for Enterprise Shiny Applications. https://CRAN.R-project.org/package=rhino. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
